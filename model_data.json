{
    "spleen_ct_segmentation": {
        "model_name": "Spleen CT segmentation",
        "description": "A pre-trained model for volumetric (3D) segmentation of the spleen from CT image",
        "authors": "MONAI team",
        "papers": [
            "Xia, Yingda, et al. '3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training. arXiv preprint arXiv:1811.12506 (2018). https://arxiv.org/abs/1811.12506.",
            "Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40"
        ],
        "version": "0.6.0",
        "model_id": "spleen_ct_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for volumetric (3D) segmentation of the spleen from CT images.</p>\n<p>This model is trained using the runner-up [1] awarded pipeline of the \"Medical Segmentation Decathlon Challenge 2018\" using the UNet architecture [2] with 32 training images and 9 validation images.</p>\n<p><img alt=\"model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_spleen_ct_segmentation_workflow.png\"/></p>\n<h2>Data</h2>\n<p>The training dataset is the Spleen Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.</p>\n<ul>\n<li>Target: Spleen</li>\n<li>Modality: CT</li>\n<li>Size: 61 3D volumes (41 Training + 20 Testing)</li>\n<li>Source: Memorial Sloan Kettering Cancer Center</li>\n<li>Challenge: Large-ranging foreground size</li>\n</ul>\n<h2>Training configuration</h2>\n<p>The segmentation of spleen region is formulated as the voxel-wise binary classification. Each voxel is predicted as either foreground (spleen) or background. And the model is optimized with gradient descent method minimizing Dice + cross entropy loss between the predicted mask and ground truth segmentation.</p>\n<p>The training was performed with the following:</p>\n<ul>\n<li>GPU: at least 12GB of GPU memory</li>\n<li>Actual Model Input: 96 x 96 x 96</li>\n<li>AMP: True</li>\n<li>Optimizer: Novograd</li>\n<li>Learning Rate: 0.002</li>\n<li>Loss: DiceCELoss</li>\n<li>Dataset Manager: CacheDataset</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h3>Input</h3>\n<p>One channel\n- CT image</p>\n<h3>Output</h3>\n<p>Two channels\n- Label 1: spleen\n- Label 0: everything else</p>\n<h2>Performance</h2>\n<p>Dice score is used for evaluating the performance of the model. This model achieves a mean dice score of 0.961.</p>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the training loss over 1260 epochs (10080 iterations).\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_ct_segmentation_train.png\"/></p>\n<h4>Validation Dice</h4>\n<p><img alt=\"A graph showing the validation mean Dice over 1260 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_ct_segmentation_val.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>spleen_ct_segmentation</code> bundle supports acceleration with TensorRT through the ONNX-TensorRT method. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">6.46</td>\n<td style=\"text-align: center;\">4.48</td>\n<td style=\"text-align: center;\">2.52</td>\n<td style=\"text-align: center;\">1.96</td>\n<td style=\"text-align: center;\">1.44</td>\n<td style=\"text-align: center;\">2.56</td>\n<td style=\"text-align: center;\">3.30</td>\n<td style=\"text-align: center;\">2.29</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">1268.03</td>\n<td style=\"text-align: center;\">1152.40</td>\n<td style=\"text-align: center;\">1137.40</td>\n<td style=\"text-align: center;\">1114.25</td>\n<td style=\"text-align: center;\">1.10</td>\n<td style=\"text-align: center;\">1.11</td>\n<td style=\"text-align: center;\">1.14</td>\n<td style=\"text-align: center;\">1.03</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>Currently, the only available method to accelerate this model is through ONNX-TensorRT. However, the Torch-TensorRT method is under development and will be available in the near future.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.1\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config and <code>evaluate</code> config to execute multi-GPU evaluation:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt; --dynamic_batchsize \"[1, 4, 8]\" --use_onnx \"True\" --use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Xia, Yingda, et al. \"3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training.\" arXiv preprint arXiv:1811.12506 (2018). https://arxiv.org/abs/1811.12506.</p>\n<p>[2] Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/spleen_ct_segmentation/tree/0.6.0",
        "changelog": {
            "0.6.0": "update to huggingface hosting",
            "0.5.9": "use monai 1.4 and update large files",
            "0.5.8": "update to use monai 1.3.2",
            "0.5.7": "update to use monai 1.3.1",
            "0.5.6": "add load_pretrain flag for infer",
            "0.5.5": "add checkpoint loader for infer",
            "0.5.4": "update to use monai 1.3.0",
            "0.5.3": "fix the wrong GPU index issue of multi-node",
            "0.5.2": "remove error dollar symbol in readme",
            "0.5.1": "add RAM warning",
            "0.5.0": "update the README file with the ONNX-TensorRT conversion",
            "0.4.9": "update TensorRT descriptions",
            "0.4.8": "update deterministic training results",
            "0.4.7": "update the TensorRT part in the README file",
            "0.4.6": "fix mgpu finalize issue",
            "0.4.5": "enable deterministic training",
            "0.4.4": "add the command of executing inference with TensorRT models",
            "0.4.3": "fix figure and weights inconsistent error",
            "0.4.2": "use torch 1.13.1",
            "0.4.1": "update the readme file with TensorRT convert",
            "0.4.0": "fix multi-gpu train config typo",
            "0.3.9": "adapt to BundleWorkflow interface",
            "0.3.8": "add name tag",
            "0.3.7": "restructure readme to match updated template",
            "0.3.6": "enhance readme with details of model training",
            "0.3.5": "update to use monai 1.0.1",
            "0.3.4": "enhance readme on commands example",
            "0.3.3": "fix license Copyright error",
            "0.3.2": "improve multi-gpu logging",
            "0.3.1": "add multi-gpu evaluation config",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.1": "disable image saving during evaluation",
            "0.1.0": "complete the model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "pancreas_ct_dints_segmentation": {
        "model_name": "Pancreas CT DiNTS segmentation",
        "description": "Searched architectures for volumetric (3D) segmentation of the pancreas from CT image",
        "authors": "MONAI team",
        "papers": [
            "He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850)."
        ],
        "version": "0.5.1",
        "model_id": "pancreas_ct_dints_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A neural architecture search algorithm for volumetric (3D) segmentation of the pancreas and pancreatic tumor from CT image. This model is trained using the neural network model from the neural architecture search algorithm, DiNTS [1].</p>\n<p><img alt=\"image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_workflow_4-1.png\"/></p>\n<h2>Data</h2>\n<p>The training dataset is the Pancreas Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.</p>\n<ul>\n<li>Target: Pancreas and pancreatic tumor</li>\n<li>Modality: Portal venous phase CT</li>\n<li>Size: 420 3D volumes (282 Training +139 Testing)</li>\n<li>Source: Memorial Sloan Kettering Cancer Center</li>\n<li>Challenge: Label unbalance with large (background), medium (pancreas) and small (tumour) structures.</li>\n</ul>\n<h3>Preprocessing</h3>\n<p>The data list/split can be created with the script <code>scripts/prepare_datalist.py</code>.</p>\n<pre><code>python scripts/prepare_datalist.py --path /path-to-Task07_Pancreas/ --output configs/dataset_0.json\n</code></pre>\n<h2>Training configuration</h2>\n<p>The training was performed with at least 16GB-memory GPUs.</p>\n<p>Actual Model Input: 96 x 96 x 96</p>\n<h3>Neural Architecture Search Configuration</h3>\n<p>The neural architecture search was performed with the following:</p>\n<ul>\n<li>AMP: True</li>\n<li>Optimizer: SGD</li>\n<li>Initial Learning Rate: 0.025</li>\n<li>Loss: DiceCELoss</li>\n</ul>\n<h3>Optimial Architecture Training Configuration</h3>\n<p>The training was performed with the following:</p>\n<ul>\n<li>AMP: True</li>\n<li>Optimizer: SGD</li>\n<li>(Initial) Learning Rate: 0.025</li>\n<li>Loss: DiceCELoss</li>\n</ul>\n<p>The segmentation of pancreas region is formulated as the voxel-wise 3-class classification. Each voxel is predicted as either foreground (pancreas body, tumour) or background. And the model is optimized with gradient descent method minimizing soft dice loss and cross-entropy loss between the predicted mask and ground truth segmentation.</p>\n<h3>Input</h3>\n<p>One channel\n- CT image</p>\n<h3>Output</h3>\n<p>Three channels\n- Label 2: pancreatic tumor\n- Label 1: pancreas\n- Label 0: everything else</p>\n<h3>Memory Consumption</h3>\n<ul>\n<li>Dataset Manager: CacheDataset</li>\n<li>Data Size: 420 3D Volumes</li>\n<li>Cache Rate: 1.0</li>\n<li>Multi GPU (8 GPUs) - System RAM Usage: 400G</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Performance</h2>\n<p>Dice score is used for evaluating the performance of the model. This model achieves a mean dice score of 0.62.</p>\n<p>Please note that this bundle is non-deterministic because of the trilinear interpolation used in the network. Therefore, reproducing the training process may not get exactly the same performance.\nPlease refer to https://pytorch.org/docs/stable/notes/randomness.html#reproducibility for more details about reproducibility.</p>\n<h4>Training Loss</h4>\n<p>The loss over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)</p>\n<p><img alt=\"Training loss over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_train_4-3.png\"/></p>\n<h4>Validation Dice</h4>\n<p>The mean dice score over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)</p>\n<p><img alt=\"Validation mean dice score over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_validation_4-3.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">133.93</td>\n<td style=\"text-align: center;\">43.41</td>\n<td style=\"text-align: center;\">35.65</td>\n<td style=\"text-align: center;\">26.63</td>\n<td style=\"text-align: center;\">3.09</td>\n<td style=\"text-align: center;\">3.76</td>\n<td style=\"text-align: center;\">5.03</td>\n<td style=\"text-align: center;\">1.63</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">54611.72</td>\n<td style=\"text-align: center;\">19240.66</td>\n<td style=\"text-align: center;\">16104.8</td>\n<td style=\"text-align: center;\">11443.57</td>\n<td style=\"text-align: center;\">2.84</td>\n<td style=\"text-align: center;\">3.39</td>\n<td style=\"text-align: center;\">4.77</td>\n<td style=\"text-align: center;\">1.68</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.6.1+cuda12.0\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.1\n - GPU models and configuration: A100 80G</p>\n<h3>Searched Architecture Visualization</h3>\n<p>Users can install Graphviz for visualization of searched architectures (needed in <a href=\"https://github.com/Project-MONAI/tutorials/blob/main/automl/DiNTS/decode_plot.py\">decode_plot.py</a>). The edges between nodes indicate global structure, and numbers next to edges represent different operations in the cell searching space. An example of searched architecture is shown as follows:</p>\n<p><img alt=\"Example of Searched Architecture\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_searched_arch_example_1.png\"/></p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute model searching:</h4>\n<pre><code>python -m scripts.search run --config_file configs/search.yaml\n</code></pre>\n<h4>Execute multi-GPU model searching (recommended):</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m scripts.search run --config_file configs/search.yaml\n</code></pre>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train.yaml','configs/multi_gpu_train.yaml']\"\n</code></pre>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.yaml','configs/evaluate.yaml']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.yaml\n</code></pre>\n<h4>Export checkpoint for TorchScript:</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.yaml\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.yaml  --precision &lt;fp32/fp16&gt; --use_trace \"True\" --dynamic_batchsize \"[1, 4, 8]\" --converter_kwargs \"{'truncate_long_and_double':True, 'torch_executed_ops': ['aten::upsample_trilinear3d']}\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.yaml', 'configs/inference_trt.yaml']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pancreas_ct_dints_segmentation/tree/0.5.1",
        "changelog": {
            "0.5.1": "update to huggingface hosting",
            "0.5.0": "use monai 1.4 and update large files",
            "0.4.9": "update to use monai 1.3.1",
            "0.4.8": "add load_pretrain flag for infer",
            "0.4.7": "add missing yaml lib requirement in metadata",
            "0.4.6": "add checkpoint loader for infer",
            "0.4.5": "set image_only to False",
            "0.4.4": "update the benchmark results of TensorRT",
            "0.4.3": "add support for TensorRT conversion and inference",
            "0.4.2": "update search function to match monai 1.2",
            "0.4.1": "fix the wrong GPU index issue of multi-node",
            "0.4.0": "remove error dollar symbol in readme",
            "0.3.9": "add cpu ram requirement in readme",
            "0.3.8": "add non-deterministic note",
            "0.3.7": "re-train model with updated dints implementation",
            "0.3.6": "black autofix format and add name tag",
            "0.3.5": "restructure readme to match updated template",
            "0.3.4": "correct typos",
            "0.3.3": "update learning rate and readme",
            "0.3.2": "update to use monai 1.0.1",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.1": "fix data type issue in searching/training configurations",
            "0.1.0": "complete the model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "brats_mri_segmentation": {
        "model_name": "BraTS MRI segmentation",
        "description": "A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data",
        "authors": "MONAI team",
        "papers": [
            "Myronenko, Andriy. '3D MRI brain tumor segmentation using autoencoder regularization.' International MICCAI Brainlesion Workshop. Springer, Cham, 2018. https://arxiv.org/abs/1810.11654"
        ],
        "version": "0.5.3",
        "model_id": "brats_mri_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data.</p>\n<p>The model is trained to segment 3 nested subregions of primary brain tumors (gliomas): the \"enhancing tumor\" (ET), the \"tumor core\" (TC), the \"whole tumor\" (WT) based on 4 aligned input MRI scans (T1c, T1, T2, FLAIR).\n- The ET is described by areas that show hyper intensity in T1c when compared to T1, but also when compared to \"healthy\" white matter in T1c.\n- The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (fluid-filled) and the non-enhancing (solid) parts of the tumor.\n-  The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edema (ED), which is typically depicted by hyper-intense signal in FLAIR.</p>\n<p><img alt=\"Model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_brain_mri_segmentation_workflow.png\"/></p>\n<h2>Data</h2>\n<p>The training data is from the <a href=\"https://www.med.upenn.edu/sbia/brats2018.html\">Multimodal Brain Tumor Segmentation Challenge (BraTS) 2018</a>.</p>\n<ul>\n<li>Target: 3 tumor subregions</li>\n<li>Task: Segmentation</li>\n<li>Modality: MRI</li>\n<li>Size: 285 3D volumes (4 channels each)</li>\n</ul>\n<p>The provided labelled data was partitioned, based on our own split, into training (200 studies), validation (42 studies) and testing (43 studies) datasets.</p>\n<h3>Preprocessing</h3>\n<p>The data list/split can be created with the script <code>scripts/prepare_datalist.py</code>.</p>\n<pre><code>python scripts/prepare_datalist.py --path your-brats18-dataset-path\n</code></pre>\n<h2>Training configuration</h2>\n<p>This model utilized a similar approach described in 3D MRI brain tumor segmentation using autoencoder regularization, which was a winning method in BraTS2018 [1]. The training was performed with the following:</p>\n<ul>\n<li>GPU: At least 16GB of GPU memory.</li>\n<li>Actual Model Input: 224 x 224 x 144</li>\n<li>AMP: True</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-4</li>\n<li>Loss: DiceLoss</li>\n</ul>\n<h2>Input</h2>\n<p>4 channel aligned MRIs at 1 x 1 x 1 mm\n- T1c\n- T1\n- T2\n- FLAIR</p>\n<h2>Output</h2>\n<p>3 channels\n- Label 0: TC tumor subregion\n- Label 1: WT tumor subregion\n- Label 2: ET tumor subregion</p>\n<h2>Performance</h2>\n<p>Dice score was used for evaluating the performance of the model. This model achieved Dice scores on the validation data of:\n- Tumor core (TC): 0.8559\n- Whole tumor (WT): 0.9026\n- Enhancing tumor (ET): 0.7905\n- Average: 0.8518</p>\n<p>Please note that this bundle is non-deterministic because of the trilinear interpolation used in the network. Therefore, reproducing the training process may not get exactly the same performance.\nPlease refer to https://pytorch.org/docs/stable/notes/randomness.html#reproducibility for more details about reproducibility.</p>\n<h4>Training Loss and Dice</h4>\n<p><img alt=\"A graph showing the training loss and the mean dice over 300 epochs\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brats_mri_segmentation_train.png\"/></p>\n<h4>Validation Dice</h4>\n<p><img alt=\"A graph showing the validation mean dice over 300 epochs\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brats_mri_segmentation_val.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>brats_mri_segmentation</code> bundle supports acceleration with TensorRT through the ONNX-TensorRT method. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">5.49</td>\n<td style=\"text-align: center;\">4.36</td>\n<td style=\"text-align: center;\">2.35</td>\n<td style=\"text-align: center;\">2.09</td>\n<td style=\"text-align: center;\">1.26</td>\n<td style=\"text-align: center;\">2.34</td>\n<td style=\"text-align: center;\">2.63</td>\n<td style=\"text-align: center;\">2.09</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">592.01</td>\n<td style=\"text-align: center;\">434.59</td>\n<td style=\"text-align: center;\">395.73</td>\n<td style=\"text-align: center;\">394.93</td>\n<td style=\"text-align: center;\">1.36</td>\n<td style=\"text-align: center;\">1.50</td>\n<td style=\"text-align: center;\">1.50</td>\n<td style=\"text-align: center;\">1.10</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>Currently, the only available method to accelerate this model is through ONNX-TensorRT. However, the Torch-TensorRT method is under development and will be available in the near future.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.0\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle trt_export --net_id network_def \\\n--filepath models/model_trt.ts --ckpt_file models/model.pt \\\n--meta_file configs/metadata.json --config_file configs/inference.json \\\n--precision &lt;fp32/fp16&gt; --input_shape \"[1, 4, 240, 240, 160]\" --use_onnx \"True\" \\\n--use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Myronenko, Andriy. \"3D MRI brain tumor segmentation using autoencoder regularization.\" International MICCAI Brainlesion Workshop. Springer, Cham, 2018. https://arxiv.org/abs/1810.11654.</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/brats_mri_segmentation/tree/0.5.3",
        "changelog": {
            "0.5.3": "update to huggingface hosting",
            "0.5.2": "use monai 1.4 and update large files",
            "0.5.1": "update to use monai 1.3.1",
            "0.5.0": "add load_pretrain flag for infer",
            "0.4.9": "add checkpoint loader for infer",
            "0.4.8": "fix the wrong GPU index issue of multi-node",
            "0.4.7": "enhance prepare datalist file",
            "0.4.6": "add dataset dir example",
            "0.4.5": "update ONNX-TensorRT descriptions",
            "0.4.4": "update error links",
            "0.4.3": "add the ONNX-TensorRT way of model conversion",
            "0.4.2": "fix mgpu finalize issue",
            "0.4.1": "add non-deterministic note",
            "0.4.0": "adapt to BundleWorkflow interface",
            "0.3.9": "black autofix format and add name tag",
            "0.3.8": "modify dataset key name",
            "0.3.7": "restructure readme to match updated template",
            "0.3.6": "added train/val graphs",
            "0.3.5": "update prepare datalist function",
            "0.3.4": "update output format of inference",
            "0.3.3": "update to use monai 1.0.1",
            "0.3.2": "enhance readme on commands example",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.1": "fix network_data_format error",
            "0.2.0": "unify naming",
            "0.1.1": "update for MetaTensor",
            "0.1.0": "complete the model package"
        }
    },
    "spleen_deepedit_annotation": {
        "model_name": "Spleen DeepEdit annotation",
        "description": "This is a pre-trained model for 3D segmentation of the spleen organ from CT images using DeepEdit.",
        "authors": "MONAI team",
        "papers": [
            "Sakinis, Tomas, et al. 'Interactive segmentation of medical images through fully convolutional neural networks.' arXiv preprint arXiv:1903.08205 (2019)"
        ],
        "version": "0.5.7",
        "model_id": "spleen_deepedit_annotation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for 3D segmentation of the spleen organ from CT images using DeepEdit.</p>\n<p>DeepEdit is an algorithm that combines the power of two models in one single architecture. It allows the user to perform inference as a standard segmentation method (i.e., UNet) and interactively segment part of an image using clicks [2]. DeepEdit aims to facilitate the user experience and, at the same time, develop new active learning techniques.</p>\n<p>The model was trained on 32 images and validated on 9 images.</p>\n<h2>Data</h2>\n<p>The training dataset is the Spleen Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.</p>\n<ul>\n<li>Target: Spleen</li>\n<li>Modality: CT</li>\n<li>Size: 61 3D volumes (41 Training + 20 Testing)</li>\n<li>Source: Memorial Sloan Kettering Cancer Center</li>\n<li>Challenge: Large-ranging foreground size</li>\n</ul>\n<h2>Training configuration</h2>\n<p>The training as performed with the following:\n- GPU: at least 12GB of GPU memory\n- Actual Model Input: 128 x 128 x 128\n- AMP: True\n- Optimizer: Adam\n- Learning Rate: 1e-4\n- Loss: DiceCELoss</p>\n<h3>Input</h3>\n<p>Three channels\n- CT image\n- Spleen Segment\n- Background Segment</p>\n<h3>Output</h3>\n<p>Two channels\n- Label 1: spleen\n- Label 0: everything else</p>\n<h2>Performance</h2>\n<p>Dice score is used for evaluating the performance of the model. This model achieves a dice score of 0.97, depending on the number of simulated clicks.</p>\n<h4>Training Dice</h4>\n<p><img alt=\"A graph showing the train dice over 90 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_train_dice_v2.png\"/></p>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the training loss over 90 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_train_loss_v2.png\"/></p>\n<h4>Validation Dice</h4>\n<p><img alt=\"A graph showing the validation dice over 90 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_val_dice_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>spleen_deepedit_annotation</code> bundle supports acceleration with TensorRT through the ONNX-TensorRT method. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">147.52</td>\n<td style=\"text-align: center;\">40.32</td>\n<td style=\"text-align: center;\">28.87</td>\n<td style=\"text-align: center;\">11.94</td>\n<td style=\"text-align: center;\">3.66</td>\n<td style=\"text-align: center;\">5.11</td>\n<td style=\"text-align: center;\">12.36</td>\n<td style=\"text-align: center;\">3.38</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">1292.39</td>\n<td style=\"text-align: center;\">1204.62</td>\n<td style=\"text-align: center;\">1168.09</td>\n<td style=\"text-align: center;\">1149.88</td>\n<td style=\"text-align: center;\">1.07</td>\n<td style=\"text-align: center;\">1.11</td>\n<td style=\"text-align: center;\">1.12</td>\n<td style=\"text-align: center;\">1.05</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>Currently, the only available method to accelerate this model is through ONNX-TensorRT. However, the Torch-TensorRT method is under development and will be available in the near future.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.0\n - GPU models and configuration: A100 80G</p>\n<h3>Memory Consumption</h3>\n<ul>\n<li>Dataset Manager: CacheDataset</li>\n<li>Data Size: 61 3D Volumes</li>\n<li>Cache Rate: 1.0</li>\n<li>Single GPU - System RAM Usage: 8.2G</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<p>Optionally, clicks can be added to the data dictionary that is passed to the preprocessing transforms. The add keys are defined in <code>label_names</code> in <code>configs/inference.json</code>, and the corresponding values are the point coordinates. The following is an example of a data dictionary:</p>\n<pre><code>{\"image\": \"example.nii.gz\", \"background\": [], \"spleen\": [[I1, J1, K1], [I2, J2, K2]]}\n</code></pre>\n<p>where <strong>[I1,J1,K1]</strong> and <strong>[I2,J2,K2]</strong> are the point coordinates.</p>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle trt_export --net_id network_def \\\n--filepath models/model_trt.ts --ckpt_file models/model.pt \\\n--meta_file configs/metadata.json --config_file configs/inference.json \\\n--precision &lt;fp32/fp16&gt;  --use_onnx \"True\" --use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Diaz-Pinto, Andres, et al. DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images. MICCAI Workshop on Data Augmentation, Labelling, and Imperfections. MICCAI 2022.</p>\n<p>[2] Diaz-Pinto, Andres, et al. \"MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images.\" arXiv preprint arXiv:2203.12362 (2022).</p>\n<p>[3] Sakinis, Tomas, et al. \"Interactive segmentation of medical images through fully convolutional neural networks.\" arXiv preprint arXiv:1903.08205 (2019).</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/spleen_deepedit_annotation/tree/0.5.7",
        "changelog": {
            "0.5.7": "update to huggingface hosting",
            "0.5.6": "use monai 1.4 and update large files",
            "0.5.5": "update to use monai 1.3.1",
            "0.5.4": "add load_pretrain flag for infer",
            "0.5.3": "update to use monai 1.3.0",
            "0.5.2": "update the checkpoint loader logic for inference",
            "0.5.1": "add option to validate at training start, and I/O param entries",
            "0.5.0": "enable finetune and early stop",
            "0.4.9": "fix orientation issue on clicks",
            "0.4.8": "Add infer transforms to manage clicks from viewer",
            "0.4.7": "fix the wrong GPU index issue of multi-node",
            "0.4.6": "update to use rc7 which solves dynunet issue",
            "0.4.5": "remove error dollar symbol in readme",
            "0.4.4": "add RAM comsumption with Cachedataset",
            "0.4.3": "update ONNX-TensorRT descriptions",
            "0.4.2": "deterministic retrain benchmark, update fig links",
            "0.4.1": "add the ONNX-TensorRT way of model conversion",
            "0.4.0": "fix mgpu finalize issue",
            "0.3.9": "enable deterministic training",
            "0.3.8": "adapt to BundleWorkflow interface",
            "0.3.7": "add name tag",
            "0.3.6": "restructure readme to match updated template",
            "0.3.5": "update metric in metadata",
            "0.3.4": "add validate.json file and dice score in readme",
            "0.3.3": "update to use monai 1.0.1",
            "0.3.2": "enhance readme on commands example",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.0": "complete the model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "swin_unetr_btcv_segmentation": {
        "model_name": "Swin UNETR BTCV segmentation",
        "description": "A pre-trained model for volumetric (3D) multi-organ segmentation from CT image",
        "authors": "MONAI team",
        "papers": [
            "Hatamizadeh, Ali, et al. 'Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266 (2022). https://arxiv.org/abs/2201.01266.",
            "Tang, Yucheng, et al. 'Self-supervised pre-training of swin transformers for 3d medical image analysis. arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791."
        ],
        "version": "0.5.7",
        "model_id": "swin_unetr_btcv_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained Swin UNETR [1,2] for volumetric (3D) multi-organ segmentation using CT images from Beyond the Cranial Vault (BTCV) Segmentation Challenge dataset [3].</p>\n<p><img alt=\"model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_workflow_v1.png\"/></p>\n<h2>Data</h2>\n<p>The training data is from the <a href=\"https://www.synapse.org/#!Synapse:syn3193805/wiki/89480/\">BTCV dataset</a> (Register through <code>Synapse</code> and download the <code>Abdomen/RawData.zip</code>).</p>\n<ul>\n<li>Target: Multi-organs</li>\n<li>Task: Segmentation</li>\n<li>Modality: CT</li>\n<li>Size: 30 3D volumes (24 Training + 6 Testing)</li>\n</ul>\n<h3>Preprocessing</h3>\n<p>The dataset format needs to be redefined using the following commands:</p>\n<pre><code>unzip RawData.zip\nmv RawData/Training/img/ RawData/imagesTr\nmv RawData/Training/label/ RawData/labelsTr\nmv RawData/Testing/img/ RawData/imagesTs\n</code></pre>\n<h2>Training configuration</h2>\n<p>The training as performed with the following:\n- GPU: At least 32GB of GPU memory\n- Actual Model Input: 96 x 96 x 96\n- AMP: True\n- Optimizer: Adam\n- Learning Rate: 2e-4</p>\n<h3>Memory Consumption</h3>\n<ul>\n<li>Dataset Manager: CacheDataset</li>\n<li>Data Size: 30 samples</li>\n<li>Cache Rate: 1.0</li>\n<li>Single GPU - System RAM Usage: 5.8G</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h3>Input</h3>\n<p>1 channel\n- CT image</p>\n<h3>Output</h3>\n<p>14 channels:\n- 0: Background\n- 1: Spleen\n- 2: Right Kidney\n- 3: Left Kideny\n- 4: Gallbladder\n- 5: Esophagus\n- 6: Liver\n- 7: Stomach\n- 8: Aorta\n- 9: IVC\n- 10: Portal and Splenic Veins\n- 11: Pancreas\n- 12: Right adrenal gland\n- 13: Left adrenal gland</p>\n<h2>Performance</h2>\n<p>Dice score was used for evaluating the performance of the model. This model achieves a mean dice score of 0.82</p>\n<h4>Training Loss</h4>\n<p><img alt=\"The figure shows the training loss curve for 10K iterations.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_train_loss_v2.png\"/></p>\n<h4>Validation Dice</h4>\n<p><img alt=\"A graph showing the validation mean Dice for 5000 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_val_dice_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>swin_unetr</code> bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">123.64</td>\n<td style=\"text-align: center;\">123.77</td>\n<td style=\"text-align: center;\">93.22</td>\n<td style=\"text-align: center;\">42.87</td>\n<td style=\"text-align: center;\">1.00</td>\n<td style=\"text-align: center;\">1.33</td>\n<td style=\"text-align: center;\">2.88</td>\n<td style=\"text-align: center;\">2.89</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">5102</td>\n<td style=\"text-align: center;\">4895</td>\n<td style=\"text-align: center;\">2863</td>\n<td style=\"text-align: center;\">2835</td>\n<td style=\"text-align: center;\">1.04</td>\n<td style=\"text-align: center;\">1.78</td>\n<td style=\"text-align: center;\">1.80</td>\n<td style=\"text-align: center;\">1.73</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Hatamizadeh, Ali, et al. \"Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images.\" arXiv preprint arXiv:2201.01266 (2022). https://arxiv.org/abs/2201.01266.</p>\n<p>[2] Tang, Yucheng, et al. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791.</p>\n<p>[3] Landman B, et al. \"MICCAI multi-atlas labeling beyond the cranial vaultworkshop and challenge.\" In Proc. of the MICCAI Multi-Atlas Labeling Beyond Cranial VaultWorkshop Challenge 2015 Oct (Vol. 5, p. 12).</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/swin_unetr_btcv_segmentation/tree/0.5.7",
        "changelog": {
            "0.5.7": "update to huggingface hosting",
            "0.5.6": "update tensorrt benchmark results",
            "0.5.5": "enable tensorrt",
            "0.5.4": "update to use monai 1.3.1",
            "0.5.3": "add load_pretrain flag for infer",
            "0.5.2": "add checkpoint loader for infer",
            "0.5.1": "remove meta_dict usage",
            "0.5.0": "fix the wrong GPU index issue of multi-node",
            "0.4.9": "remove error dollar symbol in readme",
            "0.4.8": "add RAM usage with CacheDataset",
            "0.4.7": "deterministic retrain benchmark",
            "0.4.6": "fix mgpu finalize issue",
            "0.4.5": "enable deterministic training",
            "0.4.4": "update numbers",
            "0.4.3": "adapt to BundleWorkflow interface",
            "0.4.2": "fix train params of use_checkpoint",
            "0.4.1": "update params to supprot torch.jit.trace torchscript conversion",
            "0.4.0": "add name tag",
            "0.3.9": "use ITKreader to avoid mass logs at image loading",
            "0.3.8": "restructure readme to match updated template",
            "0.3.7": "Update metric in metadata",
            "0.3.6": "Update ckpt drive link",
            "0.3.5": "Update figure and benchmarking",
            "0.3.4": "Update figure link in readme",
            "0.3.3": "Update, verify MONAI 1.0.1 and Pytorch 1.13.0",
            "0.3.2": "enhance readme on commands example",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.0": "complete the model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "ventricular_short_axis_3label": {
        "model_name": "Ventricular short axis 3 label segmentation",
        "description": "This network segments full cycle short axis images of the ventricles, labelling LV pool separate from myocardium and RV pool",
        "authors": "Eric Kerfoot",
        "papers": [],
        "version": "0.3.4",
        "model_id": "ventricular_short_axis_3label",
        "readme": "<h1>3 Label Ventricular Segmentation</h1>\n<p>This network segments cardiac ventricle in 2D short axis MR images. The left ventricular pool is class 1, left ventricular myocardium class 2, and right ventricular pool class 3. Full cycle segmentation with this network is possible although much of the training data is composed of segmented end-diastole images. The input to the network is single 2D images thus segmenting whole time-dependent volumes consists of multiple inference operations.</p>\n<p>The network and training scheme are essentially identical to that described in:</p>\n<p><code>Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40</code></p>\n<h2>Data</h2>\n<p>The dataset used to train this network unfortunately cannot be made public as it contains unreleased image data from King's College London. Existing public datasets such as the<a href=\"http://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/\">Sunnybrook Cardiac Dataset</a> and <a href=\"https://www.creatis.insa-lyon.fr/Challenge/acdc/\">ACDC Challenge</a> set can be used to train a similar network.</p>\n<p>The <code>train.json</code> configuration assumes all data is stored in a single npz file with keys \"images\" and \"segs\" containing respectively the raw image data and their accompanying segmentations. The given network was training with stored volumes with shapes <code>(9095, 256, 256)</code> thus other data of differing spatial dimensions must be cropped to <code>(256, 256)</code> or zero-padded to that size. For the training data this was done as a preprocessing step but the original pixel values are otherwise unchanged from their original forms.</p>\n<h2>Training</h2>\n<p>The network is trained with this data in conjunction with a series of augmentations for regularisation and robustness. Many of the original images are smaller than the expected size of <code>(256, 256)</code> and so were zero-padded, the network can thus be expected to be robust against large amounts of empty space in the inputs. Rotation and zooming is also applied to force the network to learn different sizes and orientations of the heart in the field of view.</p>\n<p>Free-form deformation is applied to vary the shape of the heart and its surrounding tissues which mimics to a degree deformation like what would be observed through the cardiac cycle. This of course does not replicate the heart moving through plane during the cycle or represent other observed changes but does provide enough variation that full-cycle segmentation is generally acceptable.</p>\n<p>Smooth fields are used to vary contrast and intensity in localised regions to simulate some of the variation in image quality caused by acquisition artefacts. Guassian noise is also added to simulate poor quality acquisition. These together force the network to learn to deal with a wider variation of image quality and partially to account for the difference between scanner vendors.</p>\n<p>Training is invoked with the following command line:</p>\n<pre><code class=\"language-sh\">python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --bundle_root .\n</code></pre>\n<p>The dataset file is assumed to be <code>allimages3label.npz</code> but can be changed by setting the <code>dataset_file</code> value to your own file.</p>\n<h2>Inference</h2>\n<p>An example notebook <a href=\"./visualise.ipynb\">visualise.ipynb</a> demonstrates using the network directly with input images. Inference of 3D volumes only can be accomplished with the <code>inference.json</code> configuration:</p>\n<pre><code class=\"language-sh\">python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf --dataset_dir dataset --output_dir ./output/ --bundle_root .\n</code></pre>\n<h1>License</h1>\n<p>This model is released under the MIT License. The license file is included with the model.</p>",
        "download_url": "https://huggingface.co/MONAI/ventricular_short_axis_3label/tree/0.3.4",
        "changelog": {
            "0.3.4": "update to huggingface hosting",
            "0.3.3": "update AddChanneld with EnsureChannelFirstd",
            "0.3.2": "add name tag",
            "0.3.1": "modify dataset key name",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.0": "Initial version"
        }
    },
    "mednist_gan": {
        "model_name": "MedNIST GAN",
        "description": "This example of a GAN generator produces hand xray images like those in the MedNIST dataset",
        "authors": "MONAI Team",
        "papers": [],
        "version": "0.4.3",
        "model_id": "mednist_gan",
        "readme": "<h1>MedNIST GAN Hand Model</h1>\n<p>This model is a generator for creating images like the Hand category in the MedNIST dataset. It was trained as a GAN and accepts random values as inputs to produce an image output. The <code>train.json</code> file describes the training process along with the definition of the discriminator network used, and is based on the <a href=\"https://github.com/Project-MONAI/tutorials/blob/main/modules/mednist_GAN_workflow_dict.ipynb\">MONAI GAN tutorials</a>.</p>\n<p>This is a demonstration network meant to just show the training process for this sort of network with MONAI, its outputs are not particularly good and are of the same tiny size as the images in MedNIST. The training process was very short so a network with a longer training time would produce better results.</p>\n<h3>Downloading the Dataset</h3>\n<p>Download the dataset from <a href=\"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\">here</a> and extract the contents to a convenient location.</p>\n<p>The MedNIST dataset was gathered from several sets from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions\">TCIA</a>,\n<a href=\"http://rsnachallenges.cloudapp.net/competitions/4\">the RSNA Bone Age Challenge</a>,\nand <a href=\"https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest\">the NIH Chest X-ray dataset</a>.</p>\n<p>The dataset is kindly made available by <a href=\"https://www.mayo.edu/research/labs/radiology-informatics/overview\">Dr. Bradley J. Erickson M.D., Ph.D.</a> (Department of Radiology, Mayo Clinic)\nunder the Creative Commons <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY-SA 4.0 license</a>.</p>\n<p>If you use the MedNIST dataset, please acknowledge the source.</p>\n<h3>Training</h3>\n<p>Assuming the current directory is the bundle directory, and the dataset was extracted to the directory <code>./MedNIST</code>, the following command will train the network for 50 epochs:</p>\n<pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --bundle_root .\n</code></pre>\n<p>Not also the output from the training will be placed in the <code>models</code> directory but will not overwrite the <code>model.pt</code> file that may be there already. You will have to manually rename the most recent checkpoint file to <code>model.pt</code> to use the inference script mentioned below after checking the results are correct. This saved checkpoint contains a dictionary with the generator weights stored as <code>model</code> and omits the discriminator.</p>\n<p>Another feature in the training file is the addition of sigmoid activation to the network by modifying it's structure at runtime. This is done with a line in the <code>training</code> section calling <code>add_module</code> on a layer of the network. This works best for training although the definition of the model now doesn't strictly match what it is in the <code>generator</code> section.</p>\n<p>The generator and discriminator networks were both trained with the <code>Adam</code> optimizer with a learning rate of 0.0002 and <code>betas</code> values <code>[0.5, 0.999]</code>. These have been emperically found to be good values for the optimizer and this GAN problem.</p>\n<h3>Inference</h3>\n<p>The included <code>inference.json</code> generates a set number of png samples from the network and saves these to the directory <code>./outputs</code>. The output directory can be changed by setting the <code>output_dir</code> value, and the number of samples changed by setting the <code>num_samples</code> value. The following command line assumes it is invoked in the bundle directory:</p>\n<pre><code>python -m monai.bundle run inferring --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf --bundle_root .\n</code></pre>\n<p>Note this script uses postprocessing to apply the sigmoid activation the model's outputs and to save the results to image files.</p>\n<h3>Export</h3>\n<p>The generator can be exported to a Torchscript bundle with the following:</p>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath mednist_gan.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json\n</code></pre>\n<p>The model can be loaded without MONAI code after this operation. For example, an image can be generated from a set of random values with:</p>\n<pre><code class=\"language-python\">import torch\nnet = torch.jit.load(\"mednist_gan.ts\")\nlatent = torch.rand(1, 64)\nimg = net(latent)  # (1,1,64,64)\n</code></pre>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/mednist_gan/tree/0.4.3",
        "changelog": {
            "0.4.3": "update to huggingface hosting and compatible with py3.10",
            "0.4.2": "add name tag",
            "0.4.1": "fix license Copyright error",
            "0.4.0": "update license files",
            "0.3.0": "Update for 1.0",
            "0.2.0": "Unify naming",
            "0.1.0": "Initial version"
        }
    },
    "pathology_tumor_detection": {
        "model_name": "Pathology tumor detection",
        "description": "A pre-trained model for metastasis detection on Camelyon 16 dataset.",
        "authors": "MONAI team",
        "papers": [
            ""
        ],
        "version": "0.6.3",
        "model_id": "pathology_tumor_detection",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for automated detection of metastases in whole-slide histopathology images.</p>\n<p>The model is trained based on ResNet18 [1] with the last fully connected layer replaced by a 1x1 convolution layer.\n<img alt=\"Diagram showing the flow from model input, through the model architecture, and to model output\" src=\"http://developer.download.nvidia.com/assets/Clara/Images/clara_pt_pathology_metastasis_detection_workflow.png\"/></p>\n<h2>Data</h2>\n<p>All the data used to train, validate, and test this model is from <a href=\"https://camelyon16.grand-challenge.org/\">Camelyon-16 Challenge</a>. You can download all the images for \"CAMELYON16\" data set from various sources listed <a href=\"https://camelyon17.grand-challenge.org/Data/\">here</a>.</p>\n<p>Location information for training/validation patches (the location on the whole slide image where patches are extracted) are adopted from <a href=\"https://github.com/baidu-research/NCRF/tree/master/coords\">NCRF/coords</a>.</p>\n<p>Annotation information are adopted from <a href=\"https://github.com/baidu-research/NCRF/tree/master/jsons\">NCRF/jsons</a>.</p>\n<ul>\n<li>Target: Tumor</li>\n<li>Task: Detection</li>\n<li>Modality: Histopathology</li>\n<li>Size: 270 WSIs for training/validation, 48 WSIs for testing</li>\n</ul>\n<h3>Preprocessing</h3>\n<p>This bundle expects the training/validation data (whole slide images) reside in a <code>{dataset_dir}/training/images</code>. By default <code>dataset_dir</code> is pointing to <code>/workspace/data/medical/pathology/</code> You can modify <code>dataset_dir</code> in the bundle config files to point to a different directory.</p>\n<p>To reduce the computation burden during the inference, patches are extracted only where there is tissue and ignoring the background according to a tissue mask. Please also create a directory for prediction output. By default <code>output_dir</code> is set to <code>eval</code> folder under the bundle root.</p>\n<p>Please refer to \"Annotation\" section of <a href=\"https://camelyon17.grand-challenge.org/Data/\">Camelyon challenge</a> to prepare ground truth images, which are needed for FROC computation. By default, this data set is expected to be at <code>/workspace/data/medical/pathology/ground_truths</code>. But it can be modified in <code>evaluate_froc.sh</code>.</p>\n<h2>Training configuration</h2>\n<p>The training was performed with the following:</p>\n<ul>\n<li>Config file: train.config</li>\n<li>GPU: at least 16 GB of GPU memory.</li>\n<li>Actual Model Input: 224 x 224 x 3</li>\n<li>AMP: True</li>\n<li>Optimizer: Novograd</li>\n<li>Learning Rate: 1e-3</li>\n<li>Loss: BCEWithLogitsLoss</li>\n<li>Whole slide image reader: cuCIM (if running on Windows or Mac, please install <code>OpenSlide</code> on your system and change <code>wsi_reader</code> to \"OpenSlide\")</li>\n</ul>\n<h3>Pretrained Weights</h3>\n<p>By setting the <code>\"pretrained\"</code> parameter of <code>TorchVisionFCModel</code> in the config file to <code>true</code>, ImageNet pre-trained weights will be used for training. Please note that these weights are for non-commercial use. Each user is responsible for checking the content of the models/datasets and the applicable licenses and determining if suitable for the intended use. In order to use other pretrained weights, you can use <code>CheckpointLoader</code> in train handlers section as the first handler:</p>\n<pre><code class=\"language-json\">{\n    \"_target_\": \"CheckpointLoader\",\n    \"load_path\": \"$@bundle_root + '/pretrained_resnet18.pth'\",\n    \"strict\": false,\n    \"load_dict\": {\n        \"model_new\": \"@network\"\n    }\n}\n</code></pre>\n<h3>Input</h3>\n<p>The training pipeline is a json file (dataset.json) which includes path to each WSI, the location and the label information for each training patch.</p>\n<h3>Output</h3>\n<p>A probability number of the input patch being tumor or normal.</p>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues in traning, you can lower the <code>batch_size</code> in the configurations to reduce the System RAM requirements.</p>\n<h3>Inference on a WSI</h3>\n<p>Inference is performed on WSI in a sliding window manner with specified stride. A foreground mask is needed to specify the region where the inference will be performed on, given that background region which contains no tissue at all can occupy a significant portion of a WSI. Output of the inference pipeline is a probability map of size 1/stride of original WSI size.</p>\n<h3>Note on determinism</h3>\n<p>By default this bundle use a deterministic approach to make the results reproducible. However, it comes at a cost of performance loss. Thus if you do not care about reproducibility, you can have a performance gain by replacing <code>\"$monai.utils.set_determinism\"</code> line with <code>\"$setattr(torch.backends.cudnn, 'benchmark', True)\"</code> in initialize section of training configuration (<code>configs/train.json</code> and <code>configs/multi_gpu_train.json</code> for single GPU and multi-GPU training respectively).</p>\n<h2>Performance</h2>\n<p>FROC score is used for evaluating the performance of the model. After inference is done, <code>evaluate_froc.sh</code> needs to be run to evaluate FROC score based on predicted probability map (output of inference) and the ground truth tumor masks.\nUsing an internal pretrained weights for ResNet18, this model deterministically achieves the 0.90 accuracy on validation patches, and FROC of 0.72 on the 48 Camelyon testing data that have ground truth annotations available.</p>\n<p><img alt=\"A Graph showing Train Acc, Train Loss, and Validation Acc\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_tumor_detection_train_and_val_metrics_v5.png\"/></p>\n<p>The <code>pathology_tumor_detection</code> bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<p>Please notice that the benchmark results are tested on one WSI image since the images are too large to benchmark. And the inference time in the end-to-end line stands for one patch of the whole image.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">1.93</td>\n<td style=\"text-align: center;\">2.52</td>\n<td style=\"text-align: center;\">1.61</td>\n<td style=\"text-align: center;\">1.33</td>\n<td style=\"text-align: center;\">0.77</td>\n<td style=\"text-align: center;\">1.20</td>\n<td style=\"text-align: center;\">1.45</td>\n<td style=\"text-align: center;\">1.89</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">224.97</td>\n<td style=\"text-align: center;\">223.50</td>\n<td style=\"text-align: center;\">222.65</td>\n<td style=\"text-align: center;\">224.03</td>\n<td style=\"text-align: center;\">1.01</td>\n<td style=\"text-align: center;\">1.01</td>\n<td style=\"text-align: center;\">1.00</td>\n<td style=\"text-align: center;\">1.00</td>\n</tr>\n</tbody>\n</table>\n<p>Where:</p>\n<ul>\n<li><code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing</li>\n<li><code>end2end</code> means run the bundle end-to-end with the TensorRT based model.</li>\n<li><code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.</li>\n<li><code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.</li>\n<li><code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model</li>\n<li><code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</li>\n</ul>\n<p>This result is benchmarked under:</p>\n<ul>\n<li>TensorRT: 8.5.3+cuda11.8</li>\n<li>Torch-TensorRT Version: 1.4.0</li>\n<li>CPU Architecture: x86-64</li>\n<li>OS: ubuntu 20.04</li>\n<li>Python version:3.8.10</li>\n<li>CUDA version: 12.0</li>\n<li>GPU models and configuration: A100 80G</li>\n</ul>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<p><strong>Note:</strong> When using a container based on <a href=\"https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes\">PyTorch 24.0x</a>, you may encounter random NCCL timeout errors. To address this issue, consider the following adjustments:</p>\n<ul>\n<li>Reduce the <code>num_workers</code>: Decreasing the number of data loader workers can help minimize these errors.</li>\n<li>Set <code>pin_memory</code> to <code>False</code>: Disabling pinned memory may reduce the likelihood of timeouts.</li>\n<li>Switch to the <code>gloo</code> backend: As a workaround, you can set the distributed training backend to <code>gloo</code> to avoid NCCL-related timeouts.</li>\n</ul>\n<p>You can implement these settings by adding flags like <code>--train#dataloader#num_workers 0</code> or <code>--train#dataloader#pin_memory false</code>.</p>\n<h4>Execute inference</h4>\n<pre><code>CUDA_LAUNCH_BLOCKING=1 python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Evaluate FROC metric</h4>\n<pre><code>cd scripts &amp;&amp; source evaluate_froc.sh\n</code></pre>\n<h4>Export checkpoint to TorchScript file</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt; --dynamic_batchsize \"[1, 400, 600]\"\n</code></pre>\n<h4>Execute inference with the TensorRT model</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] He, Kaiming, et al, \"Deep Residual Learning for Image Recognition.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016. <a href=\"https://arxiv.org/pdf/1512.03385.pdf\">https://arxiv.org/pdf/1512.03385.pdf</a></p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pathology_tumor_detection/tree/0.6.3",
        "changelog": {
            "0.6.3": "update to huggingface hosting",
            "0.6.2": "enhance readme for nccl timout issue",
            "0.6.1": "fix multi-gpu issue",
            "0.6.0": "use monai 1.4 and update large files",
            "0.5.9": "update to use monai 1.3.1",
            "0.5.8": "update readme to add memory warning",
            "0.5.7": "update channel_def in metadata",
            "0.5.6": "fix the wrong GPU index issue of multi-node",
            "0.5.5": "modify mgpu logging level",
            "0.5.4": "retrain using an internal pretrained ResNet18",
            "0.5.3": "make the training bundle deterministic",
            "0.5.2": "update TensorRT descriptions",
            "0.5.1": "update the TensorRT part in the README file",
            "0.5.0": "add the command of executing inference with TensorRT models",
            "0.4.9": "adapt to BundleWorkflow interface",
            "0.4.8": "update the readme file with TensorRT convert",
            "0.4.7": "add name tag",
            "0.4.6": "modify dataset key name",
            "0.4.5": "update model weights and perfomance metrics",
            "0.4.4": "restructure readme to match updated template",
            "0.4.3": "fix wrong figure url",
            "0.4.2": "update metadata with new metrics",
            "0.4.1": "Fix inference print logger and froc",
            "0.4.0": "add lesion FROC calculation and wsi_reader",
            "0.3.3": "update to use monai 1.0.1",
            "0.3.2": "enhance readme on commands example",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.1": "fix location variable name change",
            "0.1.0": "initialize release of the bundle"
        }
    },
    "renalStructures_UNEST_segmentation": {
        "model_name": "Renal structures UNEST segmentation",
        "description": "A transformer-based model for renal segmentation from CT image",
        "authors": "Vanderbilt University + MONAI team",
        "papers": [
            "Tang, Yucheng, et al. 'Self-supervised pre-training of swin transformers for 3d medical image analysis. arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791."
        ],
        "version": "0.2.6",
        "model_id": "renalStructures_UNEST_segmentation",
        "readme": "<h1>Description</h1>\n<p>A pre-trained model for training and inferencing volumetric (3D) kidney substructures segmentation from contrast-enhanced CT images (Arterial/Portal Venous Phase). Training pipeline is provided to support model fine-tuning with bundle and MONAI Label active learning.</p>\n<p>A tutorial and release of model for kidney cortex, medulla and collecting system segmentation.</p>\n<p>Authors: Yinchi Zhou (yinchi.zhou@vanderbilt.edu) | Xin Yu (xin.yu@vanderbilt.edu) | Yucheng Tang (yuchengt@nvidia.com) |</p>\n<h1>Model Overview</h1>\n<p>A pre-trained UNEST base model [1] for volumetric (3D) renal structures segmentation using dynamic contrast enhanced arterial or venous phase CT images.</p>\n<h2>Data</h2>\n<p>The training data is from the [ImageVU RenalSeg dataset] from Vanderbilt University and Vanderbilt University Medical Center.\n(The training data is not public available yet).</p>\n<ul>\n<li>Target: Renal Cortex | Medulla | Pelvis Collecting System</li>\n<li>Task: Segmentation</li>\n<li>Modality: CT (Artrial | Venous phase)</li>\n<li>Size: 96 3D volumes</li>\n</ul>\n<p>The data and segmentation demonstration is as follow:</p>\n<p><img alt=\"\" src=\"./renal.png\"/> <br/></p>\n<h2>Method and Network</h2>\n<p>The UNEST model is a 3D hierarchical transformer-based semgnetation network.</p>\n<p>Details of the architecture:\n<img alt=\"\" src=\"./unest.png\"/> <br/></p>\n<h2>Training configuration</h2>\n<p>The training was performed with at least one 16GB-memory GPU.</p>\n<p>Actual Model Input: 96 x 96 x 96</p>\n<h2>Input and output formats</h2>\n<p>Input: 1 channel CT image</p>\n<p>Output: 4: 0:Background, 1:Renal Cortex, 2:Medulla, 3:Pelvicalyceal System</p>\n<h2>Performance</h2>\n<p>A graph showing the validation mean Dice for 5000 epochs.</p>\n<p><img alt=\"\" src=\"./val_dice.png\"/> <br/></p>\n<p>This model achieves the following Dice score on the validation data (our own split from the training dataset):</p>\n<p>Mean Valdiation Dice = 0.8523</p>\n<p>Note that mean dice is computed in the original spacing of the input data.</p>\n<h2>commands example</h2>\n<p>Download trained checkpoint model to ./model/model.pt:</p>\n<p>Add scripts component:  To run the workflow with customized components, PYTHONPATH should be revised to include the path to the customized component:</p>\n<pre><code>export PYTHONPATH=$PYTHONPATH:\"'&lt;path to the bundle root dir&gt;/scripts'\"\n\n</code></pre>\n<p>Execute Training:</p>\n<pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf\n</code></pre>\n<p>Execute inference:</p>\n<pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf\n</code></pre>\n<h2>More examples output</h2>\n<p><img alt=\"\" src=\"./demos.png\"/> <br/></p>\n<h1>Disclaimer</h1>\n<p>This is an example, not to be used for diagnostic purposes.</p>\n<h1>References</h1>\n<p>[1] Yu, Xin, Yinchi Zhou, Yucheng Tang et al. \"Characterizing Renal Structures with 3D Block Aggregate Transformers.\" arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf</p>\n<p>[2] Zizhao Zhang et al. \"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding.\" AAAI Conference on Artificial Intelligence (AAAI) 2022</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/renalStructures_UNEST_segmentation/tree/0.2.6",
        "changelog": {
            "0.2.6": "update to huggingface hosting",
            "0.2.5": "update large files",
            "0.2.4": "fix black 24.1 format error",
            "0.2.3": "update AddChanneld with EnsureChannelFirstd and remove meta_dict",
            "0.2.2": "add name tag",
            "0.2.1": "fix license Copyright error",
            "0.2.0": "update license files",
            "0.1.3": "Add training pipeline for fine-tuning models, support MONAI Label active learning",
            "0.1.2": "fixed the dimension in convolution according to MONAI 1.0 update",
            "0.1.1": "fixed the model state dict name",
            "0.1.0": "complete the model package"
        }
    },
    "wholeBrainSeg_Large_UNEST_segmentation": {
        "model_name": "Whole brain large UNEST segmentation",
        "description": "A 3D transformer-based model for whole brain segmentation from T1W MRI image",
        "authors": "Vanderbilt University + MONAI team",
        "papers": [
            "Xin, et al. Characterizing Renal Structures with 3D Block Aggregate Transformers. arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf"
        ],
        "version": "0.2.6",
        "model_id": "wholeBrainSeg_Large_UNEST_segmentation",
        "readme": "<h1>Description</h1>\n<p>Detailed whole brain segmentation is an essential quantitative technique in medical image analysis, which provides a non-invasive way of measuring brain regions from a clinical acquired structural magnetic resonance imaging (MRI).\nWe provide the pre-trained model for training and inferencing whole brain segmentation with 133 structures.\nTraining pipeline is provided to support active learning in MONAI Label and training with bundle.</p>\n<p>A tutorial and release of model for whole brain segmentation using the 3D transformer-based segmentation model UNEST.</p>\n<p>Authors:\nXin Yu (xin.yu@vanderbilt.edu)</p>\n<p>Yinchi Zhou (yinchi.zhou@vanderbilt.edu) | Yucheng Tang (yuchengt@nvidia.com)</p>\n<p align=\"center\">\n-------------------------------------------------------------------------------------\n</p>\n<p><img alt=\"\" src=\"./demo.png\"/> <br/></p>\n<p align=\"center\">\nFig.1 - The demonstration of T1w MRI images registered in MNI space and the whole brain segmentation labels with 133 classes</p>\n<h1>Model Overview</h1>\n<p>A pre-trained UNEST base model [1] for volumetric (3D) whole brain segmentation with T1w MR images.\nTo leverage information across embedded sequences, shifted window transformers\nare proposed for dense predictions and modeling multi-scale features. However, these\nattempts that aim to complicate the self-attention range often yield high computation\ncomplexity and data inefficiency. Inspired by the aggregation function in the nested\nViT, we propose a new design of a 3D U-shape medical segmentation model with\nNested Transformers (UNesT) hierarchically with the 3D block aggregation function,\nthat learn locality behaviors for small structures or small dataset. This design retains\nthe original global self-attention mechanism and achieves information communication\nacross patches by stacking transformer encoders hierarchically.</p>\n<p><img alt=\"\" src=\"./unest.png\"/> <br/></p>\n<p align=\"center\">\nFig.2 - The network architecture of UNEST Base model\n</p>\n<h2>Data</h2>\n<p>The training data is from the Vanderbilt University and Vanderbilt University Medical Center with public released OASIS and CANDI datsets.\nTraining and testing data are MRI T1-weighted (T1w) 3D volumes coming from 3 different sites. There are a total of 133 classes in the whole brain segmentation task.\nAmong 50 T1w MRI scans from Open Access Series on Imaging Studies (OASIS) (Marcus et al., 2007) dataset, 45 scans are used for training and the other 5 for validation.\n The testing cohort contains Colin27 T1w scan (Aubert-Broche et al., 2006) and 13 T1w MRI scans from the Child and Adolescent Neuro Development Initiative (CANDI)\n (Kennedy et al., 2012). All data are registered to the MNI space using the MNI305 (Evans et al., 1993) template and preprocessed follow the method in (Huo et al., 2019). Input images are randomly cropped to the size of 96  96  96.</p>\n<h3>Important</h3>\n<p>The brain MRI images for training are registered to Affine registration from the target image to the MNI305 template using NiftyReg.\nThe data should be in the MNI305 space before inference.</p>\n<p>If your images are already in MNI space, skip the registration step.</p>\n<p>You could use any resitration tool to register image to MNI space. Here is an example using ants.\nRegistration to MNI Space: Sample suggestion. E.g., use ANTS or other tools for registering T1 MRI image to MNI305 Space.</p>\n<pre><code>pip install antspyx\n\n#Sample ANTS registration\n\nimport ants\nimport sys\nimport os\n\nfixed_image = ants.image_read('&lt;fixed_image_path&gt;')\nmoving_image = ants.image_read('&lt;moving_image_path&gt;')\ntransform = ants.registration(fixed_image,moving_image,'Affine')\n\nreg3t = ants.apply_transforms(fixed_image,moving_image,transform['fwdtransforms'][0])\nants.image_write(reg3t,output_image_path)\n</code></pre>\n<h2>Training configuration</h2>\n<p>The training and inference was performed with at least one 24GB-memory GPU.</p>\n<p>Actual Model Input: 96 x 96 x 96</p>\n<h2>Input and output formats</h2>\n<p>Input: 1 channel T1w MRI image in MNI305 Space.</p>\n<h2>commands example</h2>\n<p>Download trained checkpoint model to ./model/model.pt:</p>\n<p>Add scripts component:  To run the workflow with customized components, PYTHONPATH should be revised to include the path to the customized component:</p>\n<pre><code>export PYTHONPATH=$PYTHONPATH: '&lt;path to the bundle root dir&gt;/'\n</code></pre>\n<p>Execute Training:</p>\n<pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf\n</code></pre>\n<p>Execute inference:</p>\n<pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf\n</code></pre>\n<h2>More examples output</h2>\n<p><img alt=\"\" src=\"./wholebrain.png\"/> <br/></p>\n<p align=\"center\">\nFig.3 - The output prediction comparison with variant and ground truth\n</p>\n<h2>Training/Validation Benchmarking</h2>\n<p>A graph showing the training accuracy for fine-tuning 600 epochs.</p>\n<p><img alt=\"\" src=\"./training.png\"/> <br/></p>\n<p>With 10 fine-tuned labels, the training process converges fast.</p>\n<h2>Complete ROI of the whole brain segmentation</h2>\n<p>133 brain structures are segmented.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">#1</th>\n<th style=\"text-align: left;\">#2</th>\n<th style=\"text-align: left;\">#3</th>\n<th style=\"text-align: left;\">#4</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">0:  background</td>\n<td style=\"text-align: left;\">1 :  3rd-Ventricle</td>\n<td style=\"text-align: left;\">2 :  4th-Ventricle</td>\n<td style=\"text-align: left;\">3 :  Right-Accumbens-Area</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">4 :  Left-Accumbens-Area</td>\n<td style=\"text-align: left;\">5 :  Right-Amygdala</td>\n<td style=\"text-align: left;\">6 :  Left-Amygdala</td>\n<td style=\"text-align: left;\">7 :  Brain-Stem</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">8 :  Right-Caudate</td>\n<td style=\"text-align: left;\">9 :  Left-Caudate</td>\n<td style=\"text-align: left;\">10 :  Right-Cerebellum-Exterior</td>\n<td style=\"text-align: left;\">11 :  Left-Cerebellum-Exterior</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">12 :  Right-Cerebellum-White-Matter</td>\n<td style=\"text-align: left;\">13 :  Left-Cerebellum-White-Matter</td>\n<td style=\"text-align: left;\">14 :  Right-Cerebral-White-Matter</td>\n<td style=\"text-align: left;\">15 :  Left-Cerebral-White-Matter</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">16 :  Right-Hippocampus</td>\n<td style=\"text-align: left;\">17 :  Left-Hippocampus</td>\n<td style=\"text-align: left;\">18 :  Right-Inf-Lat-Vent</td>\n<td style=\"text-align: left;\">19 :  Left-Inf-Lat-Vent</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">20 :  Right-Lateral-Ventricle</td>\n<td style=\"text-align: left;\">21 :  Left-Lateral-Ventricle</td>\n<td style=\"text-align: left;\">22 :  Right-Pallidum</td>\n<td style=\"text-align: left;\">23 :  Left-Pallidum</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">24 :  Right-Putamen</td>\n<td style=\"text-align: left;\">25 :  Left-Putamen</td>\n<td style=\"text-align: left;\">26 :  Right-Thalamus-Proper</td>\n<td style=\"text-align: left;\">27 :  Left-Thalamus-Proper</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">28 :  Right-Ventral-DC</td>\n<td style=\"text-align: left;\">29 :  Left-Ventral-DC</td>\n<td style=\"text-align: left;\">30 :  Cerebellar-Vermal-Lobules-I-V</td>\n<td style=\"text-align: left;\">31 :  Cerebellar-Vermal-Lobules-VI-VII</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">32 :  Cerebellar-Vermal-Lobules-VIII-X</td>\n<td style=\"text-align: left;\">33 :  Left-Basal-Forebrain</td>\n<td style=\"text-align: left;\">34 :  Right-Basal-Forebrain</td>\n<td style=\"text-align: left;\">35 :  Right-ACgG--anterior-cingulate-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">36 :  Left-ACgG--anterior-cingulate-gyrus</td>\n<td style=\"text-align: left;\">37 :  Right-AIns--anterior-insula</td>\n<td style=\"text-align: left;\">38 :  Left-AIns--anterior-insula</td>\n<td style=\"text-align: left;\">39 :  Right-AOrG--anterior-orbital-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">40 :  Left-AOrG--anterior-orbital-gyrus</td>\n<td style=\"text-align: left;\">41 :  Right-AnG---angular-gyrus</td>\n<td style=\"text-align: left;\">42 :  Left-AnG---angular-gyrus</td>\n<td style=\"text-align: left;\">43 :  Right-Calc--calcarine-cortex</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">44 :  Left-Calc--calcarine-cortex</td>\n<td style=\"text-align: left;\">45 :  Right-CO----central-operculum</td>\n<td style=\"text-align: left;\">46 :  Left-CO----central-operculum</td>\n<td style=\"text-align: left;\">47 :  Right-Cun---cuneus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">48 :  Left-Cun---cuneus</td>\n<td style=\"text-align: left;\">49 :  Right-Ent---entorhinal-area</td>\n<td style=\"text-align: left;\">50 :  Left-Ent---entorhinal-area</td>\n<td style=\"text-align: left;\">51 :  Right-FO----frontal-operculum</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">52 :  Left-FO----frontal-operculum</td>\n<td style=\"text-align: left;\">53 :  Right-FRP---frontal-pole</td>\n<td style=\"text-align: left;\">54 :  Left-FRP---frontal-pole</td>\n<td style=\"text-align: left;\">55 :  Right-FuG---fusiform-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">56 :  Left-FuG---fusiform-gyrus</td>\n<td style=\"text-align: left;\">57 :  Right-GRe---gyrus-rectus</td>\n<td style=\"text-align: left;\">58 :  Left-GRe---gyrus-rectus</td>\n<td style=\"text-align: left;\">59 :  Right-IOG---inferior-occipital-gyrus ,</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">60 :  Left-IOG---inferior-occipital-gyrus</td>\n<td style=\"text-align: left;\">61 :  Right-ITG---inferior-temporal-gyrus</td>\n<td style=\"text-align: left;\">62 :  Left-ITG---inferior-temporal-gyrus</td>\n<td style=\"text-align: left;\">63 :  Right-LiG---lingual-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">64 :  Left-LiG---lingual-gyrus</td>\n<td style=\"text-align: left;\">65 :  Right-LOrG--lateral-orbital-gyrus</td>\n<td style=\"text-align: left;\">66 :  Left-LOrG--lateral-orbital-gyrus</td>\n<td style=\"text-align: left;\">67 :  Right-MCgG--middle-cingulate-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">68 :  Left-MCgG--middle-cingulate-gyrus</td>\n<td style=\"text-align: left;\">69 :  Right-MFC---medial-frontal-cortex</td>\n<td style=\"text-align: left;\">70 :  Left-MFC---medial-frontal-cortex</td>\n<td style=\"text-align: left;\">71 :  Right-MFG---middle-frontal-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">72 :  Left-MFG---middle-frontal-gyrus</td>\n<td style=\"text-align: left;\">73 :  Right-MOG---middle-occipital-gyrus</td>\n<td style=\"text-align: left;\">74 :  Left-MOG---middle-occipital-gyrus</td>\n<td style=\"text-align: left;\">75 :  Right-MOrG--medial-orbital-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">76 :  Left-MOrG--medial-orbital-gyrus</td>\n<td style=\"text-align: left;\">77 :  Right-MPoG--postcentral-gyrus</td>\n<td style=\"text-align: left;\">78 :  Left-MPoG--postcentral-gyrus</td>\n<td style=\"text-align: left;\">79 :  Right-MPrG--precentral-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">80 :  Left-MPrG--precentral-gyrus</td>\n<td style=\"text-align: left;\">81 :  Right-MSFG--superior-frontal-gyrus</td>\n<td style=\"text-align: left;\">82 :  Left-MSFG--superior-frontal-gyrus</td>\n<td style=\"text-align: left;\">83 :  Right-MTG---middle-temporal-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">84 :  Left-MTG---middle-temporal-gyrus</td>\n<td style=\"text-align: left;\">85 :  Right-OCP---occipital-pole</td>\n<td style=\"text-align: left;\">86 :  Left-OCP---occipital-pole</td>\n<td style=\"text-align: left;\">87 :  Right-OFuG--occipital-fusiform-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">88 :  Left-OFuG--occipital-fusiform-gyrus</td>\n<td style=\"text-align: left;\">89 :  Right-OpIFG-opercular-part-of-the-IFG</td>\n<td style=\"text-align: left;\">90 :  Left-OpIFG-opercular-part-of-the-IFG</td>\n<td style=\"text-align: left;\">91 :  Right-OrIFG-orbital-part-of-the-IFG</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">92 :  Left-OrIFG-orbital-part-of-the-IFG</td>\n<td style=\"text-align: left;\">93 :  Right-PCgG--posterior-cingulate-gyrus</td>\n<td style=\"text-align: left;\">94 :  Left-PCgG--posterior-cingulate-gyrus</td>\n<td style=\"text-align: left;\">95 :  Right-PCu---precuneus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">96 :  Left-PCu---precuneus</td>\n<td style=\"text-align: left;\">97 :  Right-PHG---parahippocampal-gyrus</td>\n<td style=\"text-align: left;\">98 :  Left-PHG---parahippocampal-gyrus</td>\n<td style=\"text-align: left;\">99 :  Right-PIns--posterior-insula</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">100 :  Left-PIns--posterior-insula</td>\n<td style=\"text-align: left;\">101 :  Right-PO----parietal-operculum</td>\n<td style=\"text-align: left;\">102 :  Left-PO----parietal-operculum</td>\n<td style=\"text-align: left;\">103 :  Right-PoG---postcentral-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">104 :  Left-PoG---postcentral-gyrus</td>\n<td style=\"text-align: left;\">105 :  Right-POrG--posterior-orbital-gyrus</td>\n<td style=\"text-align: left;\">106 :  Left-POrG--posterior-orbital-gyrus</td>\n<td style=\"text-align: left;\">107 :  Right-PP----planum-polare</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">108 :  Left-PP----planum-polare</td>\n<td style=\"text-align: left;\">109 :  Right-PrG---precentral-gyrus</td>\n<td style=\"text-align: left;\">110 :  Left-PrG---precentral-gyrus</td>\n<td style=\"text-align: left;\">111 :  Right-PT----planum-temporale</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">112 :  Left-PT----planum-temporale</td>\n<td style=\"text-align: left;\">113 :  Right-SCA---subcallosal-area</td>\n<td style=\"text-align: left;\">114 :  Left-SCA---subcallosal-area</td>\n<td style=\"text-align: left;\">115 :  Right-SFG---superior-frontal-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">116 :  Left-SFG---superior-frontal-gyrus</td>\n<td style=\"text-align: left;\">117 :  Right-SMC---supplementary-motor-cortex</td>\n<td style=\"text-align: left;\">118 :  Left-SMC---supplementary-motor-cortex</td>\n<td style=\"text-align: left;\">119 :  Right-SMG---supramarginal-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">120 :  Left-SMG---supramarginal-gyrus</td>\n<td style=\"text-align: left;\">121 :  Right-SOG---superior-occipital-gyrus</td>\n<td style=\"text-align: left;\">122 :  Left-SOG---superior-occipital-gyrus</td>\n<td style=\"text-align: left;\">123 :  Right-SPL---superior-parietal-lobule</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">124 :  Left-SPL---superior-parietal-lobule</td>\n<td style=\"text-align: left;\">125 :  Right-STG---superior-temporal-gyrus</td>\n<td style=\"text-align: left;\">126 :  Left-STG---superior-temporal-gyrus</td>\n<td style=\"text-align: left;\">127 :  Right-TMP---temporal-pole</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">128 :  Left-TMP---temporal-pole</td>\n<td style=\"text-align: left;\">129 :  Right-TrIFG-triangular-part-of-the-IFG</td>\n<td style=\"text-align: left;\">130 :  Left-TrIFG-triangular-part-of-the-IFG</td>\n<td style=\"text-align: left;\">131 :  Right-TTG---transverse-temporal-gyrus</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">132 :  Left-TTG---transverse-temporal-gyrus</td>\n<td style=\"text-align: left;\"></td>\n<td style=\"text-align: left;\"></td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n<h2>Bundle Integration in MONAI Lable</h2>\n<p>The inference and training pipleine can be easily used by the MONAI Label server and 3D Slicer for fast labeling T1w MRI images in MNI space.</p>\n<p><img alt=\"\" src=\"./3DSlicer_use.png\"/> <br/></p>\n<h1>Disclaimer</h1>\n<p>This is an example, not to be used for diagnostic purposes.</p>\n<h1>References</h1>\n<p>[1] Yu, Xin, Yinchi Zhou, Yucheng Tang et al.  Characterizing Renal Structures with 3D Block Aggregate Transformers.  arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf</p>\n<p>[2] Zizhao Zhang et al.  Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding.  AAAI Conference on Artificial Intelligence (AAAI) 2022</p>\n<p>[3] Huo, Yuankai, et al.  3D whole brain segmentation using spatially localized atlas network tiles.  NeuroImage 194 (2019): 105-119.</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/wholeBrainSeg_Large_UNEST_segmentation/tree/0.2.6",
        "changelog": {
            "0.2.6": "update to huggingface hosting",
            "0.2.5": "update large files",
            "0.2.4": "fix black 24.1 format error",
            "0.2.3": "fix PYTHONPATH in readme.md",
            "0.2.2": "add name tag",
            "0.2.1": "fix license Copyright error",
            "0.2.0": "update license files",
            "0.1.2": "Add training support for whole brain segmentation, users can use active learning in the MONAI Label",
            "0.1.1": "Fix dimension according to MONAI 1.0 and fix readme file",
            "0.1.0": "complete the model package"
        }
    },
    "prostate_mri_anatomy": {
        "model_name": "Prostate MRI anatomy",
        "description": "A pre-trained model for volumetric (3D) segmentation of the prostate from MRI images",
        "authors": "Keno Bressem",
        "papers": [
            "Adams, L. C., Makowski, M. R., Engel, G., Rattunde, M., Busch, F., Asbach, P., ... & Bressem, K. K. (2022). Prostate158-An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection. Computers in Biology and Medicine, 148, 105817."
        ],
        "version": "0.3.5",
        "model_id": "prostate_mri_anatomy",
        "readme": "<h1>Prostate MRI zonal segmentation</h1>\n<h3><strong>Authors</strong></h3>\n<p>Lisa C. Adams, Keno K. Bressem</p>\n<h3><strong>Tags</strong></h3>\n<p>Segmentation, MR, Prostate</p>\n<h2><strong>Model Description</strong></h2>\n<p>This model was trained with the UNet architecture [1] and is used for 3D volumetric segmentation of the anatomical prostate zones on T2w MRI images. The segmentation of the anatomical regions is formulated as a voxel-wise classification. Each voxel is classified as either central gland (1), peripheral zone (2), or background (0). The model is optimized using a gradient descent method that minimizes the focal soft-dice loss between the predicted mask and the actual segmentation.</p>\n<h2><strong>Data</strong></h2>\n<p>The model was trained in the prostate158 training data, which is available at https://doi.org/10.5281/zenodo.6481141. Only T2w images were used for this task.</p>\n<h3><strong>Preprocessing</strong></h3>\n<p>MRI images in the prostate158 dataset were preprocessed, including center cropping and resampling. When applying the model to new data, this preprocessing should be repeated.</p>\n<h4><strong>Center cropping</strong></h4>\n<p>T2w images were acquired with a voxel spacing of 0.47 x 0.47 x 3 mm and an axial FOV size of 180 x 180 mm. However, the prostate rarely exceeds an axial diameter of 100 mm, and for zonal segmentation, the tissue surrounding the prostate is not of interest and only increases the image size and thus the computational cost. Center-cropping can reduce the image size without sacrificing information.</p>\n<p>The script <code>center_crop.py</code> allows to reproduce center-cropping as performed in the prostate158 paper.</p>\n<pre><code class=\"language-bash\">python scripts/center_crop.py --file_name path/to/t2_image --out_name cropped_t2\n</code></pre>\n<h4><strong>Resampling</strong></h4>\n<p>DWI and ADC sequences in prostate158 were resampled to the orientation and voxel spacing of the T2w sequence. As the zonal segmentation uses T2w images, no additional resampling is nessecary. However, the training script will perform additonal resampling automatically.</p>\n<h2><strong>Performance</strong></h2>\n<p>The model achives the following performance on the prostate158 test dataset:</p>\n<table border=\"1\" frame=\"void\" rules=\"rows\">\n<thead>\n<tr>\n<td></td>\n<td colspan=\"3\"><b><center>Rater 1</center></b></td>\n<td></td>\n<td colspan=\"3\"><b><center>Rater 2</center></b></td>\n</tr>\n<tr>\n<th>Metric</th>\n<th>Transitional Zone</th>\n<th>Peripheral Zone</th>\n<th></th>\n<th>Transitional Zone</th>\n<th>Peripheral Zone</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\">Dice Coefficient </a></td>\n<td> 0.877</td>\n<td> 0.754</td>\n<td></td>\n<td> 0.875</td>\n<td> 0.730</td>\n</tr>\n<tr>\n<td><a href=\"https://en.wikipedia.org/wiki/Hausdorff_distance\">Hausdorff Distance </a></td>\n<td> 18.3</td>\n<td> 22.8</td>\n<td></td>\n<td> 17.5</td>\n<td> 33.2</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/deepmind/surface-distance\">Surface Distance </a></td>\n<td> 2.19</td>\n<td> 1.95</td>\n<td></td>\n<td> 2.59</td>\n<td> 1.88</td>\n</tr>\n</tbody>\n</table>\n<p>For more details, please see the original <a href=\"https://doi.org/10.1016/j.compbiomed.2022.105817\">publication</a> or official <a href=\"https://github.com/kbressem/prostate158\">GitHub repository</a></p>\n<h2><strong>System Configuration</strong></h2>\n<p>The model was trained for 100 epochs on a workstaion with a single Nvidia RTX 3080 GPU. This takes approximatly 8 hours.</p>\n<h2><strong>Limitations</strong> (Optional)</h2>\n<p>This training and inference pipeline was developed for research purposes only. This research use only software that has not been cleared or approved by FDA or any regulatory agency. The model is for research/developmental purposes only and cannot be used directly for clinical procedures.</p>\n<h2><strong>Citation Info</strong> (Optional)</h2>\n<pre><code>@article{ADAMS2022105817,\ntitle = {Prostate158 - An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection},\njournal = {Computers in Biology and Medicine},\nvolume = {148},\npages = {105817},\nyear = {2022},\nissn = {0010-4825},\ndoi = {https://doi.org/10.1016/j.compbiomed.2022.105817},\nurl = {https://www.sciencedirect.com/science/article/pii/S0010482522005789},\nauthor = {Lisa C. Adams and Marcus R. Makowski and Gnther Engel and Maximilian Rattunde and Felix Busch and Patrick Asbach and Stefan M. Niehues and Shankeeth Vinayahalingam and Bram {van Ginneken} and Geert Litjens and Keno K. Bressem},\nkeywords = {Prostate cancer, Deep learning, Machine learning, Artificial intelligence, Magnetic resonance imaging, Biparametric prostate MRI}\n}\n</code></pre>\n<h2><strong>References</strong></h2>\n<p>[1] Sakinis, Tomas, et al. \"Interactive segmentation of medical images through fully convolutional neural networks.\" arXiv preprint arXiv:1903.08205 (2019).</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/prostate_mri_anatomy/tree/0.3.5",
        "changelog": {
            "0.3.5": "update to huggingface hosting",
            "0.3.4": "support monai 1.4",
            "0.3.3": "add invertd transformation",
            "0.3.2": "add name tag",
            "0.3.1": "fix license Copyright error",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.1": "add torchscript model",
            "0.1.0": "complete the model package"
        }
    },
    "valve_landmarks": {
        "model_name": "Valve landmarks regression",
        "description": "This network is used to find where valves attach to heart to help construct 3D FEM models for computation. The output is an array of 10 2D coordinates.",
        "authors": "Eric Kerfoot",
        "papers": [
            "Kerfoot, E, King, CE, Ismail, T, Nordsletten, D & Miller, R 2021, Estimation of Cardiac Valve Annuli Motion with Deep Learning. https://doi.org/10.1007/978-3-030-68107-4_15"
        ],
        "version": "0.5.1",
        "model_id": "valve_landmarks",
        "readme": "<h1>2D Cardiac Valve Landmark Regressor</h1>\n<p>This network identifies 10 different landmarks in 2D+t MR images of the heart (2 chamber, 3 chamber, and 4 chamber) representing the insertion locations of valve leaflets into the myocardial wall. These coordinates are used in part of the construction of 3D FEM cardiac models suitable for physics simulation of heart functions.</p>\n<p>Input images are individual 2D slices from the time series, and the output from the network is a <code>(2, 10)</code> set of 2D points in <code>HW</code> image coordinate space. The 10 coordinates correspond to the attachment point for these valves:</p>\n<ol>\n<li>Mitral anterior in 2CH</li>\n<li>Mitral posterior in 2CH</li>\n<li>Mitral septal in 3CH</li>\n<li>Mitral free wall in 3CH</li>\n<li>Mitral septal in 4CH</li>\n<li>Mitral free wall in 4CH</li>\n<li>Aortic septal</li>\n<li>Aortic free wall</li>\n<li>Tricuspid septal</li>\n<li>Tricuspid free wall</li>\n</ol>\n<p>Landmarks which do not appear in a particular image are predicted to be <code>(0, 0)</code> or close to this location. The mitral valve is expected to appear in all three views. Landmarks are not provided for the pulmonary valve.</p>\n<p>Example plot of landmarks on a single frame, see <a href=\"./view_results.ipynb\">view_results.ipynb</a> for visualising network output:</p>\n<p><img alt=\"Landmark Example Image\" src=\"./prediction_example.png\"/></p>\n<h2>Training</h2>\n<p>The training script <code>train.json</code> is provided to train the network using a dataset of image pairs containing the MR image and a landmark image. This is done to reuse image-based transforms which do not currently operate on geometry. A number of other transforms are provided in <code>valve_landmarks.py</code> to implement Fourier-space dropout, image shifting which preserve landmarks, and smooth-field deformation applied to images and landmarks.</p>\n<p>The dataset used for training unfortunately cannot be made public, however the training script can be used with any NPZ file containing the training image stack in key <code>trainImgs</code> and landmark image stack in <code>trainLMImgs</code>, plus <code>testImgs</code> and <code>testLMImgs</code> containing validation data. The landmark images are defined as 0 for every non-landmark pixel, with landmark pixels containing the following values for each landmark type:</p>\n<ul>\n<li>10: Mitral anterior in 2CH</li>\n<li>15: Mitral posterior in 2CH</li>\n<li>20: Mitral septal in 3CH</li>\n<li>25: Mitral free wall in 3CH</li>\n<li>30: Mitral septal in 4CH</li>\n<li>35: Mitral free wall in 4CH</li>\n<li>100: Aortic septal</li>\n<li>150: Aortic free wall</li>\n<li>200: Tricuspid septal</li>\n<li>250: Tricuspid free wall</li>\n</ul>\n<p>The MR and landmark images should be 2D with no channel dimension. Within the npz file these images should be stored in single large arrays with the batch dimension first. For example, the contents of the training dataset are:</p>\n<ul>\n<li>trainImgs (8574, 256, 256)</li>\n<li>testImgs (930, 256, 256)</li>\n<li>trainLMImgs (8574, 256, 256)</li>\n<li>testLMImgs (930, 256, 256)</li>\n</ul>\n<p>This shows a training set of 8574 image pairs and a test set of 930 image pairs. The transforms provided with the bundle assume these dimensions so your own dataset should stick to this format.</p>\n<p>The following command will train with the default NPZ filename <code>./valvelandmarks.npz</code>, assuming the current directory is the bundle directory:</p>\n<pre><code class=\"language-sh\">python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json \\\n    --bundle_root . --dataset_file ./valvelandmarks.npz --output_dir /path/to/outputs\n</code></pre>\n<h2>Inference</h2>\n<p>The included <code>inference.json</code> script will run inference on a directory containing Nifti files whose images have shape <code>(256, 256, 1, N)</code> for <code>N</code> timesteps. For each image the output in the <code>output_dir</code> directory will be a npy file containing a result array of shape <code>(N, 2, 10)</code> storing the 10 coordinates for each <code>N</code> timesteps. Invoking this script can be done as follows, assuming the current directory is the bundle directory:</p>\n<pre><code class=\"language-sh\">python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json \\\n    --bundle_root . --dataset_dir /path/to/data --output_dir /path/to/outputs\n</code></pre>\n<p>The provided test Nifti file can be placed in a directory which is then used as the <code>dataset_dir</code> value. This image was derived from <a href=\"http://www.cardiacatlas.org/studies/amrg-cardiac-atlas\">the AMRG Cardiac Atlas dataset</a> (AMRG Cardiac Atlas, Auckland MRI Research Group, Auckland, New Zealand). The results from this inference can be visualised by changing path values in <a href=\"./view_results.ipynb\">view_results.ipynb</a>.</p>\n<h3>Reference</h3>\n<p>The work for this model and its application is described in:</p>\n<p><code>Kerfoot, E, King, CE, Ismail, T, Nordsletten, D &amp; Miller, R 2021, Estimation of Cardiac Valve Annuli Motion with Deep Learning. in E Puyol Anton, M Pop, M Sermesant, V Campello, A Lalande, K Lekadir, A Suinesiaputra, O Camara &amp; A Young (eds), Statistical Atlases and Computational Models of the Heart. MandMs and EMIDEC Challenges - 11th International Workshop, STACOM 2020, Held in Conjunction with MICCAI 2020, Revised Selected Papers. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 12592 LNCS, Springer Science and Business Media Deutschland GmbH, pp. 146-155, 11th International Workshop on Statistical Atlases and Computational Models of the Heart, STACOM 2020 held in Conjunction with MICCAI 2020, Lima, Peru, 4/10/2020. https://doi.org/10.1007/978-3-030-68107-4_15</code></p>\n<h1>License</h1>\n<p>This model is released under the MIT License. The license file is included with the model.</p>",
        "download_url": "https://huggingface.co/MONAI/valve_landmarks/tree/0.5.1",
        "changelog": {
            "0.5.1": "update to huggingface hosting",
            "0.5.0": "Fix transform usage",
            "0.4.3": "README.md fix",
            "0.4.2": "add name tag",
            "0.4.1": "modify dataset key name",
            "0.4.0": "update license files",
            "0.3.0": "Update to scripts",
            "0.2.0": "Unify naming",
            "0.1.0": "Initial version"
        }
    },
    "lung_nodule_ct_detection": {
        "model_name": "Lung nodule CT detection",
        "description": "A pre-trained model for volumetric (3D) detection of the lung lesion from CT image on LUNA16 dataset",
        "authors": "MONAI team",
        "papers": [
            "Lin, Tsung-Yi, et al. 'Focal loss for dense object detection. ICCV 2017"
        ],
        "version": "0.6.9",
        "model_id": "lung_nodule_ct_detection",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for volumetric (3D) detection of the lung nodule from CT image.</p>\n<p>This model is trained on LUNA16 dataset (https://luna16.grand-challenge.org/Home/), using the RetinaNet (Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" ICCV 2017. https://arxiv.org/abs/1708.02002).</p>\n<p><img alt=\"model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_workflow.png\"/></p>\n<h2>Data</h2>\n<p>The dataset we are experimenting in this example is LUNA16 (https://luna16.grand-challenge.org/Home/), which is based on <a href=\"https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI\">LIDC-IDRI database</a> [3,4,5].</p>\n<p>LUNA16 is a public dataset of CT lung nodule detection. Using raw CT scans, the goal is to identify locations of possible nodules, and to assign a probability for being a nodule to each location.</p>\n<p>Disclaimer: We are not the host of the data. Please make sure to read the requirements and usage policies of the data and give credit to the authors of the dataset! We acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study.</p>\n<h3>10-fold data splitting</h3>\n<p>We follow the official 10-fold data splitting from LUNA16 challenge and generate data split json files using the script from <a href=\"https://github.com/MIC-DKFZ/nnDetection/blob/main/projects/Task016_Luna/scripts/prepare.py\">nnDetection</a>.</p>\n<p>Please download the resulted json files from https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/LUNA16_datasplit-20220615T233840Z-001.zip.</p>\n<p>In these files, the values of \"box\" are the ground truth boxes in world coordinate.</p>\n<h3>Data resampling</h3>\n<p>The raw CT images in LUNA16 have various of voxel sizes. The first step is to resample them to the same voxel size.\nIn this model, we resampled them into 0.703125 x 0.703125 x 1.25 mm.</p>\n<p>Please following the instruction in Section 3.1 of https://github.com/Project-MONAI/tutorials/tree/main/detection to do the resampling.</p>\n<h3>Data download</h3>\n<p>The mhd/raw original data can be downloaded from <a href=\"https://luna16.grand-challenge.org/Home/\">LUNA16</a>. The DICOM original data can be downloaded from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI\">LIDC-IDRI database</a> [3,4,5]. You will need to resample the original data to start training.</p>\n<p>Alternatively, we provide <a href=\"https://drive.google.com/drive/folders/1JozrufA1VIZWJIc5A1EMV3J4CNCYovKK?usp=share_link\">resampled nifti images</a> and a copy of <a href=\"https://drive.google.com/drive/folders/1-enN4eNEnKmjltevKg3W2V-Aj0nriQWE?usp=share_link\">original mhd/raw images</a> from <a href=\"https://luna16.grand-challenge.org/Home/\">LUNA16</a> for users to download.</p>\n<h2>Training configuration</h2>\n<p>The training was performed with the following:</p>\n<ul>\n<li>GPU: at least 16GB GPU memory, requires 32G when exporting TRT model</li>\n<li>Actual Model Input: 192 x 192 x 80</li>\n<li>AMP: True</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-2</li>\n<li>Loss: BCE loss and L1 loss</li>\n</ul>\n<h3>Input</h3>\n<p>1 channel\n- List of 3D CT patches</p>\n<h3>Output</h3>\n<p>In Training Mode: A dictionary of classification and box regression loss.</p>\n<p>In Evaluation Mode: A list of dictionaries of predicted box, classification label, and classification score.</p>\n<h2>Performance</h2>\n<p>Coco metric is used for evaluating the performance of the model. The pre-trained model was trained and validated on data fold 0. This model achieves a mAP=0.852, mAR=0.998, AP(IoU=0.1)=0.858, AR(IoU=0.1)=1.0.</p>\n<p>Please note that this bundle is non-deterministic because of the max pooling layer used in the network. Therefore, reproducing the training process may not get exactly the same performance.\nPlease refer to https://pytorch.org/docs/stable/notes/randomness.html#reproducibility for more details about reproducibility.</p>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the detection train loss\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_train_loss_v2.png\"/></p>\n<h4>Validation Accuracy</h4>\n<p>The validation accuracy in this curve is the mean of mAP, mAR, AP(IoU=0.1), and AR(IoU=0.1) in Coco metric.</p>\n<p><img alt=\"A graph showing the detection val accuracy\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_val_acc_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>lung_nodule_ct_detection</code> bundle supports acceleration with TensorRT through the ONNX-TensorRT method. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that when using the TensorRT model for inference, the <code>force_sliding_window</code> parameter in the <code>inference.json</code> file must be set to <code>true</code>. This ensures that the bundle uses the <code>SlidingWindowInferer</code> during inference and maintains the input spatial size of the network. Otherwise, if given an input with spatial size less than the <code>infer_patch_size</code>, the input spatial size of the network would be changed.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">7449.84</td>\n<td style=\"text-align: center;\">996.08</td>\n<td style=\"text-align: center;\">976.67</td>\n<td style=\"text-align: center;\">626.90</td>\n<td style=\"text-align: center;\">7.63</td>\n<td style=\"text-align: center;\">7.63</td>\n<td style=\"text-align: center;\">11.88</td>\n<td style=\"text-align: center;\">1.56</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">36458.26</td>\n<td style=\"text-align: center;\">7259.35</td>\n<td style=\"text-align: center;\">6420.60</td>\n<td style=\"text-align: center;\">4698.34</td>\n<td style=\"text-align: center;\">5.02</td>\n<td style=\"text-align: center;\">5.68</td>\n<td style=\"text-align: center;\">7.76</td>\n<td style=\"text-align: center;\">1.55</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>Currently, the only available method to accelerate this model is through ONNX-TensorRT. However, the Torch-TensorRT method is under development and will be available in the near future.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.0\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference on resampled LUNA16 images by setting <code>\"whether_raw_luna16\": false</code> in <code>inference.json</code>:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<p>With the same command, we can execute inference on original LUNA16 images by setting <code>\"whether_raw_luna16\": true</code> in <code>inference.json</code>. Remember to also set <code>\"data_list_file_path\": \"$@bundle_root + '/LUNA16_datasplit/mhd_original/dataset_fold0.json'\"</code> and change <code>\"dataset_dir\"</code>.</p>\n<p>Note that in inference.json, the transform \"LoadImaged\" in \"preprocessing\" and \"AffineBoxToWorldCoordinated\" in \"postprocessing\" has <code>\"affine_lps_to_ras\": true</code>.\nThis depends on the input images. LUNA16 needs <code>\"affine_lps_to_ras\": true</code>.\nIt is possible that your inference dataset should set <code>\"affine_lps_to_ras\": false</code>.</p>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt; --input_shape \"[1, 1, 512, 512, 192]\"  --use_onnx \"True\" --use_trace \"True\" --onnx_output_names \"['output_0', 'output_1', 'output_2', 'output_3', 'output_4', 'output_5']\" --network_def#use_list_output \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Lin, Tsung-Yi, et al. \"Focal loss for dense object detection.\" ICCV 2017. https://arxiv.org/abs/1708.02002)</p>\n<p>[2] Baumgartner and Jaeger et al. \"nnDetection: A self-configuring method for medical object detection.\" MICCAI 2021. https://arxiv.org/pdf/2106.00817.pdf</p>\n<p>[3] Armato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Van Beek, E. J. R., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C. , Qing, D. P. Y. , Roberts, R. Y., Smith, A. R., Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallam, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., Clarke, L. P. (2015). Data From LIDC-IDRI [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX</p>\n<p>[4] Armato SG 3rd, McLennan G, Bidaut L, McNitt-Gray MF, Meyer CR, Reeves AP, Zhao B, Aberle DR, Henschke CI, Hoffman EA, Kazerooni EA, MacMahon H, Van Beeke EJ, Yankelevitz D, Biancardi AM, Bland PH, Brown MS, Engelmann RM, Laderach GE, Max D, Pais RC, Qing DP, Roberts RY, Smith AR, Starkey A, Batrah P, Caligiuri P, Farooqi A, Gladish GW, Jude CM, Munden RF, Petkovska I, Quint LE, Schwartz LH, Sundaram B, Dodd LE, Fenimore C, Gur D, Petrick N, Freymann J, Kirby J, Hughes B, Casteele AV, Gupte S, Sallamm M, Heath MD, Kuhn MH, Dharaiya E, Burns R, Fryd DS, Salganicoff M, Anand V, Shreter U, Vastagh S, Croft BY.  The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans. Medical Physics, 38: 915--931, 2011. DOI: https://doi.org/10.1118/1.3528204</p>\n<p>[5] Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., &amp; Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. Journal of Digital Imaging, 26(6), 10451057. https://doi.org/10.1007/s10278-013-9622-7</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/lung_nodule_ct_detection/tree/0.6.9",
        "changelog": {
            "0.6.9": "update to huggingface hosting and fix missing dependencies",
            "0.6.8": "update issue for IgniteInfo",
            "0.6.7": "use monai 1.4 and update large files",
            "0.6.6": "update to use monai 1.3.1",
            "0.6.5": "remove notes for trt_export in readme",
            "0.6.4": "add notes for trt_export in readme",
            "0.6.3": "add load_pretrain flag for infer",
            "0.6.2": "add checkpoint loader for infer",
            "0.6.1": "fix format error",
            "0.6.0": "remove meta_dict usage",
            "0.5.9": "use monai 1.2.0",
            "0.5.8": "update TRT memory requirement in readme",
            "0.5.7": "add dataset dir example",
            "0.5.6": "add the ONNX-TensorRT way of model conversion",
            "0.5.5": "update retrained validation results and training curve",
            "0.5.4": "add non-deterministic note",
            "0.5.3": "adapt to BundleWorkflow interface",
            "0.5.2": "black autofix format and add name tag",
            "0.5.1": "modify dataset key name",
            "0.5.0": "use detection inferer",
            "0.4.5": "fixed some small changes with formatting in readme",
            "0.4.4": "add data resource to readme",
            "0.4.3": "update val patch size to avoid warning in monai 1.0.1",
            "0.4.2": "update to use monai 1.0.1",
            "0.4.1": "fix license Copyright error",
            "0.4.0": "add support for raw images",
            "0.3.0": "update license files",
            "0.2.0": "unify naming",
            "0.1.1": "add reference for LIDC dataset",
            "0.1.0": "complete the model package"
        }
    },
    "endoscopic_tool_segmentation": {
        "model_name": "Endoscopic tool segmentation",
        "description": "A pre-trained binary segmentation model for endoscopic tool segmentation",
        "authors": "NVIDIA DLMED team",
        "papers": [
            "Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. ICML, 2019a. https://arxiv.org/pdf/1905.11946.pdf",
            "O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. https://arxiv.org/pdf/1505.04597.pdf"
        ],
        "version": "0.6.1",
        "model_id": "endoscopic_tool_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for the endoscopic tool segmentation task, trained using a flexible unet structure with an efficientnet-b2 [1] as the backbone and a UNet architecture [2] as the decoder. Datasets use private samples from <a href=\"https://www.activsurgical.com/\">Activ Surgical</a>.</p>\n<p>The <a href=\"https://drive.google.com/file/d/1I7UtWDKDEcezMqYiA-i_hsRTCrvWwJ61/view?usp=sharing\">PyTorch model</a> and <a href=\"https://drive.google.com/file/d/1e_wYd1HjJQ0dz_HKdbthRcMOyUL02aLG/view?usp=sharing\">torchscript model</a> are shared in google drive. Details can be found in <code>large_files.yml</code> file. Modify the \"bundle_root\" parameter specified in configs/train.json and configs/inference.json to reflect where models are downloaded. Expected directory path to place downloaded models is \"models/\" under \"bundle_root\".</p>\n<p><img alt=\"image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_workflow.png\"/></p>\n<h2>Pre-trained weights</h2>\n<p>A pre-trained encoder weights would benefit the model training. In this bundle, the encoder is trained with pre-trained weights from some internal data. We provide two options to enable users to load pre-trained weights:</p>\n<ol>\n<li>Via setting the <code>use_imagenet_pretrain</code> parameter in the config file to <code>True</code>, <a href=\"https://ieeexplore.ieee.org/document/5206848\">ImageNet</a> pre-trained weights from the <a href=\"https://github.com/lukemelas/EfficientNet-PyTorch\">EfficientNet-PyTorch repo</a> can be loaded. Please note that these weights are for non-commercial use. Each user is responsible for checking the content of the models/datasets and the applicable licenses and determining if suitable for the intended use.</li>\n<li>Via adding a <code>CheckpointLoader</code> as the first handler to the <code>handlers</code> section of the <code>train.json</code> config file, weights from a local path can be loaded. Here is an example <code>CheckpointLoader</code>:</li>\n</ol>\n<pre><code class=\"language-json\">{\n    \"_target_\": \"CheckpointLoader\",\n    \"load_path\": \"/path/to/local/weight/model.pt\",\n    \"load_dict\": {\n        \"model\": \"@network\"\n    },\n    \"strict\": false,\n    \"map_location\": \"@device\"\n}\n</code></pre>\n<p>When executing the training command, if neither adding the <code>CheckpointLoader</code> to the <code>train.json</code> nor setting the <code>use_imagenet_pretrain</code> parameter to <code>True</code>, a training process would start from scratch.</p>\n<h2>Data</h2>\n<p>Datasets used in this work were provided by <a href=\"https://www.activsurgical.com/\">Activ Surgical</a>.</p>\n<p>Since datasets are private, existing public datasets like <a href=\"https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/Data/\">EndoVis 2017</a> can be used to train a similar model.</p>\n<h3>Preprocessing</h3>\n<p>When using EndoVis or any other dataset, it should be divided into \"train\", \"valid\" and \"test\" folders. Samples in each folder would better be images and converted to jpg format. Otherwise, \"images\", \"labels\", \"val_images\" and \"val_labels\" parameters in <code>configs/train.json</code> and \"datalist\" in <code>configs/inference.json</code> should be modified to fit given dataset. After that, \"dataset_dir\" parameter in <code>configs/train.json</code> and <code>configs/inference.json</code> should be changed to root folder which contains \"train\", \"valid\" and \"test\" folders.</p>\n<p>Please notice that loading data operation in this bundle is adaptive. If images and labels are not in the same format, it may lead to a mismatching problem. For example, if images are in jpg format and labels are in npy format, PIL and Numpy readers will be used separately to load images and labels. Since these two readers have their own way to parse file's shape, loaded labels will be transpose of the correct ones and incur a missmatching problem.</p>\n<h2>Training configuration</h2>\n<p>The training as performed with the following:\n- GPU: At least 12GB of GPU memory\n- Actual Model Input: 736 x 480 x 3\n- Optimizer: Adam\n- Learning Rate: 1e-4\n- Dataset Manager: CacheDataset</p>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h3>Input</h3>\n<p>A three channel video frame</p>\n<h3>Output</h3>\n<p>Two channels:\n- Label 1: tools\n- Label 0: everything else</p>\n<h2>Performance</h2>\n<p>IoU was used for evaluating the performance of the model. This model achieves a mean IoU score of 0.86.</p>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the training loss over 100 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_train_loss_v3.png\"/></p>\n<h4>Validation IoU</h4>\n<p><img alt=\"A graph showing the validation mean IoU over 100 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_val_iou_v3.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>endoscopic_tool_segmentation</code> bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">12.00</td>\n<td style=\"text-align: center;\">14.06</td>\n<td style=\"text-align: center;\">6.59</td>\n<td style=\"text-align: center;\">5.20</td>\n<td style=\"text-align: center;\">0.85</td>\n<td style=\"text-align: center;\">1.82</td>\n<td style=\"text-align: center;\">2.31</td>\n<td style=\"text-align: center;\">2.70</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">170.04</td>\n<td style=\"text-align: center;\">172.20</td>\n<td style=\"text-align: center;\">155.26</td>\n<td style=\"text-align: center;\">155.57</td>\n<td style=\"text-align: center;\">0.99</td>\n<td style=\"text-align: center;\">1.10</td>\n<td style=\"text-align: center;\">1.09</td>\n<td style=\"text-align: center;\">1.11</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.0\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config and <code>evaluate</code> config to execute multi-GPU evaluation:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TorchScript file:</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt;\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. ICML, 2019a. https://arxiv.org/pdf/1905.11946.pdf</p>\n<p>[2] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. https://arxiv.org/pdf/1505.04597.pdf</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/endoscopic_tool_segmentation/tree/0.6.1",
        "changelog": {
            "0.6.1": "update to huggingface hosting and fix missing dependencies",
            "0.6.0": "use monai 1.4 and update large files",
            "0.5.9": "update to use monai 1.3.1",
            "0.5.8": "add load_pretrain flag for infer",
            "0.5.7": "add checkpoint loader for infer",
            "0.5.6": "update to use monai 1.3.0",
            "0.5.5": "update AddChanneld with EnsureChannelFirstd and set image_only to False",
            "0.5.4": "fix the wrong GPU index issue of multi-node",
            "0.5.3": "remove error dollar symbol in readme",
            "0.5.2": "remove the CheckpointLoader from the train.json",
            "0.5.1": "add RAM warning",
            "0.5.0": "update TensorRT descriptions",
            "0.4.9": "update the model weights",
            "0.4.8": "update the TensorRT part in the README file",
            "0.4.7": "fix mgpu finalize issue",
            "0.4.6": "enable deterministic training",
            "0.4.5": "add the command of executing inference with TensorRT models",
            "0.4.4": "adapt to BundleWorkflow interface",
            "0.4.3": "update this bundle to support TensorRT convert",
            "0.4.2": "support monai 1.2 new FlexibleUNet",
            "0.4.1": "add name tag",
            "0.4.0": "add support for multi-GPU training and evaluation",
            "0.3.2": "restructure readme to match updated template",
            "0.3.1": "add figures of workflow and metrics, add invert transform",
            "0.3.0": "update dataset processing",
            "0.2.1": "update to use monai 1.0.1",
            "0.2.0": "update license files",
            "0.1.0": "complete the first version model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "endoscopic_inbody_classification": {
        "model_name": "Endoscopic inbody classification",
        "description": "A pre-trained binary classification model for endoscopic inbody classification task",
        "authors": "NVIDIA DLMED team",
        "papers": [
            "J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132-7141. https://arxiv.org/pdf/1709.01507.pdf"
        ],
        "version": "0.5.0",
        "model_id": "endoscopic_inbody_classification",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for the endoscopic inbody classification task and trained using the SEResNet50 structure, whose details can be found in [1]. All datasets are from private samples of <a href=\"https://www.activsurgical.com/\">Activ Surgical</a>. Samples in training and validation dataset are from the same 4 videos, while test samples are from different two videos.</p>\n<p>The <a href=\"https://drive.google.com/file/d/14CS-s1uv2q6WedYQGeFbZeEWIkoyNa-x/view?usp=sharing\">PyTorch model</a> and <a href=\"https://drive.google.com/file/d/1fOoJ4n5DWKHrt9QXTZ2sXwr9C-YvVGCM/view?usp=sharing\">torchscript model</a> are shared in google drive. Modify the <code>bundle_root</code> parameter specified in <code>configs/train.json</code> and <code>configs/inference.json</code> to reflect where models are downloaded. Expected directory path to place downloaded models is <code>models/</code> under <code>bundle_root</code>.</p>\n<p><img alt=\"image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_workflow.png\"/></p>\n<h2>Data</h2>\n<p>The datasets used in this work were provided by <a href=\"https://www.activsurgical.com/\">Activ Surgical</a>.</p>\n<p>Since datasets are private, we provide a <a href=\"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/inbody_outbody_samples.zip\">link</a> of 20 samples (10 in-body and 10 out-body) to show what they look like.</p>\n<h3>Preprocessing</h3>\n<p>After downloading this dataset, python script in <code>scripts</code> folder named <code>data_process</code> can be used to generate label json files by running the command below and modifying <code>datapath</code> to path of unziped downloaded data. Generated label json files will be stored in <code>label</code> folder under the bundle path.</p>\n<pre><code>python scripts/data_process.py --datapath /path/to/data/root\n</code></pre>\n<p>By default, label path parameter in <code>train.json</code> and <code>inference.json</code> of this bundle is point to the generated <code>label</code> folder under bundle path. If you move these generated label files to another place, please modify the <code>train_json</code>, <code>val_json</code> and <code>test_json</code> parameters specified in <code>configs/train.json</code> and <code>configs/inference.json</code> to where these label files are.</p>\n<p>The input label json should be a list made up by dicts which includes <code>image</code> and <code>label</code> keys. An example format is shown below.</p>\n<pre><code>[\n    {\n        \"image\":\"/path/to/image/image_name0.jpg\",\n        \"label\": 0\n    },\n    {\n        \"image\":\"/path/to/image/image_name1.jpg\",\n        \"label\": 0\n    },\n    {\n        \"image\":\"/path/to/image/image_name2.jpg\",\n        \"label\": 1\n    },\n    ....\n    {\n        \"image\":\"/path/to/image/image_namek.jpg\",\n        \"label\": 0\n    },\n]\n</code></pre>\n<h2>Training configuration</h2>\n<p>The training as performed with the following:\n- GPU: At least 12GB of GPU memory\n- Actual Model Input: 256 x 256 x 3\n- Optimizer: Adam\n- Learning Rate: 1e-3</p>\n<h3>Input</h3>\n<p>A three channel video frame</p>\n<h3>Output</h3>\n<p>Two Channels\n- Label 0: in body\n- Label 1: out body</p>\n<h2>Performance</h2>\n<p>Accuracy was used for evaluating the performance of the model. This model achieves an accuracy score of 0.99</p>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the training loss over 25 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_train_loss_v2.png\"/></p>\n<h4>Validation Accuracy</h4>\n<p><img alt=\"A graph showing the validation accuracy over 25 epochs.\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_val_accuracy_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>The <code>endoscopic_inbody_classification</code> bundle supports acceleration with TensorRT through the ONNX-TensorRT method. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">6.50</td>\n<td style=\"text-align: center;\">9.23</td>\n<td style=\"text-align: center;\">2.78</td>\n<td style=\"text-align: center;\">2.31</td>\n<td style=\"text-align: center;\">0.70</td>\n<td style=\"text-align: center;\">2.34</td>\n<td style=\"text-align: center;\">2.81</td>\n<td style=\"text-align: center;\">4.00</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">23.54</td>\n<td style=\"text-align: center;\">23.78</td>\n<td style=\"text-align: center;\">7.37</td>\n<td style=\"text-align: center;\">7.14</td>\n<td style=\"text-align: center;\">0.99</td>\n<td style=\"text-align: center;\">3.19</td>\n<td style=\"text-align: center;\">3.30</td>\n<td style=\"text-align: center;\">3.33</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>Currently, the only available method to accelerate this model is through ONNX-TensorRT. However, the Torch-TensorRT method is under development and will be available in the near future.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.5.3+cuda11.8\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.0\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run \\\n    --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<p>In addition, if using the 20 samples example dataset, the preprocessing script will divide the samples to 16 training samples, 2 validation samples and 2 test samples. However, pytorch multi-gpu training requires number of samples in dataloader larger than gpu numbers. Therefore, please use no more than 2 gpus to run this bundle if using the 20 samples example dataset.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<p>The classification result of every images in <code>test.json</code> will be printed to the screen.</p>\n<h4>Export checkpoint to TorchScript file:</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle trt_export --net_id network_def \\\n--filepath models/model_trt.ts --ckpt_file models/model.pt \\\n--meta_file configs/metadata.json --config_file configs/inference.json \\\n--precision &lt;fp32/fp16&gt;  --use_onnx \"True\" --use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132-7141. https://arxiv.org/pdf/1709.01507.pdf</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/endoscopic_inbody_classification/tree/0.5.0",
        "changelog": {
            "0.5.0": "update to huggingface hosting and fix missing dependencies",
            "0.4.9": "use monai 1.4 and update large files",
            "0.4.8": "update to use monai 1.3.1",
            "0.4.7": "add load_pretrain flag for infer",
            "0.4.6": "add output for inference",
            "0.4.5": "update with EnsureChannelFirstd and remove meta dict usage",
            "0.4.4": "fix the wrong GPU index issue of multi-node",
            "0.4.3": "add dataset dir example",
            "0.4.2": "update ONNX-TensorRT descriptions",
            "0.4.1": "update the model weights with the deterministic training",
            "0.4.0": "add the ONNX-TensorRT way of model conversion",
            "0.3.9": "fix mgpu finalize issue",
            "0.3.8": "enable deterministic training",
            "0.3.7": "adapt to BundleWorkflow interface",
            "0.3.6": "add name tag",
            "0.3.5": "fix a comment issue in the data_process script",
            "0.3.4": "add note for multi-gpu training with example dataset",
            "0.3.3": "enhance data preprocess script and readme file",
            "0.3.2": "restructure readme to match updated template",
            "0.3.1": "add workflow, train loss and validation accuracy figures",
            "0.3.0": "update dataset processing",
            "0.2.2": "update to use monai 1.0.1",
            "0.2.1": "enhance readme on commands example",
            "0.2.0": "update license files",
            "0.1.0": "complete the first version model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "breast_density_classification": {
        "model_name": "Breast density classification",
        "description": "A pre-trained model for classifying breast images (mammograms)  ",
        "authors": "Center for Augmented Intelligence in Imaging, Mayo Clinic Florida",
        "papers": [
            "Gupta, Vikash, et al. A multi-reconstruction study of breast density estimation using Deep Learning. arXiv preprint arXiv:2202.08238 (2022)."
        ],
        "version": "0.1.7",
        "model_id": "breast_density_classification",
        "readme": "<h1>Description</h1>\n<p>A pre-trained model for breast-density classification.</p>\n<h1>Model Overview</h1>\n<p>This model is trained using transfer learning on InceptionV3. The model weights were fine tuned using the Mayo Clinic Data. The details of training and data is outlined in https://arxiv.org/abs/2202.08238. The images should be resampled to a size [299, 299, 3] for training.\nA training pipeline will be added to the model zoo in near future.\nThe bundle does not support torchscript.</p>\n<h1>Sample Data</h1>\n<p>In the folder <code>sample_data</code> few example input images are stored for each category of images. These images are stored in jpeg format for sharing purpose.</p>\n<h1>Input and Output Formats</h1>\n<p>The input image should have the size [299, 299, 3]. For a dicom image which are single channel. The channel can be repeated 3 times.\nThe output is an array with probabilities for each of the four class.</p>\n<h1>Commands Example</h1>\n<p>Create a json file with names of all the input files. Execute the following command</p>\n<pre><code>python scripts/create_dataset.py -base_dir &lt;path to the bundle root dir&gt;/sample_data -output_file configs/sample_image_data.json\n</code></pre>\n<p>Change the <code>filename</code> for the field <code>data</code> with the absolute path for <code>sample_image_data.json</code></p>\n<h1>Add scripts folder to your python path as follows</h1>\n<pre><code>export PYTHONPATH=$PYTHONPATH:&lt;path to the bundle root dir&gt;/scripts\n</code></pre>\n<h1>Execute Inference</h1>\n<p>The inference can be executed as follows</p>\n<pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json configs/logging.conf\n</code></pre>\n<h1>Execute training</h1>\n<p>It is a work in progress and will be shared in the next version soon.</p>\n<h1>Contributors</h1>\n<p>This model is made available from Center for Augmented Intelligence in Imaging, Mayo Clinic Florida. For questions email Vikash Gupta (gupta.vikash@mayo.edu).</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/breast_density_classification/tree/0.1.7",
        "changelog": {
            "0.1.7": "update to huggingface hosting",
            "0.1.6": "Remove meta dict usage",
            "0.1.5": "Fixed duplication of input output format section",
            "0.1.4": "Changed Readme",
            "0.1.3": "Change input_dim from 229 to 299",
            "0.1.2": "black autofix format and add name tag",
            "0.1.1": "update license files",
            "0.1.0": "complete the model package"
        }
    },
    "mednist_reg": {
        "model_name": "MedNIST registration",
        "description": "This is an example of a ResNet and spatial transformer for hand xray image registration",
        "authors": "MONAI team",
        "papers": [],
        "version": "0.0.6",
        "model_id": "mednist_reg",
        "readme": "<h1>MedNIST Hand Image Registration</h1>\n<p>Based on <a href=\"https://github.com/Project-MONAI/tutorials/tree/main/2d_registration\">the tutorial of 2D registration</a></p>\n<h2>Downloading the Dataset</h2>\n<p>Download the dataset <a href=\"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\">from here</a> and extract the contents to a convenient location.</p>\n<p>The MedNIST dataset was gathered from several sets from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions\">TCIA</a>,\n<a href=\"http://rsnachallenges.cloudapp.net/competitions/4\">the RSNA Bone Age Challenge</a>,\nand <a href=\"https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest\">the NIH Chest X-ray dataset</a>.</p>\n<p>The dataset is kindly made available by <a href=\"https://www.mayo.edu/research/labs/radiology-informatics/overview\">Dr. Bradley J. Erickson M.D., Ph.D.</a> (Department of Radiology, Mayo Clinic)\nunder the Creative Commons <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\">CC BY-SA 4.0 license</a>.</p>\n<p>If you use the MedNIST dataset, please acknowledge the source.</p>\n<h2>Training</h2>\n<p>Training with same-subject image inputs</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --config_file configs/train.yaml --dataset_dir \"/workspace/data/MedNIST/Hand\"\n</code></pre>\n<p>Training with cross-subject image inputs</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training \\\n  --config_file configs/train.yaml \\\n  --dataset_dir \"/workspace/data/MedNIST/Hand\" \\\n  --cross_subjects True\n</code></pre>\n<p>Training from an existing checkpoint file, for example, <code>models/model_key_metric=-0.0734.pt</code>:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --config_file configs/train.yaml [...omitting other args] --ckpt \"models/model_key_metric=-0.0734.pt\"\n</code></pre>\n<h2>Inference</h2>\n<p>The following figure shows an intra-subject (<code>--cross_subjects False</code>) model inference results (Fixed, moving and predicted images from left to right)</p>\n<p><img alt=\"fixed\" src=\"./examples/008502_fixed_6.png\"/>\n<img alt=\"moving\" src=\"./examples/008502_moving_6.png\"/>\n<img alt=\"predicted\" src=\"./examples/008502_pred_6.png\"/></p>\n<p>The command shows an inference workflow with the checkpoint <code>\"models/model_key_metric=-0.0890.pt\"</code> and using device <code>\"cuda:1\"</code>:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run eval \\\n  --config_file configs/inference.yaml \\\n  --ckpt \"models/model_key_metric=-0.0890.pt\" \\\n  --logging_file configs/logging.conf \\\n  --device \"cuda:1\"\n</code></pre>\n<h2>Fine-tuning for cross-subject alignments</h2>\n<p>The following commands starts a finetuning workflow based on the checkpoint <code>\"models/model_key_metric=-0.0065.pt\"</code>\nfor <code>5</code> epochs using the global mutual information loss.</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training \\\n  --config_file configs/train.yaml \\\n  --cross_subjects True \\\n  --ckpt \"models/model_key_metric=-0.0065.pt\" \\\n  --lr 0.000001 \\\n  --trainer#loss_function \"@mutual_info_loss\" \\\n  --max_epochs 5\n</code></pre>\n<p>The following figure shows an inter-subject (<code>--cross_subjects True</code>) model inference results (Fixed, moving and predicted images from left to right)</p>\n<p><img alt=\"fixed\" src=\"./examples/008501_fixed_7.png\"/>\n<img alt=\"moving\" src=\"./examples/008504_moving_7.png\"/>\n<img alt=\"predicted\" src=\"./examples/008504_pred_7.png\"/></p>\n<h2>Visualize the first pair of images for debugging (requires <code>matplotlib</code>)</h2>\n<pre><code class=\"language-bash\">python -m monai.bundle run display --config_file configs/train.yaml\n</code></pre>\n<pre><code class=\"language-bash\">python -m monai.bundle run display --config_file configs/train.yaml --cross_subjects True\n</code></pre>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/mednist_reg/tree/0.0.6",
        "changelog": {
            "0.0.6": "update to huggingface hosting",
            "0.0.5": "update large files",
            "0.0.4": "add name tag",
            "0.0.3": "update to use monai 1.1.0",
            "0.0.2": "update to use rc1",
            "0.0.1": "Initial version"
        }
    },
    "pathology_nuclei_classification": {
        "model_name": "Pathology nuclei classification",
        "description": "A pre-trained model for Nuclei Classification within Haematoxylin & Eosin stained histology images",
        "authors": "MONAI team",
        "papers": [
            "S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. \"HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.\" Medical Image Analysis, Sept. 2019. https://doi.org/10.1016/j.media.2019.101563"
        ],
        "version": "0.2.1",
        "model_id": "pathology_nuclei_classification",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for classifying nuclei cells as the following types\n - Other\n - Inflammatory\n - Epithelial\n - Spindle-Shaped</p>\n<p>This model is trained using <a href=\"https://docs.monai.io/en/latest/networks.html#densenet121\">DenseNet121</a> over <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet\">ConSeP</a> dataset.</p>\n<h2>Data</h2>\n<p>The training dataset is from https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet</p>\n<pre><code class=\"language-commandline\">wget https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\nunzip -q consep_dataset.zip\n</code></pre>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_classification_dataset.jpeg\"/><br/></p>\n<h3>Preprocessing</h3>\n<p>After <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\">downloading this dataset</a>,\npython script <code>data_process.py</code> from <code>scripts</code> folder can be used to preprocess and generate the final dataset for training.</p>\n<pre><code class=\"language-commandline\">python scripts/data_process.py --input /path/to/data/CoNSeP --output /path/to/data/CoNSePNuclei\n</code></pre>\n<p>After generating the output files, please modify the <code>dataset_dir</code> parameter specified in <code>configs/train.json</code> and <code>configs/inference.json</code> to reflect the output folder which contains new dataset.json.</p>\n<p>Class values in dataset are</p>\n<ul>\n<li>1 = other</li>\n<li>2 = inflammatory</li>\n<li>3 = healthy epithelial</li>\n<li>4 = dysplastic/malignant epithelial</li>\n<li>5 = fibroblast</li>\n<li>6 = muscle</li>\n<li>7 = endothelial</li>\n</ul>\n<p>As part of pre-processing, the following steps are executed.</p>\n<ul>\n<li>Crop and Extract each nuclei Image + Label (128x128) based on the centroid given in the dataset.</li>\n<li>Combine classes 3 &amp; 4 into the epithelial class and 5,6 &amp; 7 into the spindle-shaped class.</li>\n<li>Update the label index for the target nuclie based on the class value</li>\n<li>Other cells which are part of the patch are modified to have label idex = 255</li>\n</ul>\n<p>Example <code>dataset.json</code> in output folder:</p>\n<pre><code class=\"language-json\">{\n  \"training\": [\n    {\n      \"image\": \"/workspace/data/CoNSePNuclei/Train/Images/train_1_3_0001.png\",\n      \"label\": \"/workspace/data/CoNSePNuclei/Train/Labels/train_1_3_0001.png\",\n      \"nuclei_id\": 1,\n      \"mask_value\": 3,\n      \"centroid\": [\n        64,\n        64\n      ]\n    }\n  ],\n  \"validation\": [\n    {\n      \"image\": \"/workspace/data/CoNSePNuclei/Test/Images/test_1_3_0001.png\",\n      \"label\": \"/workspace/data/CoNSePNuclei/Test/Labels/test_1_3_0001.png\",\n      \"nuclei_id\": 1,\n      \"mask_value\": 3,\n      \"centroid\": [\n        64,\n        64\n      ]\n    }\n  ]\n}\n</code></pre>\n<h2>Training configuration</h2>\n<p>The training was performed with the following:</p>\n<ul>\n<li>GPU: at least 12GB of GPU memory</li>\n<li>Actual Model Input: 4 x 128 x 128</li>\n<li>AMP: True</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-4</li>\n<li>Loss: torch.nn.CrossEntropyLoss</li>\n<li>Dataset Manager: CacheDataset</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Input</h2>\n<p>4 channels\n- 3 RGB channels\n- 1 signal channel (label mask)</p>\n<h2>Output</h2>\n<p>4 channels\n - 0 = Other\n - 1 = Inflammatory\n - 2 = Epithelial\n - 3 = Spindle-Shaped</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_classification_val_in_out.jpeg\"/></p>\n<h2>Performance</h2>\n<p>This model achieves the following F1 score on the validation data provided as part of the dataset:</p>\n<ul>\n<li>Train F1 score = 0.926</li>\n<li>Validation F1 score = 0.852</li>\n</ul>\n<hr/>\n<p>Confusion Metrics for <b>Validation</b> for individual classes are:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Other</th>\n<th>Inflammatory</th>\n<th>Epithelial</th>\n<th>Spindle-Shaped</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Precision</td>\n<td>0.6909</td>\n<td>0.7773</td>\n<td>0.9078</td>\n<td>0.8478</td>\n</tr>\n<tr>\n<td>Recall</td>\n<td>0.2754</td>\n<td>0.7831</td>\n<td>0.9533</td>\n<td>0.8514</td>\n</tr>\n<tr>\n<td>F1-score</td>\n<td>0.3938</td>\n<td>0.7802</td>\n<td>0.9300</td>\n<td>0.8496</td>\n</tr>\n</tbody>\n</table>\n<hr/>\n<p>Confusion Metrics for <b>Training</b> for individual classes are:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Other</th>\n<th>Inflammatory</th>\n<th>Epithelial</th>\n<th>Spindle-Shaped</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Precision</td>\n<td>0.8000</td>\n<td>0.9076</td>\n<td>0.9560</td>\n<td>0.9019</td>\n</tr>\n<tr>\n<td>Recall</td>\n<td>0.6512</td>\n<td>0.9028</td>\n<td>0.9690</td>\n<td>0.8989</td>\n</tr>\n<tr>\n<td>F1-score</td>\n<td>0.7179</td>\n<td>0.9052</td>\n<td>0.9625</td>\n<td>0.9004</td>\n</tr>\n</tbody>\n</table>\n<h4>Training Loss and F1</h4>\n<p>A graph showing the training Loss and F1-score over 100 epochs.</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_classification_train_loss_v3.png\"/> <br/>\n<img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_classification_train_f1_v3.png\"/> <br/></p>\n<h4>Validation F1</h4>\n<p>A graph showing the validation F1-score over 100 epochs.</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_classification_val_f1_v3.png\"/> <br/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">20.47</td>\n<td style=\"text-align: center;\">20.57</td>\n<td style=\"text-align: center;\">2.49</td>\n<td style=\"text-align: center;\">1.48</td>\n<td style=\"text-align: center;\">1.00</td>\n<td style=\"text-align: center;\">8.22</td>\n<td style=\"text-align: center;\">13.83</td>\n<td style=\"text-align: center;\">13.90</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">45</td>\n<td style=\"text-align: center;\">49</td>\n<td style=\"text-align: center;\">18</td>\n<td style=\"text-align: center;\">18</td>\n<td style=\"text-align: center;\">0.92</td>\n<td style=\"text-align: center;\">2.50</td>\n<td style=\"text-align: center;\">2.50</td>\n<td style=\"text-align: center;\">2.72</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config and <code>evaluate</code> config to execute multi-GPU evaluation:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. \"HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.\" Medical Image Analysis, Sept. 2019. [<a href=\"https://doi.org/10.1016/j.media.2019.101563\">doi</a>]</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pathology_nuclei_classification/tree/0.2.1",
        "changelog": {
            "0.2.1": "update to huggingface hosting",
            "0.2.0": "update issue for IgniteInfo",
            "0.1.9": "update tensorrt benchmark results",
            "0.1.8": "enable tensorrt",
            "0.1.7": "update to use monai 1.3.1",
            "0.1.6": "set image_only to False",
            "0.1.5": "add support for TensorRT conversion and inference",
            "0.1.4": "fix the wrong GPU index issue of multi-node",
            "0.1.3": "remove error dollar symbol in readme",
            "0.1.2": "add RAM warning",
            "0.1.1": "enable deterministic eval and inference",
            "0.1.0": "Update deterministic results",
            "0.0.9": "Update README Formatting",
            "0.0.8": "enable deterministic training",
            "0.0.7": "update benchmark on A100",
            "0.0.6": "adapt to BundleWorkflow interface",
            "0.0.5": "add name tag",
            "0.0.4": "Fix evaluation",
            "0.0.3": "Update to use MONAI 1.1.0",
            "0.0.2": "Update The Torch Vision Transform",
            "0.0.1": "initialize the model package structure"
        }
    },
    "pathology_nuclick_annotation": {
        "model_name": "Pathology nuclick annotation",
        "description": "A pre-trained model for segmenting nuclei cells with user clicks/interactions",
        "authors": "MONAI team",
        "papers": [
            "Koohbanani, Navid Alemi, et al. \"NuClick: A Deep Learning Framework for Interactive Segmentation of Microscopy Images.\" https://arxiv.org/abs/2005.14511",
            "S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. \"HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.\" Medical Image Analysis, Sept. 2019. https://doi.org/10.1016/j.media.2019.101563",
            "NuClick PyTorch Implementation, https://github.com/mostafajahanifar/nuclick_torch"
        ],
        "version": "0.2.2",
        "model_id": "pathology_nuclick_annotation",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for segmenting nuclei cells with user clicks/interactions.</p>\n<p><img alt=\"nuclick\" src=\"https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/11.gif\"/>\n<img alt=\"nuclick\" src=\"https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/33.gif\"/>\n<img alt=\"nuclick\" src=\"https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/22.gif\"/></p>\n<p>This model is trained using <a href=\"https://docs.monai.io/en/latest/networks.html#basicunet\">BasicUNet</a> over <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet\">ConSeP</a> dataset.</p>\n<h2>Data</h2>\n<p>The training dataset is from https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet</p>\n<pre><code class=\"language-commandline\">wget https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\nunzip -q consep_dataset.zip\n</code></pre>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclick_annotation_dataset.jpeg\"/><br/></p>\n<h3>Preprocessing</h3>\n<p>After <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\">downloading this dataset</a>,\npython script <code>data_process.py</code> from <code>scripts</code> folder can be used to preprocess and generate the final dataset for training.</p>\n<pre><code>python scripts/data_process.py --input /path/to/data/CoNSeP --output /path/to/data/CoNSePNuclei\n</code></pre>\n<p>After generating the output files, please modify the <code>dataset_dir</code> parameter specified in <code>configs/train.json</code> and <code>configs/inference.json</code> to reflect the output folder which contains new dataset.json.</p>\n<p>Class values in dataset are</p>\n<ul>\n<li>1 = other</li>\n<li>2 = inflammatory</li>\n<li>3 = healthy epithelial</li>\n<li>4 = dysplastic/malignant epithelial</li>\n<li>5 = fibroblast</li>\n<li>6 = muscle</li>\n<li>7 = endothelial</li>\n</ul>\n<p>As part of pre-processing, the following steps are executed.</p>\n<ul>\n<li>Crop and Extract each nuclei Image + Label (128x128) based on the centroid given in the dataset.</li>\n<li>Combine classes 3 &amp; 4 into the epithelial class and 5,6 &amp; 7 into the spindle-shaped class.</li>\n<li>Update the label index for the target nuclei based on the class value</li>\n<li>Other cells which are part of the patch are modified to have label idx = 255</li>\n</ul>\n<p>Example dataset.json</p>\n<pre><code class=\"language-json\">{\n  \"training\": [\n    {\n      \"image\": \"/workspace/data/CoNSePNuclei/Train/Images/train_1_3_0001.png\",\n      \"label\": \"/workspace/data/CoNSePNuclei/Train/Labels/train_1_3_0001.png\",\n      \"nuclei_id\": 1,\n      \"mask_value\": 3,\n      \"centroid\": [\n        64,\n        64\n      ]\n    }\n  ],\n  \"validation\": [\n    {\n      \"image\": \"/workspace/data/CoNSePNuclei/Test/Images/test_1_3_0001.png\",\n      \"label\": \"/workspace/data/CoNSePNuclei/Test/Labels/test_1_3_0001.png\",\n      \"nuclei_id\": 1,\n      \"mask_value\": 3,\n      \"centroid\": [\n        64,\n        64\n      ]\n    }\n  ]\n}\n</code></pre>\n<h2>Training Configuration</h2>\n<p>The training was performed with the following:</p>\n<ul>\n<li>GPU: at least 12GB of GPU memory</li>\n<li>Actual Model Input: 5 x 128 x 128</li>\n<li>AMP: True</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-4</li>\n<li>Loss: DiceLoss</li>\n</ul>\n<h3>Memory Consumption</h3>\n<ul>\n<li>Dataset Manager: CacheDataset</li>\n<li>Data Size: 13,136 PNG images</li>\n<li>Cache Rate: 1.0</li>\n<li>Single GPU - System RAM Usage: 4.7G</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Input</h2>\n<p>5 channels\n- 3 RGB channels\n- +ve signal channel (this nuclei)\n- -ve signal channel (other nuclei)</p>\n<h2>Output</h2>\n<p>2 channels\n - 0 = Background\n - 1 = Nuclei</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclick_annotation_train_in_out.jpeg\"/></p>\n<h2>Performance</h2>\n<p>This model achieves the following Dice score on the validation data provided as part of the dataset:</p>\n<ul>\n<li>Train Dice score = 0.89</li>\n<li>Validation Dice score = 0.85</li>\n</ul>\n<h4>Training Loss and Dice</h4>\n<p>A graph showing the training Loss and Dice over 50 epochs.</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclick_annotation_train_loss_v2.png\"/> <br/>\n<img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclick_annotation_train_dice_v2.png\"/> <br/></p>\n<h4>Validation Dice</h4>\n<p>A graph showing the validation mean Dice over 50 epochs.</p>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclick_annotation_val_dice_v2.png\"/> <br/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">3.27</td>\n<td style=\"text-align: center;\">4.31</td>\n<td style=\"text-align: center;\">2.12</td>\n<td style=\"text-align: center;\">1.73</td>\n<td style=\"text-align: center;\">0.76</td>\n<td style=\"text-align: center;\">1.54</td>\n<td style=\"text-align: center;\">1.89</td>\n<td style=\"text-align: center;\">2.49</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">705.32</td>\n<td style=\"text-align: center;\">752.64</td>\n<td style=\"text-align: center;\">290.45</td>\n<td style=\"text-align: center;\">347.07</td>\n<td style=\"text-align: center;\">0.94</td>\n<td style=\"text-align: center;\">2.43</td>\n<td style=\"text-align: center;\">2.03</td>\n<td style=\"text-align: center;\">2.17</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.6.1+cuda12.0\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.1\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config and <code>evaluate</code> config to execute multi-GPU evaluation:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt; --use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Koohbanani, Navid Alemi, et al. \"NuClick: a deep learning framework for interactive segmentation of microscopic images.\" Medical Image Analysis 65 (2020): 101771. https://arxiv.org/abs/2005.14511.</p>\n<p>[2] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. \"HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.\" Medical Image Analysis, Sept. 2019. [<a href=\"https://doi.org/10.1016/j.media.2019.101563\">doi</a>]</p>\n<p>[3] NuClick <a href=\"https://github.com/mostafajahanifar/nuclick_torch\">PyTorch</a> Implementation</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pathology_nuclick_annotation/tree/0.2.2",
        "changelog": {
            "0.2.2": "update to huggingface hosting",
            "0.2.1": "update issue for IgniteInfo",
            "0.2.0": "use monai 1.4 and update large files",
            "0.1.9": "update to use monai 1.3.1",
            "0.1.8": "add load_pretrain flag for infer",
            "0.1.7": "add checkpoint loader for infer",
            "0.1.6": "set image_only to False",
            "0.1.5": "add support for TensorRT conversion and inference",
            "0.1.4": "fix the wrong GPU index issue of multi-node",
            "0.1.3": "remove error dollar symbol in readme",
            "0.1.2": "add RAM usage with CachDataset",
            "0.1.1": "deterministic retrain benchmark and add link",
            "0.1.0": "fix mgpu finalize issue",
            "0.0.9": "Update README Formatting",
            "0.0.8": "enable deterministic training",
            "0.0.7": "Update with figure links",
            "0.0.6": "adapt to BundleWorkflow interface",
            "0.0.5": "add name tag",
            "0.0.4": "Fix evaluation",
            "0.0.3": "Update to use MONAI 1.1.0",
            "0.0.2": "Update The Torch Vision Transform",
            "0.0.1": "initialize the model package structure"
        }
    },
    "pathology_nuclei_segmentation_classification": {
        "model_name": "Nuclear segmentation and classification",
        "description": "A simultaneous segmentation and classification of nuclei within multitissue histology images based on CoNSeP data",
        "authors": "MONAI team",
        "papers": [
            "Simon Graham. 'HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.' Medical Image Analysis, 2019. https://arxiv.org/abs/1812.06499"
        ],
        "version": "0.2.7",
        "model_id": "pathology_nuclei_segmentation_classification",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for simultaneous segmentation and classification of nuclei within multi-tissue histology images based on CoNSeP data. The details of the model can be found in [1].</p>\n<p>The model is trained to simultaneously segment and classify nuclei, and a two-stage training approach is utilized:</p>\n<ul>\n<li>Initialize the model with pre-trained weights, and train the decoder only for 50 epochs.</li>\n<li>Finetune all layers for another 50 epochs.</li>\n</ul>\n<p>There are two training modes in total. If \"original\" mode is specified, [270, 270] and [80, 80] are used for <code>patch_size</code> and <code>out_size</code> respectively. If \"fast\" mode is specified, [256, 256] and [164, 164] are used for <code>patch_size</code> and <code>out_size</code> respectively. The results shown below are based on the \"fast\" mode.</p>\n<p>In this bundle, the first stage is trained with pre-trained weights from some internal data. The <a href=\"https://github.com/vqdang/hover_net\">original author's repo</a> and <a href=\"https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#ResNet18_Weights\">torchvison</a> also provide pre-trained weights but for non-commercial use.\nEach user is responsible for checking the content of models/datasets and the applicable licenses and determining if suitable for the intended use.</p>\n<p>If you want to train the first stage with pre-trained weights, just specify <code>--network_def#pretrained_url &lt;your pretrain weights URL&gt;</code> in the training command below, such as <a href=\"https://download.pytorch.org/models/resnet18-f37072fd.pth\">ImageNet</a>.</p>\n<p><img alt=\"Model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_hovernet_pipeline.png\"/></p>\n<h2>Data</h2>\n<p>The training data is from <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/\">https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/</a>.</p>\n<ul>\n<li>Target: segment instance-level nuclei and classify the nuclei type</li>\n<li>Task: Segmentation and classification</li>\n<li>Modality: RGB images</li>\n<li>Size: 41 image tiles (2009 patches)</li>\n</ul>\n<p>The provided labelled data was partitioned, based on the original split, into training (27 tiles) and testing (14 tiles) datasets.</p>\n<p>You can download the dataset by using this command:</p>\n<pre><code>wget https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\nunzip consep_dataset.zip\n</code></pre>\n<h3>Preprocessing</h3>\n<p>After download the <a href=\"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip\">datasets</a>, please run <code>scripts/prepare_patches.py</code> to prepare patches from tiles. Prepared patches are saved in <code>&lt;your concep dataset path&gt;</code>/Prepared. The implementation is referring to <a href=\"https://github.com/vqdang/hover_net\">https://github.com/vqdang/hover_net</a>. The command is like:</p>\n<pre><code>python scripts/prepare_patches.py --root &lt;your concep dataset path&gt;\n</code></pre>\n<h2>Training configuration</h2>\n<p>This model utilized a two-stage approach. The training was performed with the following:</p>\n<ul>\n<li>GPU: At least 24GB of GPU memory.</li>\n<li>Actual Model Input: 256 x 256</li>\n<li>AMP: True</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-4</li>\n<li>Loss: HoVerNetLoss</li>\n<li>Dataset Manager: CacheDataset</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Input</h2>\n<p>Input: RGB images</p>\n<h2>Output</h2>\n<p>Output: a dictionary with the following keys:</p>\n<ol>\n<li>nucleus_prediction: predict whether or not a pixel belongs to the nuclei or background</li>\n<li>horizontal_vertical: predict the horizontal and vertical distances of nuclear pixels to their centres of mass</li>\n<li>type_prediction: predict the type of nucleus for each pixel</li>\n</ol>\n<h2>Performance</h2>\n<p>The achieved metrics on the validation data are:</p>\n<p>Fast mode:\n- Binary Dice: 0.8291\n- PQ: 0.4973\n- F1d: 0.7417</p>\n<p>Note:\n- Binary Dice is calculated based on the whole input. PQ and F1d were calculated from https://github.com/vqdang/hover_net#inference.\n- This bundle is non-deterministic because of the bilinear interpolation used in the network. Therefore, reproducing the training process may not get exactly the same performance.\nPlease refer to https://pytorch.org/docs/stable/notes/randomness.html#reproducibility for more details about reproducibility.</p>\n<h4>Training Loss and Dice</h4>\n<p>stage1:\n<img alt=\"A graph showing the training loss and the mean dice over 50 epochs in stage1\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_segmentation_classification_train_stage0_v2.png\"/></p>\n<p>stage2:\n<img alt=\"A graph showing the training loss and the mean dice over 50 epochs in stage2\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_segmentation_classification_train_stage1_v2.png\"/></p>\n<h4>Validation Dice</h4>\n<p>stage1:</p>\n<p><img alt=\"A graph showing the validation mean dice over 50 epochs in stage1\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_segmentation_classification_val_stage0_v2.png\"/></p>\n<p>stage2:</p>\n<p><img alt=\"A graph showing the validation mean dice over 50 epochs in stage2\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_segmentation_classification_val_stage1_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">24.55</td>\n<td style=\"text-align: center;\">20.14</td>\n<td style=\"text-align: center;\">10.85</td>\n<td style=\"text-align: center;\">5.63</td>\n<td style=\"text-align: center;\">1.22</td>\n<td style=\"text-align: center;\">2.26</td>\n<td style=\"text-align: center;\">4.36</td>\n<td style=\"text-align: center;\">3.58</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">3451</td>\n<td style=\"text-align: center;\">3312</td>\n<td style=\"text-align: center;\">1318</td>\n<td style=\"text-align: center;\">878</td>\n<td style=\"text-align: center;\">1.04</td>\n<td style=\"text-align: center;\">2.62</td>\n<td style=\"text-align: center;\">3.93</td>\n<td style=\"text-align: center;\">3.77</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training, the evaluation during the training were evaluated on patches:</h4>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<ul>\n<li>Run first stage</li>\n</ul>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --stage 0 --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<ul>\n<li>Run second stage</li>\n</ul>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --network_def#freeze_encoder False --stage 1 --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<ul>\n<li>Run first stage</li>\n</ul>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\" --batch_size 8 --network_def#freeze_encoder True --stage 0\n</code></pre>\n<ul>\n<li>Run second stage</li>\n</ul>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\" --batch_size 4 --network_def#freeze_encoder False --stage 1\n</code></pre>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model, here we evaluated dice from the whole input instead of the patches:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, Nasir Rajpoot, Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images, Medical Image Analysis, 2019 https://doi.org/10.1016/j.media.2019.101563</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pathology_nuclei_segmentation_classification/tree/0.2.7",
        "changelog": {
            "0.2.7": "update to huggingface hosting",
            "0.2.6": "update tensorrt benchmark results",
            "0.2.5": "enable tensorrt",
            "0.2.4": "update to use monai 1.3.1",
            "0.2.3": "remove meta_dict usage",
            "0.2.2": "add requiremnts for torchvision",
            "0.2.1": "fix the wrong GPU index issue of multi-node",
            "0.2.0": "Update README for how to download dataset",
            "0.1.9": "add RAM warning",
            "0.1.8": "Update README for pretrained weights and save metrics in evaluate",
            "0.1.7": "Update README Formatting",
            "0.1.6": "add non-deterministic note",
            "0.1.5": "update benchmark on A100",
            "0.1.4": "adapt to BundleWorkflow interface",
            "0.1.3": "add name tag",
            "0.1.2": "update the workflow figure",
            "0.1.1": "update to use monai 1.1.0",
            "0.1.0": "complete the model package"
        }
    },
    "wholeBody_ct_segmentation": {
        "model_name": "Whole body CT segmentation",
        "description": "A pre-trained SegResNet model for volumetric (3D) segmentation of the 104 whole body segments",
        "authors": "MONAI team",
        "papers": [
            "Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868.",
            "Myronenko, A., Siddiquee, M.M.R., Yang, D., He, Y. and Xu, D., 2022. Automated head and neck tumor segmentation from 3D PET/CT. arXiv preprint arXiv:2209.10809.",
            "Tang, Y., Gao, R., Lee, H.H., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Abramson, R.G. and Bao, S., 2021. High-resolution 3D abdominal segmentation with random patch network fusion. Medical image analysis, 69, p.101894."
        ],
        "version": "0.2.6",
        "model_id": "wholeBody_ct_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>Body CT segmentation models are evolving. Starting from abdominal multi-organ segmentation model [1]. Now the community is developing hundreds of target anatomies. In this bundle, we provide re-trained models for (3D) segmentation of 104 whole-body segments.</p>\n<p>This model is trained using the SegResNet [3] network. The model is trained using TotalSegmentator datasets [2].</p>\n<p><img alt=\"structures\" src=\"https://github.com/wasserth/TotalSegmentator/blob/30cfde5e7dcd164cd47435f7d3d85505e8e7d7bb/resources/imgs/overview_classes.png\"/></p>\n<p>Figure source from the TotalSegmentator [2].</p>\n<h3>MONAI Label Showcase</h3>\n<ul>\n<li>We highlight the use of this bundle to use and visualize in MONAI Label + 3D Slicer integration.</li>\n</ul>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_wholeBody_ct_segmentation_monailabel.png\"/> <br/></p>\n<h2>Data</h2>\n<p>The training set is the 104 whole-body structures from the TotalSegmentator released datasets. Users can find more details on the datasets at https://github.com/wasserth/TotalSegmentator. All rights and licenses are reserved to the original authors.</p>\n<ul>\n<li>Target: 104 structures</li>\n<li>Modality: CT</li>\n<li>Source: TotalSegmentator</li>\n<li>Challenge: Large volumes of structures in CT images</li>\n</ul>\n<h3>Preprocessing</h3>\n<p>To use the bundle, users need to download the data and merge all annotated labels into one NIFTI file. Each file contains 0-104 values, each value represents one anatomy class. We provide sample datasets and step-by-step instructions on how to get prepared:</p>\n<p>Instruction on how to start with the prepared sample dataset:</p>\n<ol>\n<li>Download the sample set with this <a href=\"https://drive.google.com/file/d/1DtDmERVMjks1HooUhggOKAuDm0YIEunG/view?usp=share_link\">link</a>.</li>\n<li>Unzip the dataset into a workspace folder.</li>\n<li>There will be three sub-folders, each with several preprocessed CT volumes:<ul>\n<li>imagesTr: 20 samples of training scans and validation scans.</li>\n<li>labelsTr: 20 samples of pre-processed label files.</li>\n<li>imagesTs: 5 samples of sample testing scans.</li>\n</ul>\n</li>\n<li>Usage: users can add <code>--dataset_dir &lt;totalSegmentator_mergedLabel_samples&gt;</code> to the bundle run command to specify the data path.</li>\n</ol>\n<p>Instruction on how to merge labels with the raw dataset:</p>\n<ul>\n<li>There are 104 binary masks associated with each CT scan, each mask corresponds to anatomy. These pixel-level labels are class-exclusive, users can assign each anatomy a class number then merge to a single NIFTI file as the ground truth label file. The order of anatomies can be found <a href=\"https://github.com/Project-MONAI/model-zoo/blob/dev/models/wholeBody_ct_segmentation/configs/metadata.json\">here</a>.</li>\n</ul>\n<h2>Training Configuration</h2>\n<p>The segmentation of 104 tissues is formulated as voxel-wise multi-label segmentation. The model is optimized with the gradient descent method minimizing Dice + cross-entropy loss between the predicted mask and ground truth segmentation.</p>\n<p>The training was performed with the following:</p>\n<ul>\n<li>GPU: 48 GB of GPU memory</li>\n<li>Actual Model Input: 96 x 96 x 96</li>\n<li>AMP: True</li>\n<li>Optimizer: AdamW</li>\n<li>Learning Rate: 1e-4</li>\n<li>Loss: DiceCELoss</li>\n</ul>\n<h2>Evaluation Configuration</h2>\n<p>The model predicts 105 channels output at the same time using softmax and argmax. It requires higher GPU memory when calculating\n metrics between predicted masked and ground truth. The consumption of hardware requirements, such as GPU memory is dependent on the input CT volume size.</p>\n<p>The recommended evaluation configuration and the metrics were acquired with the following hardware:</p>\n<ul>\n<li>GPU: equal to or larger than 48 GB of GPU memory</li>\n<li>Model: high resolution model pre-trained at a slice thickness of 1.5 mm.</li>\n</ul>\n<p>Note: there are two pre-trained models provided. The default is the high resolution model, evaluation pipeline at slice thickness of <strong>1.5mm</strong>,\nusers can use the lower resolution model if out of memory (OOM) occurs, which the model is pre-trained with CT scans at a slice thickness of <strong>3.0mm</strong>.</p>\n<p>Users can also use the inference pipeline for predicted masks, we provide detailed GPU memory consumption in the following sections.</p>\n<h3>Memory Consumption</h3>\n<ul>\n<li>Dataset Manager: CacheDataset</li>\n<li>Data Size: 1000 3D Volumes</li>\n<li>Cache Rate: 0.4</li>\n<li>Single GPU - System RAM Usage: 83G</li>\n<li>Multi GPU (8 GPUs) - System RAM Usage: 666G</li>\n</ul>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with CacheDataset, you can either switch to a regular Dataset class or lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h3>Input</h3>\n<p>One channel\n- CT image</p>\n<h3>Output</h3>\n<p>105 channels\n- Label 0: Background (everything else)\n- label 1-105: Foreground classes (104)</p>\n<h2>Resource Requirements and Latency Benchmarks</h2>\n<h3>GPU Consumption Warning</h3>\n<p>The model is trained with 104 classes in single instance, for predicting 104 structures, the GPU consumption can be large.</p>\n<p>For inference pipeline, please refer to the following section for benchmarking results. Normally, a CT scans with 300 slices will take about 27G memory, if your CT is larger, please prepare larger GPU memory or use CPU for inference.</p>\n<h3>High-Resolution and Low-Resolution Models</h3>\n<p>We retrained two versions of the totalSegmentator models, following the original paper and implementation.\nTo meet multiple demands according to computation resources and performance, we provide a 1.5 mm model and a 3.0 mm model, both models are trained with 104 foreground output channels.</p>\n<p>In this bundle, we configured a parameter called <code>highres</code>, users can set it to <code>true</code> when using 1.5 mm model, and set it to <code>false</code> to use the 3.0 mm model. The high-resolution model is named <code>model.pt</code> by default, the low-resolution model is named <code>model_lowres.pt</code>.</p>\n<p>In MONAI Label use case, users can set the parameter in 3D Slicer plugin to control which model to infer and train.</p>\n<ul>\n<li>Pretrained Checkpoints</li>\n<li>1.5 mm model: <a href=\"https://drive.google.com/file/d/1PHpFWboimEXmMSe2vBra6T8SaCMC2SHT/view?usp=share_link\">Download link</a></li>\n<li>3.0 mm model: <a href=\"https://drive.google.com/file/d/1c3osYscnr6710ObqZZS8GkZJQlWlc7rt/view?usp=share_link\">Download link</a></li>\n</ul>\n<p>Latencies and memory performance of using the bundle with MONAI Label:</p>\n<p>Tested Image Dimension: <strong>(512, 512, 397)</strong>, the slice thickness is <strong>1.5mm</strong> in this case. After resample to <strong>1.5</strong> isotropic resolution, the dimension is   <strong>(287, 287, 397)</strong></p>\n<h3>1.5 mm (highres) model (Single Model with 104 foreground classes)</h3>\n<p>Benchmarking on GPU: Memory: <strong>28.73G</strong></p>\n<ul>\n<li><code>++ Latencies =&gt; Total: 6.0277; Pre: 1.6228; Inferer: 4.1153; Invert: 0.0000; Post: 0.0897; Write: 0.1995</code></li>\n</ul>\n<p>Benchmarking on CPU: Memory: <strong>26G</strong></p>\n<ul>\n<li><code>++ Latencies =&gt; Total: 38.3108; Pre: 1.6643; Inferer: 30.3018; Invert: 0.0000; Post: 6.1656; Write: 0.1786</code></li>\n</ul>\n<h3>3.0 mm (lowres) model (single model with 104 foreground classes)</h3>\n<p>GPU: Memory: <strong>5.89G</strong></p>\n<ul>\n<li><code>++ Latencies =&gt; Total: 1.9993; Pre: 1.2363; Inferer: 0.5207; Invert: 0.0000; Post: 0.0358; Write: 0.2060</code></li>\n</ul>\n<p>CPU: Memory: <strong>2.3G</strong></p>\n<ul>\n<li><code>++ Latencies =&gt; Total: 6.6138; Pre: 1.3192; Inferer: 3.6746; Invert: 0.0000; Post: 1.4431; Write: 0.1760</code></li>\n</ul>\n<h2>Performance</h2>\n<h3>1.5 mm Model Training</h3>\n<h4>Training Accuracy</h4>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_wholeBody_ct_segmentation_train_accuracy.png\"/> <br/></p>\n<h4>Validation Dice</h4>\n<p><img alt=\"\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_wholeBody_ct_segmentation_15mm_validation.png\"/> <br/></p>\n<p>Please note that this bundle is non-deterministic because of the trilinear interpolation used in the network. Therefore, reproducing the training process may not get exactly the same performance.\nPlease refer to https://pytorch.org/docs/stable/notes/randomness.html#reproducibility for more details about reproducibility.</p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_fp32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_fp32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup fp32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">88.20</td>\n<td style=\"text-align: center;\">37.1</td>\n<td style=\"text-align: center;\">39.2</td>\n<td style=\"text-align: center;\">36.9</td>\n<td style=\"text-align: center;\">2.38</td>\n<td style=\"text-align: center;\">2.25</td>\n<td style=\"text-align: center;\">2.39</td>\n<td style=\"text-align: center;\">1.01</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">3717.14</td>\n<td style=\"text-align: center;\">2596.77</td>\n<td style=\"text-align: center;\">2517.29</td>\n<td style=\"text-align: center;\">2501.37</td>\n<td style=\"text-align: center;\">1.43</td>\n<td style=\"text-align: center;\">1.48</td>\n<td style=\"text-align: center;\">1.49</td>\n<td style=\"text-align: center;\">1.04</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_fp32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_fp32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup fp32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 8.6.1+cuda12.0\n - Torch-TensorRT Version: 1.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.8.10\n - CUDA version: 12.1\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config and <code>evaluate</code> config to execute multi-GPU evaluation:</h4>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with Data Samples:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json --datalist \"['sampledata/imagesTr/s0037.nii.gz','sampledata/imagesTr/s0038.nii.gz']\"\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/model_trt.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json --precision &lt;fp32/fp16&gt; --use_trace \"True\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Tang, Y., Gao, R., Lee, H.H., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Abramson, R.G. and Bao, S., 2021. High-resolution 3D abdominal segmentation with random patch network fusion. Medical image analysis, 69, p.101894.</p>\n<p>[2] Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868.</p>\n<p>[3] Myronenko, A., Siddiquee, M.M.R., Yang, D., He, Y. and Xu, D., 2022. Automated head and neck tumor segmentation from 3D PET/CT. arXiv preprint arXiv:2209.10809.</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/wholeBody_ct_segmentation/tree/0.2.6",
        "changelog": {
            "0.2.6": "update to huggingface hosting",
            "0.2.5": "use monai 1.4 and update large files",
            "0.2.4": "update to use monai 1.3.1",
            "0.2.3": "add load_pretrain flag for infer",
            "0.2.2": "add checkpoint loader for infer",
            "0.2.1": "remove meta_dict usage",
            "0.2.0": "add support for TensorRT conversion and inference",
            "0.1.9": "fix the wrong GPU index issue of multi-node",
            "0.1.8": "Update evalaute doc, GPU usage details, and dataset preparation instructions",
            "0.1.7": "remove error dollar symbol in readme",
            "0.1.6": "add RAM usage with CacheDataset and GPU consumtion warning",
            "0.1.5": "fix mgpu finalize issue",
            "0.1.4": "Update README Formatting",
            "0.1.3": "add non-deterministic note",
            "0.1.2": "Update figure with links",
            "0.1.1": "adapt to BundleWorkflow interface and val metric",
            "0.1.0": "complete the model package",
            "0.0.1": "initialize the model package structure"
        }
    },
    "brats_mri_generative_diffusion": {
        "model_name": "BraTS MRI image latent diffusion generation",
        "description": "A generative model for creating 3D brain MRI from Gaussian noise based on BraTS dataset",
        "authors": "MONAI team",
        "papers": [],
        "version": "1.1.3",
        "model_id": "brats_mri_generative_diffusion",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for volumetric (3D) Brats MRI 3D Latent Diffusion Generative Model.</p>\n<p>This model is trained on BraTS 2016 and 2017 data from <a href=\"http://medicaldecathlon.com/\">Medical Decathlon</a>, using the Latent diffusion model [1].</p>\n<p><img alt=\"model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_network.png\"/></p>\n<p>This model is a generator for creating images like the Flair MRIs based on BraTS 2016 and 2017 data. It was trained as a 3d latent diffusion model and accepts Gaussian random noise as inputs to produce an image output. The <code>train_autoencoder.json</code> file describes the training process of the variational autoencoder with GAN loss. The <code>train_diffusion.json</code> file describes the training process of the 3D latent diffusion model.</p>\n<p>In this bundle, the autoencoder uses perceptual loss, which is based on ResNet50 with pre-trained weights (the network is frozen and will not be trained in the bundle). In default, the <code>pretrained</code> parameter is specified as <code>False</code> in <code>train_autoencoder.json</code>. To ensure correct training, changing the default settings is necessary. There are two ways to utilize pretrained weights:\n1. if set <code>pretrained</code> to <code>True</code>, ImageNet pretrained weights from <a href=\"https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#ResNet50_Weights\">torchvision</a> will be used. However, the weights are for non-commercial use only.\n2. if set <code>pretrained</code> to <code>True</code> and specifies the <code>perceptual_loss_model_weights_path</code> parameter, users are able to load weights from a local path. This is the way this bundle used to train, and the pre-trained weights are from some internal data.</p>\n<p>Please note that each user is responsible for checking the data source of the pre-trained models, the applicable licenses, and determining if suitable for the intended use.</p>\n<h4>Example synthetic image</h4>\n<p>An example result from inference is shown below:\n<img alt=\"Example synthetic image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_example_generation_v2.png\"/></p>\n<p><strong>This is a demonstration network meant to just show the training process for this sort of network with MONAI. To achieve better performance, users need to use larger dataset like <a href=\"https://www.synapse.org/#!Synapse:syn25829067/wiki/610865\">Brats 2021</a> and have GPU with memory larger than 32G to enable larger networks and attention layers.</strong></p>\n<h2>Data</h2>\n<p>The training data is BraTS 2016 and 2017 from the Medical Segmentation Decathalon. Users can find more details on the dataset (<code>Task01_BrainTumour</code>) at http://medicaldecathlon.com/.</p>\n<ul>\n<li>Target: Image Generation</li>\n<li>Task: Synthesis</li>\n<li>Modality: MRI</li>\n<li>Size: 388 3D volumes (1 channel used)</li>\n</ul>\n<h2>Training Configuration</h2>\n<p>If you have a GPU with less than 32G of memory, you may need to decrease the batch size when training. To do so, modify the <code>train_batch_size</code> parameter in the <a href=\"../configs/train_autoencoder.json\">configs/train_autoencoder.json</a> and <a href=\"../configs/train_diffusion.json\">configs/train_diffusion.json</a> configuration files.</p>\n<h3>Training Configuration of Autoencoder</h3>\n<p>The autoencoder was trained using the following configuration:</p>\n<ul>\n<li>GPU: at least 32GB GPU memory</li>\n<li>Actual Model Input: 112 x 128 x 80</li>\n<li>AMP: False</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-5</li>\n<li>Loss: L1 loss, perceptual loss, KL divergence loss, adversarial loss, GAN BCE loss</li>\n</ul>\n<h4>Input</h4>\n<p>1 channel 3D MRI Flair patches</p>\n<h4>Output</h4>\n<ul>\n<li>1 channel 3D MRI reconstructed patches</li>\n<li>8 channel mean of latent features</li>\n<li>8 channel standard deviation of latent features</li>\n</ul>\n<h3>Training Configuration of Diffusion Model</h3>\n<p>The latent diffusion model was trained using the following configuration:</p>\n<ul>\n<li>GPU: at least 32GB GPU memory</li>\n<li>Actual Model Input: 36 x 44 x 28</li>\n<li>AMP: False</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-5</li>\n<li>Loss: MSE loss</li>\n</ul>\n<h4>Training Input</h4>\n<ul>\n<li>8 channel noisy latent features</li>\n<li>a long int that indicates the time step</li>\n</ul>\n<h4>Training Output</h4>\n<p>8 channel predicted added noise</p>\n<h4>Inference Input</h4>\n<p>8 channel noise</p>\n<h4>Inference Output</h4>\n<p>8 channel denoised latent features</p>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with data loading, you can lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Performance</h2>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the autoencoder training curve\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_train_autoencoder_loss_v2.png\"/></p>\n<p><img alt=\"A graph showing the latent diffusion training curve\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_train_diffusion_loss_v2.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation (diffusion)</td>\n<td style=\"text-align: center;\">44.57</td>\n<td style=\"text-align: center;\">44.59</td>\n<td style=\"text-align: center;\">40.89</td>\n<td style=\"text-align: center;\">18.79</td>\n<td style=\"text-align: center;\">1.00</td>\n<td style=\"text-align: center;\">1.09</td>\n<td style=\"text-align: center;\">2.37</td>\n<td style=\"text-align: center;\">2.37</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">model computation (autoencoder)</td>\n<td style=\"text-align: center;\">96.29</td>\n<td style=\"text-align: center;\">97.01</td>\n<td style=\"text-align: center;\">78.51</td>\n<td style=\"text-align: center;\">44.03</td>\n<td style=\"text-align: center;\">0.99</td>\n<td style=\"text-align: center;\">1.23</td>\n<td style=\"text-align: center;\">2.19</td>\n<td style=\"text-align: center;\">2.20</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">2826</td>\n<td style=\"text-align: center;\">2538</td>\n<td style=\"text-align: center;\">2759</td>\n<td style=\"text-align: center;\">1472</td>\n<td style=\"text-align: center;\">1.11</td>\n<td style=\"text-align: center;\">1.02</td>\n<td style=\"text-align: center;\">1.92</td>\n<td style=\"text-align: center;\">1.72</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h3>Execute Autoencoder Training</h3>\n<h4>Execute Autoencoder Training on single GPU</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train_autoencoder.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path (it should be the path that contains <code>Task01_BrainTumour</code>) in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train_autoencoder.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training for Autoencoder</h4>\n<p>To train with multiple GPUs, use the following command, which requires scaling up the learning rate according to the number of GPUs.</p>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/multi_gpu_train_autoencoder.json']\" --lr 8e-5\n</code></pre>\n<h4>Check the Autoencoder Training result</h4>\n<p>The following code generates a reconstructed image from a random input image.\nWe can visualize it to see if the autoencoder is trained correctly.</p>\n<pre><code>python -m monai.bundle run --config_file configs/inference_autoencoder.json\n</code></pre>\n<p>An example of reconstructed image from inference is shown below. If the autoencoder is trained correctly, the reconstructed image should look similar to original image.</p>\n<p><img alt=\"Example reconstructed image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_recon_example.jpg\"/></p>\n<h3>Execute Latent Diffusion Training</h3>\n<h4>Execute Latent Diffusion Model Training on single GPU</h4>\n<p>After training the autoencoder, run the following command to train the latent diffusion model. This command will print out the scale factor of the latent feature space. If your autoencoder is well trained, this value should be close to 1.0.</p>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/train_diffusion.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training for Latent Diffusion Model</h4>\n<p>To train with multiple GPUs, use the following command, which requires scaling up the learning rate according to the number of GPUs.</p>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/train_diffusion.json','configs/multi_gpu_train_autoencoder.json','configs/multi_gpu_train_diffusion.json']\"  --lr 8e-5\n</code></pre>\n<h4>Execute inference</h4>\n<p>The following code generates a synthetic image from a random sampled noise.</p>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/brats_mri_generative_diffusion/tree/1.1.3",
        "changelog": {
            "1.1.3": "update to huggingface hosting and fix missing dependencies",
            "1.1.2": "update issue for IgniteInfo",
            "1.1.1": "enable tensorrt",
            "1.1.0": "update to use monai 1.4, model ckpt not changed, rm GenerativeAI repo",
            "1.0.9": "update to use monai 1.3.1",
            "1.0.8": "update run section",
            "1.0.7": "update with EnsureChannelFirstd",
            "1.0.6": "update with new lr scheduler api in inference",
            "1.0.5": "fix the wrong GPU index issue of multi-node",
            "1.0.4": "update with new lr scheduler api",
            "1.0.3": "update required packages",
            "1.0.2": "unify dataset dir in different configs",
            "1.0.1": "update dependency, update trained model weights",
            "1.0.0": "Initial release"
        }
    },
    "brats_mri_axial_slices_generative_diffusion": {
        "model_name": "BraTS MRI axial slices latent diffusion generation",
        "description": "A generative model for creating 2D brain MRI axial slices from Gaussian noise based on BraTS dataset",
        "authors": "MONAI team",
        "papers": [],
        "version": "1.1.3",
        "model_id": "brats_mri_axial_slices_generative_diffusion",
        "readme": "<h1>Model Overview</h1>\n<p>A pre-trained model for 2D Latent Diffusion Generative Model on axial slices of BraTS MRI.</p>\n<p>This model is trained on BraTS 2016 and 2017 data from <a href=\"http://medicaldecathlon.com/\">Medical Decathlon</a>, using the Latent diffusion model [1].</p>\n<p><img alt=\"model workflow\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm3d_network.png\"/></p>\n<p>This model is a generator for creating images like the Flair MRIs based on BraTS 2016 and 2017 data. It was trained as a 2d latent diffusion model and accepts Gaussian random noise as inputs to produce an image output. The <code>train_autoencoder.json</code> file describes the training process of the variational autoencoder with GAN loss. The <code>train_diffusion.json</code> file describes the training process of the 2D latent diffusion model.</p>\n<p>In this bundle, the autoencoder uses perceptual loss, which is based on ResNet50 with pre-trained weights (the network is frozen and will not be trained in the bundle). In default, the <code>pretrained</code> parameter is specified as <code>False</code> in <code>train_autoencoder.json</code>. To ensure correct training, changing the default settings is necessary. There are two ways to utilize pretrained weights:\n1. if set <code>pretrained</code> to <code>True</code>, ImageNet pretrained weights from <a href=\"https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#ResNet50_Weights\">torchvision</a> will be used. However, the weights are for non-commercial use only.\n2. if set <code>pretrained</code> to <code>True</code> and specifies the <code>perceptual_loss_model_weights_path</code> parameter, users are able to load weights from a local path. This is the way this bundle used to train, and the pre-trained weights are from some internal data.</p>\n<p>Please note that each user is responsible for checking the data source of the pre-trained models, the applicable licenses, and determining if suitable for the intended use.</p>\n<h4>Example synthetic image</h4>\n<p>An example result from inference is shown below:\n<img alt=\"Example synthetic image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm2d_example_generation_v2.png\"/></p>\n<p><strong>This is a demonstration network meant to just show the training process for this sort of network with MONAI. To achieve better performance, users need to use larger dataset like <a href=\"https://www.synapse.org/#!Synapse:syn25829067/wiki/610865\">BraTS 2021</a>.</strong></p>\n<h2>Data</h2>\n<p>The training data is BraTS 2016 and 2017 from the Medical Segmentation Decathalon. Users can find more details on the dataset (<code>Task01_BrainTumour</code>) at http://medicaldecathlon.com/.</p>\n<ul>\n<li>Target: Image Generation</li>\n<li>Task: Synthesis</li>\n<li>Modality: MRI</li>\n<li>Size: 388 3D MRI volumes (1 channel used)</li>\n<li>Training data size: 38800 2D MRI axial slices (1 channel used)</li>\n</ul>\n<h2>Training Configuration</h2>\n<p>If you have a GPU with less than 32G of memory, you may need to decrease the batch size when training. To do so, modify the <code>\"train_batch_size_img\"</code> and <code>\"train_batch_size_slice\"</code> parameters in the <code>configs/train_autoencoder.json</code> and <code>configs/train_diffusion.json</code> configuration files.\n- <code>\"train_batch_size_img\"</code> is number of 3D volumes loaded in each batch.\n- <code>\"train_batch_size_slice\"</code> is the number of 2D axial slices extracted from each image. The actual batch size is the product of them.</p>\n<h3>Training Configuration of Autoencoder</h3>\n<p>The autoencoder was trained using the following configuration:</p>\n<ul>\n<li>GPU: at least 32GB GPU memory</li>\n<li>Actual Model Input: 240 x 240</li>\n<li>AMP: False</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 5e-5</li>\n<li>Loss: L1 loss, perceptual loss, KL divergence loss, adversarial  loss, GAN BCE loss</li>\n</ul>\n<h4>Input</h4>\n<p>1 channel 2D MRI Flair axial patches</p>\n<h4>Output</h4>\n<ul>\n<li>1 channel 2D MRI reconstructed patches</li>\n<li>1 channel mean of latent features</li>\n<li>1 channel standard deviation of latent features</li>\n</ul>\n<h3>Training Configuration of Diffusion Model</h3>\n<p>The latent diffusion model was trained using the following configuration:</p>\n<ul>\n<li>GPU: at least 32GB GPU memory</li>\n<li>Actual Model Input: 64 x 64</li>\n<li>AMP: False</li>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 5e-5</li>\n<li>Loss: MSE loss</li>\n</ul>\n<h4>Training Input</h4>\n<ul>\n<li>1 channel noisy latent features</li>\n<li>a long int that indicates the time step</li>\n</ul>\n<h4>Training Output</h4>\n<p>1 channel predicted added noise</p>\n<h4>Inference Input</h4>\n<p>1 channel noise</p>\n<h4>Inference Output</h4>\n<p>1 channel denoised latent features</p>\n<h3>Memory Consumption Warning</h3>\n<p>If you face memory issues with data loading, you can lower the caching rate <code>cache_rate</code> in the configurations within range [0, 1] to minimize the System RAM requirements.</p>\n<h2>Performance</h2>\n<h4>Training Loss</h4>\n<p><img alt=\"A graph showing the autoencoder training curve\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm2d_train_autoencoder_loss_v3.png\"/></p>\n<p><img alt=\"A graph showing the latent diffusion training curve\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm2d_train_diffusion_loss_v3.png\"/></p>\n<h4>TensorRT speedup</h4>\n<p>This bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation (diffusion)</td>\n<td style=\"text-align: center;\">32.11</td>\n<td style=\"text-align: center;\">32.45</td>\n<td style=\"text-align: center;\">2.58</td>\n<td style=\"text-align: center;\">2.11</td>\n<td style=\"text-align: center;\">0.99</td>\n<td style=\"text-align: center;\">12.45</td>\n<td style=\"text-align: center;\">15.22</td>\n<td style=\"text-align: center;\">15.38</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">model computation (autoencoder)</td>\n<td style=\"text-align: center;\">17.74</td>\n<td style=\"text-align: center;\">18.15</td>\n<td style=\"text-align: center;\">5.47</td>\n<td style=\"text-align: center;\">3.66</td>\n<td style=\"text-align: center;\">0.98</td>\n<td style=\"text-align: center;\">3.24</td>\n<td style=\"text-align: center;\">4.85</td>\n<td style=\"text-align: center;\">4.96</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">1389</td>\n<td style=\"text-align: center;\">1973</td>\n<td style=\"text-align: center;\">332</td>\n<td style=\"text-align: center;\">314</td>\n<td style=\"text-align: center;\">0.70</td>\n<td style=\"text-align: center;\">4.18</td>\n<td style=\"text-align: center;\">4.42</td>\n<td style=\"text-align: center;\">6.28</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h3>Execute Autoencoder Training</h3>\n<h4>Execute Autoencoder Training on single GPU</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train_autoencoder.json\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path (it should be the path that contains Task01_BrainTumour) in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train_autoencoder.json --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training for Autoencoder</h4>\n<p>To train with multiple GPUs, use the following command, which requires scaling up the learning rate according to the number of GPUs.</p>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/multi_gpu_train_autoencoder.json']\" --lr 4e-4\n</code></pre>\n<h4>Check the Autoencoder Training result</h4>\n<p>The following code generates a reconstructed image from a random input image.\nWe can visualize it to see if the autoencoder is trained correctly.</p>\n<pre><code>python -m monai.bundle run --config_file configs/inference_autoencoder.json\n</code></pre>\n<p>An example of reconstructed image from inference is shown below. If the autoencoder is trained correctly, the reconstructed image should look similar to original image.</p>\n<p><img alt=\"Example reconstructed image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_brain_image_gen_ldm2d_recon_example.png\"/></p>\n<h3>Execute Latent Diffusion Model Training</h3>\n<h4>Execute Latent Diffusion Model Training on single GPU</h4>\n<p>After training the autoencoder, run the following command to train the latent diffusion model. This command will print out the scale factor of the latent feature space. If your autoencoder is well trained, this value should be close to 1.0.</p>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/train_diffusion.json']\"\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training for Latent Diffusion Model</h4>\n<p>To train with multiple GPUs, use the following command, which requires scaling up the learning rate according to the number of GPUs.</p>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file \"['configs/train_autoencoder.json','configs/train_diffusion.json','configs/multi_gpu_train_autoencoder.json','configs/multi_gpu_train_diffusion.json']\"  --lr 4e-4\n</code></pre>\n<h3>Execute inference</h3>\n<p>The following code generates a synthetic image from a random sampled noise.</p>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/brats_mri_axial_slices_generative_diffusion/tree/1.1.3",
        "changelog": {
            "1.1.3": "update to huggingface hosting and fix missing dependencies",
            "1.1.2": "update issue for IgniteInfo",
            "1.1.1": "enable tensorrt",
            "1.1.0": "update to use monai 1.4, model ckpt not changed, rm GenerativeAI repo",
            "1.0.9": "update to use monai 1.3.1",
            "1.0.8": "define arg for output file and put infer logic into a function",
            "1.0.7": "update AddChanneld with EnsureChannelFirstd",
            "1.0.6": "update with new lr scheduler api in inference",
            "1.0.5": "fix the wrong GPU index issue of multi-node",
            "1.0.4": "update with new lr scheduler api",
            "1.0.3": "update required packages",
            "1.0.2": "remove unused saver in inference",
            "1.0.1": "fix inference folder error",
            "1.0.0": "Initial release"
        }
    },
    "renalStructures_CECT_segmentation": {
        "model_name": "Segmentation of renal structures based on contrast computed tomography scans",
        "description": "A UNET-based model for renal segmentation from contrast enhanced CT image",
        "authors": "Sechenov university",
        "papers": [
            "Chernenkiy I. M. et al. Segmentation of renal structures based on contrast computed tomography scans using a convolutional neural network //Sechenov Medical Journal.  2023.  . 14.  . 1.  . 39-49. URL - https://www.sechenovmedj.com/jour/article/view/899"
        ],
        "version": "0.2.2",
        "model_id": "renalStructures_CECT_segmentation",
        "readme": "<h1>Model Title</h1>\n<p>Renal structures CECT segmentation</p>\n<h3><strong>Authors</strong></h3>\n<p>Ivan Chernenkiy,   Michael Chernenkiy,   Dmitry Fiev,   Evgeny Sirota, Center for Neural Network Technologies / Institute of Urology and Human Reproductive Systems / Sechenov First Moscow State Medical University</p>\n<h3><strong>Tags</strong></h3>\n<p>Segmentation, CT, CECT, Kidney, Renal, Supervised</p>\n<h2><strong>Model Description</strong></h2>\n<p>The model is the SegResNet architecture[1] for volumetric (3D) renal structures segmentation. Input is artery, vein, excretory phases after mutual registration and concatenated to 3 channel 3D tensor.</p>\n<h2><strong>Data</strong></h2>\n<p>DICOM data from 41 patients with kidney neoplasms were used [2]. The images and segmentation data are available under a CC BY-NC-SA 4.0 license. Data included all phases of contrast-enhanced multispiral computed tomography. We split the data: 32 observations for the training set and 9  for the validation set. At the labeling stage, the arterial, venous, and excretory phases were taken, affine registration was performed to jointly match the location of the kidneys, and noise was removed using a median filter and a non-local means filter. Validation set ip published to Yandex.Disk. You can download via <a href=\"https://disk.yandex.ru/d/pWEKt6D3qi3-aw\">link</a> or use following command:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run download_data --meta_file configs/metadata.json --config_file \"['configs/train.json', 'configs/evaluate.json']\"\n</code></pre>\n<p><strong>NB</strong>: underlying data is in LPS orientation. IF! you want to test model on your own data, reorient it from RAS to LPS with <code>Orientation</code> transform. You can see example of preprocessing pipeline in <code>inference.json</code> file of this bundle.</p>\n<h4><strong>Preprocessing</strong></h4>\n<p>Images are (1) croped to kidney region, all (artery,vein,excret) phases are (2) <a href=\"https://simpleitk.readthedocs.io/en/master/registrationOverview.html#lbl-registration-overview\">registered</a> with affine transform, noise removed with (3) median and (4) non-local means filter. After that, images are (5) resampled to (0.8,0.8,0.8) density and intesities are (6) scaled from [-1000,1000] to [0,1] range.</p>\n<h2><strong>Performance</strong></h2>\n<p>On the validation subset, the values of the Dice score of the SegResNet architecture were: 0.89 for the normal parenchyma of the kidney, 0.58 for the kidney neoplasms, 0.86 for arteries, 0.80 for veins, 0.80 for ureters.</p>\n<p>When compared with the nnU-Net model, which was trained on KiTS 21 dataset, the Dice score was greater for the kidney parenchyma in SegResNet  0.89 compared to three model variants: lowres  0.69, fullres  0.70, cascade  0.69. At the same time, for the neoplasms of the parenchyma of the kidney, the Dice score was comparable: for SegResNet  0.58, for nnU-Net fullres  0.59; lowres and cascade had lower Dice score of 0.37 and 0.45, respectively. To reproduce, visit - https://github.com/blacky-i/nephro-segmentation</p>\n<h2><strong>Additional Usage Steps</strong></h2>\n<h4>Execute training:</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json\n</code></pre>\n<p>Expected result: finished, Training process started</p>\n<h4>Execute training with finetuning</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --dont_finetune false --meta_file configs/metadata.json --config_file configs/train.json\n</code></pre>\n<p>Expected result: finished, Training process started, model variables are restored</p>\n<h4>Execute validation:</h4>\n<p>Download validation data (described in <a href=\"#data\">Data</a> section).</p>\n<p>With provided model weights mean dice score is expected to be ~0.78446.</p>\n<h5>Run validation script:</h5>\n<pre><code class=\"language-bash\">python -m monai.bundle run evaluate --meta_file configs/metadata.json --config_file \"['configs/train.json', 'configs/evaluate.json']\"\n</code></pre>\n<p>Expected result: finished, <code>Key metric: val_mean_dice best value: ...</code> is printed.</p>\n<h2><strong>System Configuration</strong></h2>\n<p>The model was trained for 10000 epochs on 2 RTX2080Ti GPUs with <a href=\"https://docs.monai.io/en/stable/data.html#smartcachedataset\">SmartCacheDataset</a>. This takes 1 days and 2 hours, with 4 images per GPU.\nTraining progress is available on <a href=\"https://tensorboard.dev/experiment/VlEMjLdURH6SyFp216dFBg\">tensorboard.dev</a></p>\n<p>To perform training in minimal settings, at least one 12GB-memory GPU is required.\nActual Model Input: 96 x 96 x 96</p>\n<h2><strong>Limitations</strong></h2>\n<p>For developmental purposes only and cannot be used directly for clinical procedures.</p>\n<h2><strong>Citation Info</strong></h2>\n<pre><code>@article{chernenkiy2023segmentation,\n  title={Segmentation of renal structures based on contrast computed tomography scans using a convolutional neural network},\n  author={Chernenkiy, I and Chernenkiy, MM and Fiev, DN and Sirota, ES},\n  journal={Sechenov Medical Journal},\n  volume={14},\n  number={1},\n  pages={39--49},\n  year={2023}\n}\n</code></pre>\n<h2><strong>References</strong></h2>\n<p>[1] Myronenko, A. (2019). 3D MRI Brain Tumor Segmentation Using Autoencoder Regularization. In: Crimi, A., Bakas, S., Kuijf, H., Keyvan, F., Reyes, M., van Walsum, T. (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2018. Lecture Notes in Computer Science(), vol 11384. Springer, Cham. https://doi.org/10.1007/978-3-030-11726-9_28</p>\n<p>[2] Chernenkiy, I. ., et al. \"Segmentation of renal structures based on contrast computed tomography scans using a convolutional neural network.\" Sechenov Medical Journal 14.1 (2023): 39-49.https://doi.org/10.47093/2218-7332.2023.14.1.39-49</p>\n<h4><strong>Tests used for bundle checking</strong></h4>\n<p>Checking with ci script file</p>\n<pre><code class=\"language-bash\">python ci/verify_bundle.py -b renalStructures_CECT_segmentation -p models\n</code></pre>\n<p>Expected result: passed, model.pt file downloaded</p>\n<p>Checking downloading validation data file</p>\n<pre><code class=\"language-bash\">cd models/renalStructures_CECT_segmentation\npython -m monai.bundle run download_data --meta_file configs/metadata.json --config_file \"['configs/train.json', 'configs/evaluate.json']\"\n</code></pre>\n<p>Expected result: finished, <code>data/</code> folder is created and filled with images.</p>\n<p>Checking evaluation script</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run evaluate --meta_file configs/metadata.json --config_file \"['configs/train.json', 'configs/evaluate.json']\"\n</code></pre>\n<p>Expected result: finished, <code>Key metric: val_mean_dice best value: ...</code> is printed.</p>\n<p>Checking train script</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json\n</code></pre>\n<p>Expected result: finished, Training process started</p>\n<p>Checking train script with finetuning</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run training --dont_finetune false --meta_file configs/metadata.json --config_file configs/train.json\n</code></pre>\n<p>Expected result: finished, Training process started, model variables are restored</p>\n<p>Checking inference script</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run inference --meta_file configs/metadata.json --config_file configs/inference.json\n</code></pre>\n<p>Expected result: finished, in <code>eval</code> folder masks are created</p>\n<p>Check unit test with script:</p>\n<pre><code class=\"language-bash\">python ci/unit_tests/runner.py --b renalStructures_CECT_segmentation\n</code></pre>",
        "download_url": "https://huggingface.co/MONAI/renalStructures_CECT_segmentation/tree/0.2.2",
        "changelog": {
            "0.2.2": "update to huggingface hosting",
            "0.2.1": "fix pytype error",
            "0.2.0": "set image_only to False",
            "0.1.0": "complete the model package"
        }
    },
    "multi_organ_segmentation": {
        "model_name": "Abdominal multi-organ segmentation",
        "description": "DiNTS architectures for volumetric (3D) segmentation of the abdominal from CT image",
        "authors": "Chen Shen, Holger R. Roth, Kazunari Misawa, Kensaku Mori",
        "papers": [
            "He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).",
            "Roth, H., Shen C, Oda H., Sugino T., Oda M., Hayashi Y., Misawa K., Mori K., 2018. A multi-scale pyramid of 3D fully convolutional networks for abdominal multi-organ segmentation. International conference on medical image computing and computer-assisted intervention",
            "Shen, C., Roth, H. R., Nath, V., Hayashi, Y., Oda, M., Misawa, K., Mori, K., 2022. Effective hyperparameter optimization with proxy data for multi-organ segmentation. In Medical Imaging 2022: Image Processing (Vol. 12032, pp. 200-206)"
        ],
        "version": "0.0.5",
        "model_id": "multi_organ_segmentation",
        "readme": "<h1>Multi-organ segmentation in abdominal CT</h1>\n<h3><strong>Authors</strong></h3>\n<p>Chen Shen<sup>1</sup>, Holger R. Roth<sup>2</sup>, Kazunari Misawa<sup>3</sup>, Kensaku Mori<sup>1</sup></p>\n<ol>\n<li>\n<p>Nagoya University, Japan</p>\n</li>\n<li>\n<p>NVIDIA Corporation, USA</p>\n</li>\n<li>\n<p>Aichi Cancer Center, Japan</p>\n</li>\n</ol>\n<h3><strong>Tags</strong></h3>\n<p>Segmentation, Multi-organ, Abdominal</p>\n<h2><strong>Model Description</strong></h2>\n<p>This model uses the DiNTS model architecture searched on <a href=\"http://medicaldecathlon.com/\">Medical Segmentation Decathlon</a> Pancreas [1] and re-trained for multi-organ segmentation from abdominal CT images [2,3].</p>\n<h2><strong>Data</strong></h2>\n<p>This model was trained on an abdominal CT dataset in portal venous phase collected from Aichi Cancer Center in Japan. Since this is a private dataset, similar models can be trained using other public multi-organ datasets like <a href=\"https://www.synapse.org/#!Synapse:syn3193805/wiki/89480\">BTCV</a>.</p>\n<p>For this bundle, we split the 420 cases into training, validation and testing with 300, 60 and 60 cases, respectively.</p>\n<h2><strong>Output</strong></h2>\n<p>8 channels</p>\n<ul>\n<li>0: Background</li>\n<li>1: Artery</li>\n<li>2: Portal vein</li>\n<li>3: Liver</li>\n<li>4: Spleen</li>\n<li>5: Stomach</li>\n<li>6: Gallbladder</li>\n<li>7: Pancreas</li>\n</ul>\n<p>Here is an example of output.</p>\n<p><img alt=\"alt\" src=\"output_example.png\"/></p>\n<h2><strong>Scores</strong></h2>\n<p>This model achieves the following Dice score on the validation data (our own split from the whole dataset):</p>\n<p>Mean Dice = 88.6%</p>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute model searching:</h4>\n<pre><code>python -m scripts.search run --config_file configs/search.yaml\n</code></pre>\n<h4>Execute multi-GPU model searching (recommended):</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m scripts.search run --config_file configs/search.yaml\n</code></pre>\n<h4>Execute training:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml\n</code></pre>\n<h4>Override the <code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 \\\n     -m scripts.search run \\\n     --config_file configs/search.yaml\n</code></pre>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run \\\n    --config_file \"['configs/train.yaml','configs/evaluate.yaml']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.yaml\n</code></pre>\n<h4>Export checkpoint for TorchScript:</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.yaml\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.yaml', 'configs/inference_trt.yaml']\"\n</code></pre>\n<h2><strong>References</strong></h2>\n<p>[1] He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).</p>\n<p>\n[2] Roth, Holger R., et al. \"A multi-scale pyramid of 3D fully convolutional networks for abdominal multi-organ segmentation.\" International conference on medical image computing and computer-assisted intervention. Springer, Cham, 2018.\n</p>\n<p>[3] Shen, Chen, et al. \"Effective hyperparameter optimization with proxy data for multi-organ segmentation.\" Medical Imaging 2022: Image Processing. Vol. 12032. SPIE, 2022.</p>\n<h2><strong>License</strong></h2>\n<p>The Licensee is not allowed to distribute or make the model to any third party, either for free or for a fee. Reverse engineering of the model is not allowed. This includes, but is not limited to, providing the model as part of a commercial offering, sharing the model on a public or private network, or making the model available for download on the Internet.</p>",
        "download_url": "https://huggingface.co/MONAI/multi_organ_segmentation/tree/0.0.5",
        "changelog": {
            "0.0.5": "update to huggingface hosting",
            "0.0.4": "Set image_only to False",
            "0.0.3": "Update for stable MONAI version",
            "0.0.2": "Retrain with new MONAI",
            "0.0.1": "initialize the model package structure"
        }
    },
    "segmentation_template": {
        "model_name": "Segmentation Template",
        "description": "This is a template bundle for segmenting in 3D, take this as a basis for your own bundles.",
        "authors": "Eric Kerfoot",
        "papers": [],
        "version": "0.0.3",
        "model_id": "segmentation_template",
        "readme": "<h1>Template Segmentation Bundle</h1>\n<p>This bundle is meant to be an example of segmentation in 3D which you can copy and modify to create your own bundle.\nIt is only roughly trained for the synthetic data you can generate with <a href=\"./generate_data.ipynb\">this notebook</a>\nso doesn't do anything useful on its own. The purpose is to demonstrate the base line for segmentation network\nbundles compatible with MONAILabel amongst other things.</p>\n<p>To use this bundle, copy the contents of the whole directory and change the definitions for network, data, transforms,\nor whatever else you want for your own new segmentation bundle. Some of the names are critical for MONAILable but\notherwise you're free to change just about whatever else is defined here to suit your network.</p>\n<p>This bundle should also demonstrate good practice and design, however there is one caveat about definitions being\ncopied between config files. Ideally there should be a <code>common.yaml</code> file for all the definitions used by every other\nconfig file which is then included with that file. MONAILabel doesn't support this yet so this bundle will be updated\nonce it does to exemplify this better practice.</p>\n<h2>Generating Demo Data</h2>\n<p>Run all the cells of <a href=\"./generate_data.ipynb\">this notebook</a> to generate training and test data. These will be 3D\nnifti files containing volumes with randomly generated spheres of varying intensities and some noise for fun. The\nsegmentation task is very easy so your network will train in minutes with the default configuration of values. A test\ndata directory will separately be created since the test and inference configs are configured to apply the network to\nevery nifti file in a given directory with a certain pattern.</p>\n<h2>Training</h2>\n<p>To train a new network the <code>train.yaml</code> script can be used alone with no other arguments (assume <code>BUNDLE</code> is the root\ndirectory of the bundle):</p>\n<pre><code class=\"language-sh\">python -m monai.bundle run \\\n    --meta_file \"$BUNDLE/configs/metadata.json\" \\\n    --config_file \"$BUNDLE/configs/train.yaml\" \\\n    --bundle_root \"$BUNDLE\"\n</code></pre>\n<p>A <code>train.sh</code> script is also provided in <code>docs</code> which implements this invocation with some helper commands. It\nrelies on a Conda environment called <code>monai</code> so comment or modify those lines if you're not using such an environment.\nSee MONAI installation information about what environment to create for the features you want.</p>\n<p>The training config includes a number of hyperparameters like <code>learning_rate</code> and <code>num_workers</code>. These control aspects\nof how training operates in terms of how many processes to use, when to perform validation, when to save checkpoints,\nand other things. Other aspects of the script can be modified on the command line so these aren't exhaustive but are a\nguide to the kind of parameterisation that make sense for a bundle.</p>\n<h2>Testing and Inference</h2>\n<p>Two configs are provided (<code>test.yaml</code> and <code>inference.yaml</code>) for doing post-training inference with the model. The first\nrequires image and segmentation pairs which are used with network outputs to assess performance using metrics. This is\nvery similar to training validation but is done on separate images. This config can be set to save predicted segmentations\nby setting <code>save_pred</code> to true but by default it will just run metrics and print their results.</p>\n<p>The inference config is for generating new segmentations from images which don't have ground truths, so this is used for\nactually applying the network in practice. This will apply the network to every image in an input directory matching a\npattern and save the predicted segmentations to an output directory.</p>\n<p>Using inference on the command line is demonstrated in <a href=\"./visualise_inference.ipynb\">this notebook</a> with visualisation.\nSome explanation of some command line choices are given in the notebook as well, similar command line invocations can\nalso be done with the included <code>inference.sh</code> script file.</p>\n<h2>Other Considerations</h2>\n<p>There is no <code>scripts</code> directory containing a valid Python module to be imported in your configs. This wasn't necessary\nfor this bundle but if you want to include custom code in a bundle please follow the bundle tutorials on how to do this.</p>\n<p>The <code>multi_gpu_train.yaml</code> config is defined as a \"mixin\" to implement DDP based multi-gpu training. The script\n<code>train_multigpu.sh</code> illustrates an example of how to invoke these configs together with <code>torchrun</code>.</p>\n<p>The <code>inference.yaml</code> config is compatible with MONAILabel such that you can load one of the synthetic images and perform\ninference through a label server. This doesn't permit active learning however, that is a later enhancement for this\nbundle. If you're changing definitions in the <code>inference.yaml</code> config file be careful about changing names and consult\nthe MONAILabel documentation about required definition names. An example script to start a server is given in\n<code>run_monailabel.sh</code> which will download the bundle application and \"install\" this bundle using a symlink then start\nthe server. Future updates to MONAILabel will improve this process.</p>",
        "download_url": "https://huggingface.co/MONAI/segmentation_template/tree/0.0.3",
        "changelog": {
            "0.0.3": "update to huggingface hosting",
            "0.0.2": "Minor train.yaml clarifications",
            "0.0.1": "Initial version"
        }
    },
    "classification_template": {
        "model_name": "Classification Template",
        "description": "This is a template bundle for classifying in 2D, take this as a basis for your own bundles.",
        "authors": "Yun Liu",
        "papers": [],
        "version": "0.0.3",
        "model_id": "classification_template",
        "readme": "<h1>Template Classification Bundle</h1>\n<p>This bundle is meant to be an example of classification in 2D which you can copy and modify to create your own bundle.\nIt is only roughly trained for the synthetic data you can generate with <a href=\"./generate_data.ipynb\">this notebook</a>\nso doesn't do anything useful on its own. The purpose is to demonstrate the base line for classification network bundles.</p>\n<p>To use this bundle, copy the contents of the whole directory and change the definitions for network, data, transforms,\nor whatever else you want for your own new classification bundle.</p>\n<h2>Generating Demo Data</h2>\n<p>Run all the cells of <a href=\"./generate_data.ipynb\">this notebook</a> to generate training and test data. These will be 2D\nnifti files containing volumes with randomly generated circle, triangle or rectangle. The classification task\nis very easy so your network will train in minutes with the default configuration of values. A test\ndata directory will separately be created since the inference config is configured to apply the network to\nevery nifti file in a given directory with a certain pattern.</p>\n<h2>Training</h2>\n<p>To train a new network the <code>train.yaml</code> script can be used alone with no other arguments (assume <code>BUNDLE</code> is the root\ndirectory of the bundle):</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml\n</code></pre>\n<p>The training config includes a number of hyperparameters like <code>learning_rate</code> and <code>num_workers</code>. These control aspects\nof how training operates in terms of how many processes to use, when to perform validation, when to save checkpoints,\nand other things. Other aspects of the script can be modified on the command line so these aren't exhaustive but are a\nguide to the kind of parameterisation that make sense for a bundle.</p>\n<h2>Override the <code>train</code> config to execute multi-GPU training:</h2>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.yaml','configs/multi_gpu_train.yaml']\"\n</code></pre>\n<p>Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove <code>--standalone</code>, modify <code>--nnodes</code>, or do some other necessary changes according to the machine used. For more details, please refer to <a href=\"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\">pytorch's official tutorial</a>.</p>\n<h2>Override the <code>train</code> config to execute evaluation with the trained model:</h2>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.yaml','configs/evaluate.yaml']\"\n</code></pre>\n<h2>Execute inference:</h2>\n<pre><code>python -m monai.bundle run --config_file configs/inference.yaml\n</code></pre>\n<h2>Other Considerations</h2>\n<p>There is no <code>scripts</code> directory containing a valid Python module to be imported in your configs. This wasn't necessary\nfor this bundle but if you want to include custom code in a bundle please follow the bundle tutorials on how to do this.</p>",
        "download_url": "https://huggingface.co/MONAI/classification_template/tree/0.0.3",
        "changelog": {
            "0.0.3": "update to huggingface hosting",
            "0.0.2": "update large file yml",
            "0.0.1": "Initial version"
        }
    },
    "vista3d": {
        "model_name": "VISTA3D",
        "description": "VISTA3D bundle",
        "authors": "MONAI team",
        "papers": [],
        "version": "0.5.8",
        "model_id": "vista3d",
        "readme": "<h1>Model Overview</h1>\n<p>Vista3D model fintuning/evaluation/inference pipeline. VISTA3D is trained using over 20 partial datasets with more complicated pipeline. To avoid confusion, we will only provide finetuning/continual learning APIs for users to finetune on their\nown datasets.</p>\n<h2>Continual learning</h2>\n<p>For continual learning, user can change <code>configs/train_continual.json</code>. More advanced users can change configurations in <code>configs/train.json</code>. The hyperparameters in <code>configs/train_continual.json</code> will overwrite ones in <code>configs/train.json</code>. Most hyperparameters are straighforward and user can tell based on their names. We list hyperparameters that needs to be modified.</p>\n<h3>Data</h3>\n<p>The spleen Task from the Medical Segmentation Decathalon is selected as an example to show how to continuous learning. Users can find more details on the datasets at http://medicaldecathlon.com/.</p>\n<p>To train with other datasets, users need to provide a json data split for training and continuous learning (<code>configs/msd_task09_spleen_folds.json</code> is an example for reference). The data split should meet the following format ('testing' labels are optional):</p>\n<pre><code class=\"language-json\">{\n    \"training\": [\n        {\"image\": \"img0001.nii.gz\", \"label\": \"label0001.nii.gz\", \"fold\": 0},\n        {\"image\": \"img0002.nii.gz\", \"label\": \"label0002.nii.gz\", \"fold\": 2},\n        ...\n     ],\n    \"testing\": [\n        {\"image\": \"img0003.nii.gz\", \"label\": \"label0003.nii.gz\"},\n        {\"image\": \"img0004.nii.gz\", \"label\": \"label0004.nii.gz\"},\n        ...\n   ]\n}\n</code></pre>\n<pre><code>Note the data is not the absolute path to the image and label file. The actual image file will be `os.path.join(dataset_dir, data[\"training\"][item][\"image\"])`, where `dataset_dir` is defined in `configs/train_continual.json`. Also 5-fold cross-validation is not required! `fold=0` is defined in train.json, which means any data item with fold==0 will be used as validation and other fold will be used for training. So if you only have 2 data, you can manually set one data to be validation by setting \"fold\": 0 in its datalist and the other to be training by setting \"fold\" to any number other than 0.\n</code></pre>\n<h3>Best practice to generate data list</h3>\n<p>User can use monai to generate the 5-fold data lists. Full exampls can be found in VISTA3D open source <a href=\"https://github.com/Project-MONAI/VISTA/blob/main/vista3d/data/make_datalists.py\">codebase</a></p>\n<pre><code class=\"language-python\">from monai.data.utils import partition_dataset\nfrom monai.bundle import ConfigParser\nbase_url = \"/path_to_your_folder/\"\njson_name = \"./your_5_folds.json\"\n# create matching image and label lists.\n# The code to generate the lists is based on your local data structure.\n# You can use glob.glob(\"**.nii.gz\") e.t.c.\nimage_list = ['images/1.nii.gz', 'images/2.nii.gz', ...]\nlabel_list = ['labels/1.nii.gz', 'labels/2.nii.gz', ...]\nitems = [{\"image\": img, \"label\": lab} for img, lab in zip(image_list, label_list)]\n# 80% for training 20% for testing.\ntrain_test = partition_dataset(items, ratios=[0.8, 0.2], shuffle=True, seed=0)\nprint(f\"training: {len(train_test[0])}, testing: {len(train_test[1])}\")\n# num_partitions-fold split for the training set.\ntrain_val = partition_dataset(train_test[0], num_partitions=5, shuffle=True, seed=0)\nprint(f\"training validation folds sizes: {[len(x) for x in train_val]}\")\n# add the fold index to each training data.\ntraining = []\nfor f, x in enumerate(train_val):\n   for item in x:\n      item[\"fold\"] = f\n      training.append(item)\n# save json file\nparser = ConfigParser({})\nparser[\"training\"] = training\nparser[\"testing\"] = train_test[1]\nprint(f\"writing {json_name}\\n\\n\")\nif os.path.exists(json_name):\n   logger.warning(f\"rewrite existing datalist file: {json_name}\")\nConfigParser.export_config_file(parser.config, json_name, indent=4)\n</code></pre>\n<h3>Configurations</h3>\n<h4><code>label_mappings</code></h4>\n<p>The core concept of label_mapping is to convert ground-truth label index of each dataset to a unified class index. For example, \"Spleen\" in MSD09 groundtruth will be represented by 1, while in AbdomenCT-1K it's 3. We unified a global label index (<code>docs/labels.json</code>) to represent all 132 classes, and create a label mapping to map those local index to this global index. So when a user is training on their own dataset, we need to know this mapping.</p>\n<p>The current label mapping <code>[[1, 3]]</code> indicates that training labels' class indices <code>1</code> is mapped\nto the VISTA model's class <code>3</code> (format <code>[[src_class_0, dst_class_0], [src_class_1, dst_class_1], ...]</code>). So during inference, \"3\" is used to segment spleen.</p>\n<p>Since it's finetuning, you can map your local class to any global class. If you use [[1,4]], where \"4\" represents pancreas, the finetuning can still work but requires more training data and epoch because the class \"4\" is already assigned and trained with pancreas. If you use [[1,3]], where \"3\" already represents spleen, the finetuning will converge much faster.</p>\n<h4>Best practice to set label_mapping</h4>\n<p>For a class that represent the same or similar class as the global index, directly map it to the global index. For example, \"mouse left lung\" (e.g. index 2 in the mouse dataset) can be mapped to the 28 \"left lung upper lobe\"(or 29 \"left lung lower lobe\") with [[2,28]]. After finetuning, 28 now represents \"mouse left lung\" and will be used for segmentation. If you want to segment 4 substructures of aorta, you can map one of the substructuress to 6 aorta and the rest to any unused classes (class &gt; 132), [[1,6],[2,133],[3,134],[4,135]]. For a completely novel class that none of the VISTA global classes are related, directly map to unused classes (class &gt; 132).</p>\n<pre><code>NOTE: Do not map to global index value &gt;= 255. `num_classes=255` in the config only represent the maximum mapping index, while the actual output class number only depends on your label_mapping definition. The 255 value in the inference output is also used to represent 'NaN' value.\n</code></pre>\n<h4><code>n_train_samples</code> and <code>n_val_samples</code></h4>\n<p>In <code>train_continual.json</code>, only <code>n_train_samples</code> and <code>n_val_samples</code> are used for training and validation. Remember to change these two values.</p>\n<h4><code>patch_size</code></h4>\n<p>The patch size parameter is defined in <code>configs/train_continual.json</code>: <code>\"patch_size\": [128, 128, 128]</code>. For finetuning purposes, this value needs to be changed acccording to user's task and GPU memory. Usually a larger patch_size will give better final results.</p>\n<h4><code>resample_to_spacing</code></h4>\n<p>The resample_to_spacing parameter is defined in <code>configs/train_continual.json</code> and it represents the resolution the model will be trained on. The <code>1.5,1.5,1.5</code> mm default is suitable for large CT organs, but for other tasks, this value should be changed to achive the optimal performance.</p>\n<h4>Advanced user: <code>drop_label_prob</code> and <code>drop_point_prob</code> (in train.json)</h4>\n<p>VISTA3D is trained to perform both automatic (class prompts) and interactive point segmentation.\n<code>drop_label_prob</code> and <code>drop_point_prob</code> means percentage to remove class prompts and point prompts during training respectively. If <code>drop_point_prob=1</code>, the\nmodel is only finetuning for automatic segmentation, while <code>drop_label_prob=1</code> means only finetuning for interactive segmentation. The VISTA3D foundation\nmodel is trained with interactive only (drop_label_prob=1) and then froze the point branch and trained with fully automatic segmentation (<code>drop_point_prob=1</code>).\nIn this bundle, the training is simplified by jointly training with class prompts and point prompts and both of the drop ratio is set to 0.25.</p>\n<pre><code>NOTE: If user doesn't use interactive segmentation, set `drop_point_prob=1` and `drop_label_prob=0` in train.json might provide a faster and easier finetuning process.\n</code></pre>\n<h4>Other explanatory items</h4>\n<p>In <code>train.json</code>, <code>validate[evaluator][val_head]</code> can be <code>auto</code> and <code>point</code>. If <code>auto</code>, the validation results will be automatic segmentation. If <code>point</code>,\nthe validation results will be sampling one positive point per object per patch. The validation scheme of combining auto and point is deprecated due to\nspeed issue.</p>\n<p>In <code>train_continual.json</code>, <code>valid_remap</code> is a transform that maps the groundtruth label indexes, e.g. [0,2,3,5,6] to sequential and continuous labels [0,1,2,3,4]. This is\nrequired by monai dice calculation. It is not related to mapping label index to VISTA3D defined global class index. The validation data is not mapped\nto the VISTA3D global class index.</p>\n<p><code>label_set</code> is used to identify the VISTA model classes for providing training prompts.\n<code>val_label_set</code> is used to identify the original training label classes for computing foreground/background mask during validation.\nThe default configs for both variables are derived from the <code>label_mappings</code> config and include <code>[0]</code>:</p>\n<pre><code>\"label_set\": \"$[0] + list(x[1] for x in @label_mappings#default)\"\n\"val_label_set\": \"$[0] + list(x[0] for x in @label_mappings#default)\"\n</code></pre>\n<p>Note: Please ensure the input data header is correct. The output file will use the same header as the input data, but if the input data is missing header information, MONAI will automatically provide some default values for missing values (e.g. <code>np.eye(4)</code> will be used if affine information is absent). This may cause a visualization misalignment depending on the visualization tool.</p>\n<h3>Commands</h3>\n<p>Single-GPU:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run \\\n    --config_file=\"['configs/train.json','configs/train_continual.json']\" --epochs=320 --learning_rate=0.00005\n</code></pre>\n<p>Multi-GPU:</p>\n<pre><code class=\"language-bash\">torchrun --nnodes=1 --nproc_per_node=8 -m monai.bundle run \\\n    --config_file=\"['configs/train.json','configs/train_continual.json','configs/multi_gpu_train.json']\" --epochs=320 --learning_rate=0.00005\n</code></pre>\n<h3>MLFlow support</h3>\n<p>MLflow can be enabled to track and manage your machine learning experiments. To enable MLflow, set the <code>use_mlflow</code> parameter to <code>True</code>. Below is an example of how to run a single-GPU training command with MLflow enabled:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run \\\n    --config_file=\"['configs/train.json','configs/train_continual.json']\" --epochs=320 --learning_rate=0.00005 --use_mlflow True\n</code></pre>\n<p>By default, the data of MLflow is stored in the <code>mlruns/</code> folder under the bundle's root directory. To launch the MLflow UI and track your experiment data, follow these steps:</p>\n<ol>\n<li>\n<p>Open a terminal and navigate to the root directory of your bundle where the <code>mlruns/</code> folder is located.</p>\n</li>\n<li>\n<p>Execute the following command to start the MLflow server. This will make the MLflow UI accessible.</p>\n</li>\n</ol>\n<pre><code class=\"language-Bash\">mlflow ui\n</code></pre>\n<h2>Evaluation</h2>\n<p>Evaluation can be used to calculate dice scores for the model or a finetuned model. Change the <code>ckpt_path</code> to the checkpoint you wish to evaluate. The dice score is calculated on the original image spacing using <code>invertd</code>, while the dice score during finetuning is calculated on resampled space.</p>\n<pre><code>NOTE: Evaluation does not support point evaluation.`\"validate#evaluator#hyper_kwargs#val_head` is always set to `auto`.\n</code></pre>\n<p>Single-GPU:</p>\n<pre><code>python -m monai.bundle run \\\n    --config_file=\"['configs/train.json','configs/train_continual.json','configs/evaluate.json']\"\n</code></pre>\n<p>Multi-GPU:</p>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m monai.bundle run \\\n    --config_file=\"['configs/train.json','configs/train_continual.json','configs/evaluate.json','configs/mgpu_evaluate.json']\"\n</code></pre>\n<h4>Other explanatory items</h4>\n<p>The <code>label_mapping</code> in <code>evaluation.json</code> does not include <code>0</code> because the postprocessing step performs argmax (<code>VistaPostTransformd</code>), and a <code>0</code> prediction would negatively impact performance. In continuous learning, however, <code>0</code> is included for validation because no argmax is performed, and validation is done channel-wise (include_background=False). Additionally, <code>Relabeld</code> in <code>postprocessing</code> is required to map <code>label</code> and <code>pred</code> back to sequential indexes like <code>0, 1, 2, 3, 4</code> for dice calculation, as they are not in one-hot format. Evaluation does not support <code>point</code>, but finetuning does, as it does not perform argmax.</p>\n<h2>Inference:</h2>\n<p>For inference, VISTA3d bundle requires at least one prompt for segmentation. It supports label prompt, which is the index of the class for automatic segmentation.\nIt also supports point click prompts for binary interactive segmentation. User can provide both prompts at the same time.</p>\n<p>All the configurations for inference is stored in inference.json, change those parameters:</p>\n<h3><code>input_dict</code></h3>\n<p><code>input_dict</code> defines the image to segment and the prompt for segmentation.</p>\n<pre><code>\"input_dict\": \"$[{'image': '/data/Task09_Spleen/imagesTs/spleen_15.nii.gz', 'label_prompt':[1]}]\",\n\"input_dict\": \"$[{'image': '/data/Task09_Spleen/imagesTs/spleen_15.nii.gz', 'points':[[138,245,18], [271,343,27]], 'point_labels':[1,0]}]\"\n</code></pre>\n<ul>\n<li>The input_dict must include the key <code>image</code> which contain the absolute path to the nii image file, and includes prompt keys of <code>label_prompt</code>, <code>points</code> and <code>point_labels</code>.</li>\n<li>The <code>label_prompt</code> is a list of length <code>B</code>, which can perform <code>B</code> foreground objects segmentation, e.g. <code>[2,3,4,5]</code>. If <code>B&gt;1</code>, Point prompts must NOT be provided.</li>\n<li>The <code>points</code> is of shape <code>[N, 3]</code> like <code>[[x1,y1,z1],[x2,y2,z2],...[xN,yN,zN]]</code>, representing <code>N</code> point coordinates <strong>IN THE ORIGINAL IMAGE SPACE</strong> of a single foreground object. <code>point_labels</code> is a list of length [N] like [1,1,0,-1,...], which\nmatches the <code>points</code>. 0 means background, 1 means foreground, -1 means ignoring this point. <code>points</code> and <code>point_labels</code> must pe provided together and match length.</li>\n<li><strong>B must be 1 if label_prompt and points are provided together</strong>. The inferer only supports SINGLE OBJECT point click segmentatation.</li>\n<li>If no prompt is provided, the model will use <code>everything_labels</code> to segment 117 classes:</li>\n</ul>\n<pre><code class=\"language-Python\">list(set([i+1 for i in range(132)]) - set([2,16,18,20,21,23,24,25,26,27,128,129,130,131,132]))\n</code></pre>\n<ul>\n<li>The <code>points</code> together with <code>label_prompts</code> for \"Kidney\", \"Lung\", \"Bone\" (class index [2, 20, 21]) are not allowed since those prompts will be divided into sub-categories (e.g. left kidney and right kidney). Use <code>points</code> for the sub-categories as defined in the <code>inference.json</code>.</li>\n<li>To specify a new class for zero-shot segmentation, set the <code>label_prompt</code> to a value between 133 and 254. Ensure that <code>points</code> and <code>point_labels</code> are also provided; otherwise, the inference result will be a tensor of zeros.</li>\n</ul>\n<h3><code>label_prompt</code> and <code>label_dict</code></h3>\n<p>The <code>label_dict</code> defined in <code>docs/labels.json</code> has in total 132 classes. However, there are 5 we do not support and we keep them due to legacy issue. So in total\nVISTA3D support 127 classes.</p>\n<pre><code>\"16, # prostate or uterus\" since we already have \"prostate\" class,\n\"18, # rectum\", insufficient data or dataset excluded.\n\"130, # liver tumor\" already have hepatic tumor.\n\"129, # kidney mass\" insufficient data or dataset excluded.\n\"131, # vertebrae L6\", insufficient data or dataset excluded.\n</code></pre>\n<p>These 5 are excluded in the <code>everything_labels</code>. Another 7 tumor and vessel classes are also removed since they will overlap with other organs and make the output messy. To segment those 7 classes, we recommend users to directly set <code>label_prompt</code> to those indexes and avoid using them in <code>everything_labels</code>. For \"Kidney\", \"Lung\", \"Bone\" (class index [2, 20, 21]), VISTA3D did not directly use the class index for segmentation, but instead convert them to their subclass indexes as defined by <code>subclass</code> dict. For example, \"2-Kidney\" is converted to \"14-Left Kidney\" + \"5-Right Kidney\" since \"2\" is defined in <code>subclasss</code> dict.</p>\n<pre><code>Note: if the finetuning mapped the local user data index to global index \"2, 20, 21\", remove the `subclass` dict from inference.json since those values defined in `subclass` will trigger the wrong subclass segmentation.\n</code></pre>\n<h3><code>resample_spacing</code></h3>\n<p>The optimal inference resample spacing should be changed according to the task. For monkey data, a high resolution of [1,1,1] showed better automatic inference results. This spacing applies to both automatic and interactive segmentation. For zero-shot interactive segmentation for non-human CTs e.g. mouse CT or even rock/stone CT, using original resolution (set <code>resample_spacing</code> to [-1,-1,-1]) may give better interactive results.</p>\n<h3><code>use_point_window</code></h3>\n<p>When user click a point, there is no need to perform whole image sliding window inference. Set \"use_point_window\" to true in the inference.json to enable this function.\nA window centered at the clicked points will be used for inference. All values outside of the window will set to be \"NaN\" unless \"prev_mask\" is passed to the inferer (255 is used to represent NaN).\nIf no point click exists, this function will not be used. Notice if \"use_point_window\" is true and user provided point clicks, there will be obvious cut-off box artefacts.</p>\n<h3>Inference GPU benchmarks</h3>\n<p>Benchmarks on a 16GB V100 GPU with 400G system cpu memory.\n| Volume size at 1.5x1.5x1.5 mm | 333x333x603 | 512x512x512 | 512x512x768 | 1024x1024x512 | 1024x1024x768 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|RunTime| 1m07s | 2m09s | 3m25s| 9m20s| killed |</p>\n<h2>Commands</h2>\n<p>The bundle only provides single-gpu inference.</p>\n<h3>Single image inference</h3>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h3>Batch inference for segmenting everything</h3>\n<pre><code>python -m monai.bundle run --config_file=\"['configs/inference.json', 'configs/batch_inference.json']\" --input_dir=\"/data/Task09_Spleen/imagesTr\" --output_dir=\"./eval_task09\"\n</code></pre>\n<p><code>configs/batch_inference.json</code> by default runs the segment everything workflow (classes defined by <code>everything_labels</code>) on all (<code>*.nii.gz</code>) files in <code>input_dir</code>.\nThis default is overridable by changing the input folder <code>input_dir</code>, or the input image name suffix <code>input_suffix</code>, or directly setting the list of filenames <code>input_list</code>.</p>\n<h3>Execute inference with the TensorRT model:</h3>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<p>By default, the argument <code>head_trt_enabled</code> is set to <code>false</code> in <code>configs/inference_trt.json</code>. This means that the <code>class_head</code> module of the network will not be converted into a TensorRT model. Setting this to <code>true</code> may accelerate the process, but there are some limitations:</p>\n<p>Since the <code>label_prompt</code> will be converted into a tensor and input into the <code>class_head</code> module, the batch size of this input tensor will equal the length of the original <code>label_prompt</code> list (if no prompt is provided, the length is 117). To make the TensorRT model work on the <code>class_head</code> module, you should set a suitable dynamic batch size range. The maximum dynamic batch size can be configured using the argument <code>max_prompt_size</code> in <code>configs/inference_trt.json</code>. If the length of the <code>label_prompt</code> list exceeds <code>max_prompt_size</code>, the engine will fall back to using the normal PyTorch model for inference. Setting a larger <code>max_prompt_size</code> can cover more input cases but may require more GPU memory (the default value is 4, which requires 16 GB of GPU memory). Therefore, please set it to a reasonable value according to your actual requirements.</p>\n<h3>TroubleShoot for Out-of-Memory</h3>\n<ul>\n<li>Changing <code>patch_size</code> to a smaller value such as <code>\"patch_size\": [96, 96, 96]</code> would reduce the training/inference memory footprint.</li>\n<li>Changing <code>train_dataset_cache_rate</code> and <code>val_dataset_cache_rate</code> to a smaller value like <code>0.1</code> can solve the out-of-cpu memory issue when using huge finetuning dataset.</li>\n<li>Set <code>\"postprocessing#transforms#0#_disabled_\": false</code> to move the postprocessing to cpu to reduce the GPU memory footprint.</li>\n</ul>\n<h3>TensorRT speedup</h3>\n<p>The <code>vista3d</code> bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note for 32bit precision models, they are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">108.53</td>\n<td style=\"text-align: center;\">91.9</td>\n<td style=\"text-align: center;\">106.84</td>\n<td style=\"text-align: center;\">60.02</td>\n<td style=\"text-align: center;\">1.18</td>\n<td style=\"text-align: center;\">1.02</td>\n<td style=\"text-align: center;\">1.81</td>\n<td style=\"text-align: center;\">1.53</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">6740</td>\n<td style=\"text-align: center;\">5166</td>\n<td style=\"text-align: center;\">5242</td>\n<td style=\"text-align: center;\">3386</td>\n<td style=\"text-align: center;\">1.30</td>\n<td style=\"text-align: center;\">1.29</td>\n<td style=\"text-align: center;\">1.99</td>\n<td style=\"text-align: center;\">1.53</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h1>References</h1>\n<ul>\n<li>\n<p>Antonelli, M., Reinke, A., Bakas, S. et al. The Medical Segmentation Decathlon. Nat Commun 13, 4128 (2022). https://doi.org/10.1038/s41467-022-30695-9</p>\n</li>\n<li>\n<p>VISTA3D: Versatile Imaging SegmenTation and Annotation model for 3D Computed Tomography. arxiv (2024) https://arxiv.org/abs/2406.05285</p>\n</li>\n</ul>\n<h1>License</h1>\n<h2>Code License</h2>\n<p>This project includes code licensed under the Apache License 2.0.\nYou may obtain a copy of the License at</p>\n<p>http://www.apache.org/licenses/LICENSE-2.0</p>\n<h2>Model Weights License</h2>\n<p>The model weights included in this project are licensed under the NCLS v1 License.</p>\n<p>Both licenses' full texts have been combined into a single <code>LICENSE</code> file. Please refer to this <code>LICENSE</code> file for more details about the terms and conditions of both licenses.</p>",
        "download_url": "https://huggingface.co/MONAI/vista3d/tree/0.5.8",
        "changelog": {
            "0.5.8": "update to huggingface hosting",
            "0.5.7": "change sw padding mode to replicate",
            "0.5.6": "add mlflow support",
            "0.5.5": "add arg for trt compiler base path",
            "0.5.4": "add undefined label prompt check",
            "0.5.3": "update readme",
            "0.5.2": "fix eval issue",
            "0.5.1": "add description for zero-shot and upate eval",
            "0.5.0": "update json file link and add test",
            "0.4.9": "fix oom issue and update readme",
            "0.4.8": "use 0.3 overlap for inference",
            "0.4.7": "update tensorrt benchmark results",
            "0.4.6": "add tensorrt benchmark result and remove the metric part",
            "0.4.5": "remove wrong path",
            "0.4.4": "enable tensorrt inference",
            "0.4.3": "fix CL and batch infer issues",
            "0.4.2": "use MONAI components for network and utils",
            "0.4.1": "initial OSS version"
        }
    },
    "maisi_ct_generative": {
        "model_name": "CT image latent diffusion generation",
        "description": "A generative model for creating 3D CT from Gaussian noise",
        "authors": "MONAI team",
        "papers": [],
        "version": "1.0.1",
        "model_id": "maisi_ct_generative",
        "readme": "<h1>Model Overview</h1>\n<p>This bundle is for Nvidia MAISI (Medical AI for Synthetic Imaging), a 3D Latent Diffusion Model that can generate large CT images with paired segmentation masks, variable volume size and voxel size, as well as controllable organ/tumor size.</p>\n<p>The inference workflow of MAISI is depicted in the figure below. It first generates latent features from random noise by applying multiple denoising steps using the trained diffusion model. Then it decodes the denoised latent features into images using the trained autoencoder.</p>\n<p align=\"center\">\n<img alt=\"MAISI inference scheme\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/maisi_workflow_1.0.1.png\"/>\n</p>\n<p>MAISI is based on the following papers:</p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf\"><strong>Latent Diffusion:</strong> Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" CVPR 2022.</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf\"><strong>ControlNet:</strong>  Lvmin Zhang, Anyi Rao, Maneesh Agrawala; Adding Conditional Control to Text-to-Image Diffusion Models. ICCV 2023.</a></p>\n<p><a href=\"https://arxiv.org/pdf/2209.03003\"><strong>Rectified Flow:</strong> Liu, Xingchao, and Chengyue Gong. \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow.\" ICLR 2023.</a></p>\n<h4>Example synthetic image</h4>\n<p>An example result from inference is shown below:\n<img alt=\"Example synthetic image\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_maisi_ct_generative_example_synthetic_data.png\"/></p>\n<h3>Inference configuration</h3>\n<p>The inference requires:\n- GPU: at least 58GB GPU memory for 512 x 512 x 512\n- Disk Memory: at least 21GB disk memory</p>\n<h4>Inference parameters:</h4>\n<p>The information for the inference input, like body region and anatomy to generate, is stored in <a href=\"../configs/inference.json\">./configs/inference.json</a>. Please feel free to play with it. Here are the details of the parameters.</p>\n<ul>\n<li><code>\"num_output_samples\"</code>: int, the number of output image/mask pairs it will generate.</li>\n<li><code>\"spacing\"</code>: voxel size of generated images. E.g., if set to <code>[1.5, 1.5, 2.0]</code>, it will generate images with a resolution of 1.51.52.0 mm. The spacing for x and y axes has to be between 0.5 and 3.0 mm and the spacing for the z axis has to be between 0.5 and 5.0 mm.</li>\n<li><code>\"output_size\"</code>: volume size of generated images. E.g., if set to <code>[512, 512, 256]</code>, it will generate images with size of 512512256. They need to be divisible by 16. If you have a small GPU memory size, you should adjust it to small numbers. Note that <code>\"spacing\"</code> and <code>\"output_size\"</code> together decide the output field of view (FOV). For eample, if set them to <code>[1.5, 1.5, 2.0]</code>mm and <code>[512, 512, 256]</code>, the FOV is 768768512 mm. We recommend output_size is the FOV in x and y axis are same and to be at least 256mm for head, at least 384mm for other body regions like abdomen, and no larger than 640mm. The output size for the x and y axes can be selected from [256, 384, 512], while for the z axis, it can be chosen from [128, 256, 384, 512, 640, 768].</li>\n<li><code>\"controllable_anatomy_size\"</code>: a list of controllable anatomy and its size scale (0--1). E.g., if set to <code>[[\"liver\", 0.5],[\"hepatic tumor\", 0.3]]</code>, the generated image will contain liver that have a median size, with size around 50% percentile, and hepatic tumor that is relatively small, with around 30% percentile. In addition, if the size scale is set to -1, it indicates that the organ does not exist or should be removed. The output will contain paired image and segmentation mask for the controllable anatomy.\nThe following organs support generation with a controllable size: <code>[\"liver\", \"gallbladder\", \"stomach\", \"pancreas\", \"colon\", \"lung tumor\", \"bone lesion\", \"hepatic tumor\", \"colon cancer primaries\", \"pancreatic tumor\"]</code>.\nThe raw output of the current mask generation model has a fixed size of $256^3$ voxels with a spacing of $1.5^3$ mm. If the \"output_size\" differs from this default, the generated masks will be resampled to the desired <code>\"output_size\"</code> and <code>\"spacing\"</code>. Note that resampling may degrade the quality of the generated masks and could trigger multiple inference attempts if the images fail to pass the <a href=\"../scripts/quality_check.py\">image quality check</a>.</li>\n<li><code>\"body_region\"</code>: Deprecated, please leave it as empty <code>\"[]\"</code>.</li>\n<li><code>\"anatomy_list\"</code>: If \"controllable_anatomy_size\" is not specified, the output will contain paired image and segmentation mask for the anatomy in \"./configs/label_dict.json\".</li>\n<li><code>\"autoencoder_sliding_window_infer_size\"</code>: in order to save GPU memory, we use sliding window inference when decoding latents to image when <code>\"output_size\"</code> is large. This is the patch size of the sliding window. Small value will reduce GPU memory but increase time cost. They need to be divisible by 16.</li>\n<li><code>\"autoencoder_sliding_window_infer_overlap\"</code>: float between 0 and 1. Large value will reduce the stitching artifacts when stitching patches during sliding window inference, but increase time cost. If you do not observe seam lines in the generated image result, you can use a smaller value to save inference time.</li>\n</ul>\n<p>To generate images with substantial dimensions, such as 512  512  512 or larger, using GPUs with 80GB of memory, it is advisable to configure the <code>\"num_splits\"</code> parameter in <a href=\"./configs/config_maisi.json#L11-L37\">the auto-encoder configuration</a> to 16. This adjustment is crucial to avoid out-of-memory issues during inference.</p>\n<h4>Recommended spacing for different output sizes:</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\"><code>\"output_size\"</code></th>\n<th style=\"text-align: center;\">Recommended <code>\"spacing\"</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">[256, 256, 256]</td>\n<td style=\"text-align: center;\">[1.5, 1.5, 1.5]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">[512, 512, 128]</td>\n<td style=\"text-align: center;\">[0.8, 0.8, 2.5]</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">[512, 512, 512]</td>\n<td style=\"text-align: center;\">[1.0, 1.0, 1.0]</td>\n</tr>\n</tbody>\n</table>\n<h3>Execute inference</h3>\n<p>The following code generates a synthetic image from a random sampled noise.</p>\n<pre><code>python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<h2>Execute Finetuning</h2>\n<h3>Training configuration</h3>\n<p>The training was performed with the following:\n- GPU: at least 60GB GPU memory for 512 x 512 x 512 volume\n- Actual Model Input (the size of image embedding in latent space): 128 x 128 x 128\n- AMP: True</p>\n<h3>Run finetuning:</h3>\n<p>This config executes finetuning for pretrained ControlNet with with a new class (i.e., Kidney Tumor). When finetuning with new class names, please update <code>configs/train.json</code>'s <code>weighted_loss_label</code> and <code>configs/label_dict.json</code> accordingly. There are 8 dummy labels as placeholders in default <code>configs/label_dict.json</code> that can be used for finetuning.</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.json\n</code></pre>\n<h3>Override the <code>train</code> config to execute multi-GPU training:</h3>\n<pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run --config_file \"['configs/train.json','configs/multi_gpu_train.json']\"\n</code></pre>\n<h3>Data:</h3>\n<p>The preprocessed subset of <a href=\"https://www.cancerimagingarchive.net/collection/c4kc-kits/\">C4KC-KiTS</a> dataset used in this finetuning config is provided in <code>./dataset/C4KC-KiTS_subset</code>.</p>\n<pre><code>            |-*arterial*.nii.gz     # original image\n            |-*arterial_emb*.nii.gz     # encoded image embedding\nKiTS-000* --|-mask*.nii.gz      # original labels\n            |-mask_pseudo_label*.nii.gz     # pseudo labels\n            |-mask_combined_label*.nii.gz     # combined mask of original and pseudo labels\n\n</code></pre>\n<p>An example combined mask of original and pseudo labels is shown below:\n<img alt=\"example_combined_mask\" src=\"https://developer.download.nvidia.com/assets/Clara/Images/monai_maisi_ct_generative_example_combined_mask.png\"/></p>\n<p>Please note that the label of Kidney Tumor is mapped to index <code>129</code> in this preprocessed dataset. The encoded image embedding is generated by provided <code>Autoencoder</code> in <code>./models/autoencoder_epoch273.pt</code> during preprocessing to save memeory usage for training. The pseudo labels are generated by <a href=\"https://github.com/Project-MONAI/VISTA\">VISTA 3D</a>. In addition, the dimension of each volume and corresponding pseudo label is resampled to the closest multiple of 128 (e.g., 128, 256, 384, 512, ...).</p>\n<p>The training workflow requires one JSON file to specify the image embedding and segmentation pairs. The example file is located in the <code>./dataset/C4KC-KiTS_subset.json</code>.</p>\n<p>The JSON file has the following structure:</p>\n<pre><code class=\"language-python\">{\n    \"training\": [\n        {\n            \"image\": \"*/*arterial_emb*.nii.gz\",  # relative path to the image embedding file\n            \"label\": \"*/mask_combined_label*.nii.gz\",  # relative path to the combined label file\n            \"dim\": [512, 512, 512],  # the dimension of image\n            \"spacing\": [1.0, 1.0, 1.0],  # the spacing of image\n            \"top_region_index\": [0, 1, 0, 0],  # the top region index of the image\n            \"bottom_region_index\": [0, 0, 0, 1],  # the bottom region index of the image\n            \"fold\": 0  # fold index for cross validation, fold 0 is used for training\n        },\n\n        ...\n    ]\n}\n</code></pre>\n<h1>References</h1>\n<p>[1] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf</p>\n<h1>License</h1>\n<h2>Code License</h2>\n<p>This project includes code licensed under the Apache License 2.0.\nYou may obtain a copy of the License at</p>\n<p>http://www.apache.org/licenses/LICENSE-2.0</p>\n<h2>Model Weights License</h2>\n<p>The model weights included in this project are licensed under the NCLS v1 License.</p>\n<p>Both licenses' full texts have been combined into a single <code>LICENSE</code> file. Please refer to this <code>LICENSE</code> file for more details about the terms and conditions of both licenses.</p>",
        "download_url": "https://huggingface.co/MONAI/maisi_ct_generative/tree/1.0.1",
        "changelog": {
            "1.0.1": "add missing dependencies",
            "1.0.0": "accelerated maisi, inference only, is not compartible with previous maisi diffusion model weights",
            "0.4.6": "add TensorRT support",
            "0.4.5": "update README",
            "0.4.4": "update issue for IgniteInfo",
            "0.4.3": "remove download large files, add weights_only when loading weights and add label_dict to large files",
            "0.4.2": "update train.json to fix finetune ckpt bug",
            "0.4.1": "update large files",
            "0.4.0": "update to use monai 1.4, model ckpt updated, rm GenerativeAI repo, add quality check",
            "0.3.6": "first oss version"
        }
    },
    "vista2d": {
        "model_name": "VISTA-Cell",
        "description": "VISTA2D bundle for cell image analysis",
        "authors": "MONAI team",
        "papers": [],
        "version": "0.3.1",
        "model_id": "vista2d",
        "readme": "<h2>Overview</h2>\n<p>The <strong>VISTA2D</strong> is a cell segmentation training and inference pipeline for cell imaging [<a href=\"https://developer.nvidia.com/blog/advancing-cell-segmentation-and-morphology-analysis-with-nvidia-ai-foundation-model-vista-2d/\"><code>Blog</code></a>].</p>\n<p>A pretrained model was trained on collection of 15K public microscopy images. The data collection and training can be reproduced following the <code>download_preprocessor/</code>. Alternatively, the model can be retrained on your own dataset. The pretrained vista2d model achieves good performance on diverse set of cell types, microscopy image modalities, and can be further finetuned if necessary.  The codebase utilizes several components from other great works including <a href=\"https://github.com/facebookresearch/segment-anything\">SegmentAnything</a>  and <a href=\"https://www.cellpose.org/\">Cellpose</a>, which must be pip installed as dependencies.  Vista2D codebase follows MONAI bundle format and its <a href=\"https://docs.monai.io/en/stable/mb_specification.html\">specifications</a>.</p>\n<div align=\"center\"> <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/04/magnified-cells-1.png\" width=\"800\"/> </div>\n<h3>Model highlights</h3>\n<ul>\n<li>Robust deep learning algorithm based on transformers</li>\n<li>Generalist model as compared to specialist models</li>\n<li>Multiple dataset sources and file formats supported</li>\n<li>Multiple modalities of imaging data collectively supported</li>\n<li>Multi-GPU and multinode training support</li>\n</ul>\n<h3>Generalization performance</h3>\n<p>Evaluation was performed for the VISTA2D model with multiple public datasets, such as TissueNet, LIVECell, Omnipose, DeepBacs, Cellpose, and more. For more details about dataset licenses, please refer to <code>/docs/data_license.txt</code>. A total of ~15K annotated cell images were collected to train the generalist VISTA2D model. This ensured broad coverage of many different types of cells, which were acquired by various imaging acquisition types. The benchmark results of the experiment were performed on held-out test sets for each public dataset that were already defined by the dataset contributors. Average precision at an IoU threshold of 0.5 was used for evaluating performance. The benchmark results are reported in comparison with the best numbers found in the literature, in addition to a specialist VISTA2D model trained only on a particular dataset or a subset of data.</p>\n<div align=\"center\"> <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/04/vista-2d-model-precision-versus-specialist-model-baseline-performance.png\" width=\"800\"/> </div>\n<h3>TensorRT speedup</h3>\n<p>The <code>vista2d</code> bundle supports acceleration with TensorRT. The table below displays the speedup ratios observed on an A100 80G GPU. Please note that 32-bit precision models are benchmarked with tf32 weight format.</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">method</th>\n<th style=\"text-align: center;\">torch_tf32(ms)</th>\n<th style=\"text-align: center;\">torch_amp(ms)</th>\n<th style=\"text-align: center;\">trt_tf32(ms)</th>\n<th style=\"text-align: center;\">trt_fp16(ms)</th>\n<th style=\"text-align: center;\">speedup amp</th>\n<th style=\"text-align: center;\">speedup tf32</th>\n<th style=\"text-align: center;\">speedup fp16</th>\n<th style=\"text-align: center;\">amp vs fp16</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">model computation</td>\n<td style=\"text-align: center;\">39.72</td>\n<td style=\"text-align: center;\">39.68</td>\n<td style=\"text-align: center;\">26.13</td>\n<td style=\"text-align: center;\">17.32</td>\n<td style=\"text-align: center;\">1.00</td>\n<td style=\"text-align: center;\">1.52</td>\n<td style=\"text-align: center;\">2.29</td>\n<td style=\"text-align: center;\">2.29</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">end2end</td>\n<td style=\"text-align: center;\">1562</td>\n<td style=\"text-align: center;\">1903</td>\n<td style=\"text-align: center;\">1494</td>\n<td style=\"text-align: center;\">1440</td>\n<td style=\"text-align: center;\">0.82</td>\n<td style=\"text-align: center;\">1.05</td>\n<td style=\"text-align: center;\">1.08</td>\n<td style=\"text-align: center;\">1.32</td>\n</tr>\n</tbody>\n</table>\n<p>Where:\n- <code>model computation</code> means the speedup ratio of model's inference with a random input without preprocessing and postprocessing\n- <code>end2end</code> means run the bundle end-to-end with the TensorRT based model.\n- <code>torch_tf32</code> and <code>torch_amp</code> are for the PyTorch models with or without <code>amp</code> mode.\n- <code>trt_tf32</code> and <code>trt_fp16</code> are for the TensorRT based models converted in corresponding precision.\n- <code>speedup amp</code>, <code>speedup tf32</code> and <code>speedup fp16</code> are the speedup ratios of corresponding models versus the PyTorch float32 model\n- <code>amp vs fp16</code> is the speedup ratio between the PyTorch amp model and the TensorRT float16 based model.</p>\n<p>This result is benchmarked under:\n - TensorRT: 10.3.0+cuda12.6\n - Torch-TensorRT Version: 2.4.0\n - CPU Architecture: x86-64\n - OS: ubuntu 20.04\n - Python version:3.10.12\n - CUDA version: 12.6\n - GPU models and configuration: A100 80G</p>\n<h3>Prepare Data Lists and Datasets</h3>\n<p>The default dataset for training, validation, and inference is the <a href=\"https://www.cellpose.org/\">Cellpose</a> dataset. Please follow the <code>download_preprocessor/</code> to prepare the dataset before executing any commands below.</p>\n<p>Additionally, all data lists are available in the <code>datalists.zip</code> file located in the root directory of the bundle. Extract the contents of the <code>.zip</code> file to access the data lists.</p>\n<h3>Dependencies</h3>\n<p>Please refer to the <code>required_packages_version</code> section in <code>configs/metadata.json</code> to install all necessary dependencies before execution. If youre using the MONAI container, you can simply run the commands below and ignore any \"opencv-python-headless not installed\" error message, as this package is already included in the container.</p>\n<pre><code>pip install fastremap==1.15.0 roifile==2024.5.24 natsort==8.4.0\npip install --no-deps cellpose\n</code></pre>\n<p>Important Note: if your environment already contains OpenCV, installing <code>cellpose</code> may lead to conflicts and produce errors such as:</p>\n<pre><code>AttributeError: partially initialized module 'cv2' has no attribute 'dnn' (most likely due to a circular import)\n</code></pre>\n<p>To resolve this, uninstall <code>OpenCV</code> first, and then install <code>cellpose</code> using the following commands:</p>\n<pre><code class=\"language-Bash\">pip uninstall -y opencv &amp;&amp; rm /usr/local/lib/python3.*/dist-packages/cv2\n</code></pre>\n<p>Make sure to replace 3.* with your actual Python version (e.g., 3.10).</p>\n<p>Alternatively, you can install <code>cellpose</code> without its dependencies to avoid potential conflicts:</p>\n<pre><code>pip install --no-deps cellpose\n</code></pre>\n<h3>Execute training</h3>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml\n</code></pre>\n<p>You can override the <code>basedir</code> to specify a different dataset directory by using the following command:</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --basedir &lt;actual dataset ditectory&gt;\n</code></pre>\n<h4>Quick run with a few data points</h4>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --quick True --train#trainer#max_epochs 3\n</code></pre>\n<h3>Execute multi-GPU training</h3>\n<pre><code class=\"language-bash\">torchrun --nproc_per_node=gpu -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml\n</code></pre>\n<h3>Execute validation</h3>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --pretrained_ckpt_name model.pt --mode eval\n</code></pre>\n<p>(can append <code>--quick True</code> for quick demoing)</p>\n<h3>Execute multi-GPU validation</h3>\n<pre><code class=\"language-bash\">torchrun --nproc_per_node=gpu -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --mode eval\n</code></pre>\n<h3>Execute inference</h3>\n<pre><code class=\"language-bash\">python -m monai.bundle run --config_file configs/inference.json\n</code></pre>\n<p>Please note that the data used in this config file is: \"/cellpose_dataset/test/001_img.png\", if the dataset path is different or you want to do inference on another file, please modify in <code>configs/inference.json</code> accordingly.</p>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.json', 'configs/inference_trt.json']\"\n</code></pre>\n<h3>Execute multi-GPU inference</h3>\n<pre><code class=\"language-bash\">torchrun --nproc_per_node=gpu -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --mode infer --pretrained_ckpt_name model.pt\n</code></pre>\n<p>(can append <code>--quick True</code> for quick demoing)</p>\n<h4>Finetune starting from a trained checkpoint</h4>\n<p>(we use a smaller learning rate, small number of epochs, and initialize from a checkpoint)</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --learning_rate=0.001 --train#trainer#max_epochs 20 --pretrained_ckpt_path /path/to/saved/model.pt\n</code></pre>\n<h4>Configuration options</h4>\n<p>To disable the segmentation writing:</p>\n<pre><code>--postprocessing []\n</code></pre>\n<p>Load a checkpoint for validation or inference (relative path within results directory):</p>\n<pre><code>--pretrained_ckpt_name \"model.pt\"\n</code></pre>\n<p>Load a checkpoint for validation or inference (absolute path):</p>\n<pre><code>--pretrained_ckpt_path \"/path/to/another/location/model.pt\"\n</code></pre>\n<p><code>--mode eval</code> or <code>--mode infer</code>will use the corresponding configurations from the <code>validate</code> or <code>infer</code>\nof the <code>configs/hyper_parameters.yaml</code>.</p>\n<p>By default the generated <code>model.pt</code> corresponds to the checkpoint at the best validation score,\n<code>model_final.pt</code> is the checkpoint after the latest training epoch.</p>\n<h3>Development</h3>\n<p>For development purposes it's possible to run the script directly (without monai bundle calls)</p>\n<pre><code class=\"language-bash\">python scripts/workflow.py --config_file configs/hyper_parameters.yaml ...\ntorchrun --nproc_per_node=gpu -m  scripts/workflow.py --config_file configs/hyper_parameters.yaml  ..\n</code></pre>\n<h3>MLFlow support</h3>\n<p>Enable MLFlow logging by specifying \"mlflow_tracking_uri\" (can be local or remote URL).</p>\n<pre><code class=\"language-bash\">python -m monai.bundle run_workflow \"scripts.workflow.VistaCell\" --config_file configs/hyper_parameters.yaml --mlflow_tracking_uri=http://127.0.0.1:8080\n</code></pre>\n<p>Optionally use \"--mlflow_run_name=..\" to specify MLFlow experiment name, and \"--mlflow_log_system_metrics=True/False\" to enable logging of CPU/GPU resources (requires pip install psutil pynvml)</p>\n<h3>Unit tests</h3>\n<p>Test single GPU training:</p>\n<pre><code>python unit_tests/test_vista2d.py\n</code></pre>\n<p>Test multi-GPU training (may need to uncomment the <code>\"--standalone\"</code> in the <code>unit_tests/utils.py</code> file):</p>\n<pre><code>python unit_tests/test_vista2d_mgpu.py\n</code></pre>\n<h2>Compute Requirements</h2>\n<p>Min GPU memory requirements 16Gb.</p>\n<h2>Contributing</h2>\n<p>Vista2D codebase follows MONAI bundle format and its <a href=\"https://docs.monai.io/en/stable/mb_specification.html\">specifications</a>.\nMake sure to run pre-commit before committing code changes to git</p>\n<pre><code class=\"language-bash\">pip install pre-commit\npython3 -m pre_commit run --all-files\n</code></pre>\n<h2>Community</h2>\n<p>Join the conversation on Twitter <a href=\"https://twitter.com/ProjectMONAI\">@ProjectMONAI</a> or join\nour <a href=\"https://projectmonai.slack.com/archives/C031QRE0M1C\">Slack channel</a>.</p>\n<p>Ask and answer questions on <a href=\"https://github.com/Project-MONAI/VISTA/discussions\">MONAI VISTA's GitHub discussions tab</a>.</p>\n<h2>License</h2>\n<p>The codebase is under Apache 2.0 Licence. The model weight is released under CC-BY-NC-SA-4.0. For various public data licenses please see <code>data_license.txt</code>.</p>\n<h2>Acknowledgement</h2>\n<ul>\n<li><a href=\"https://github.com/facebookresearch/segment-anything\">segment-anything</a></li>\n<li><a href=\"https://www.cellpose.org/\">Cellpose</a></li>\n</ul>",
        "download_url": "https://huggingface.co/MONAI/vista2d/tree/0.3.1",
        "changelog": {
            "0.3.1": "update to huggingface hosting",
            "0.3.0": "update readme",
            "0.2.9": "fix unsupported data dtype in findContours",
            "0.2.8": "remove relative path in readme",
            "0.2.7": "enhance readme",
            "0.2.6": "update tensorrt benchmark results",
            "0.2.5": "add tensorrt benchmark results",
            "0.2.4": "enable tensorrt inference",
            "0.2.3": "update weights link",
            "0.2.2": "update to use monai components",
            "0.2.1": "initial OSS version"
        }
    },
    "pediatric_abdominal_ct_segmentation": {
        "model_name": "CT-Ped-Abdominal-Seg",
        "description": "TotalSegmentator, TCIA and BTCV dataset pre-trained model for segmenting liver, spleen and pancreas, fine-tuned on Cincinnati Children's Healthy Pediatric Dataset with High Quality Masks. WandB hyperparameter search was used to find the best hyperparameters for training.",
        "authors": "Cincinnati Children's (CCHMC) - CAIIR Center (https://www.cincinnatichildrens.org/research/divisions/r/radiology/labs/caiir)",
        "papers": [
            "MedArxiv paper: url to be updated"
        ],
        "version": "0.4.5",
        "model_id": "pediatric_abdominal_ct_segmentation",
        "readme": "<h1>Model Overview</h1>\n<p>A Pediatric 3D Abdominal Organ Segmentation model, pretrained on adult and pediatric public datasets, and fine tuned for institutional pediatric data.</p>\n<p>Please cite this manuscript:\nSomasundaram E, Taylor Z, Alves VV, et al. Deep-Learning Models for Abdominal CT Organ Segmentation in Children: Development and Validation in Internal and Heterogeneous Public Datasets. AJR 2024 May 1 [published online]. Accepted manuscript. doi:10.2214/AJR.24.30931</p>\n<h2>Data</h2>\n<p>Modality:\n- CT</p>\n<p>Organs Segmented:\n- Liver\n- Spleen\n- Pancreas</p>\n<p>Pre-training data:\n- Total Segmentator (815)\n- BTCV (30)\n- TCIA Pediatric (282)</p>\n<p>Fine-tuning data:\n- Cincinnati Children's Liver Spleen CT dataset (275)\n- Cincinnati Children's Pancreas CT dataset (146)</p>\n<p>Testing data:\n- Cincinnati Children's Liver-Spleen (57)\n- Cincinnati Children's Pancreas (35)\n- TCIA-Pediatric (74)\n- Total Segmentator (50)</p>\n<p>External dataset licenses can be found in accompanying text file. Internal datasets currently not publicly available.</p>\n<p>To load data for training / inference / evaluate:</p>\n<p>Ensure that the \"image\" and \"label\" parameters within the \"training\" and \"validation\" sections in configs/TS_test.json (or a new dataset json), as well as the \"datalist\" and \"dataset_dir\" in configs/train.yaml, configs/inference.yaml, and configs/evaluate-standalone.yaml files (or the according yaml if using multigpu / parallel or different model inferencing) are each changed to match the intended dataset's values.</p>\n<p>One may make separate .json files detailing which exam images / masks are to be used in the same format as configs/TS_test.json with \"training\" and \"validation\" under root, as long as the \"datalist_file_path\" and \"dataset_dir\" values is changed accordingly in configs/train.yaml and configs/inference.yaml, and configs/evaluate-standalone.yaml (or the according yaml in different circumstances).</p>\n<p>Ensure data folder structure is as follows, with scan files in the primary dataset folder, and mask files in the /labels/final subfolder:\n    dataset/\n     exam_001.nii.gz\n     exam_002.nii.gz\n     ...\n     labels/\n       final/\n         exam_001.nii.gz\n         exam_002.nii.gz\n         ...</p>\n<p>Configuration defaults are currently set to the external TotalSegmentator CT dataset.</p>\n<h3>Model Architectures</h3>\n<ul>\n<li>DynUNet</li>\n<li>SegResNet</li>\n<li>SwinUNETR</li>\n</ul>\n<h3>Hyper-Parameter Tuning</h3>\n<p>Weights and Biases was used to extensively tune each model for learning rate, scheduler and optimizer. For fine-tuning the fraction of trainable layers was also optimized. DynUNet performed overall better on all test datasets. The Total Segmentator model was also compared and the DynUNet model significantly outperformed Total Segmentator on institutional test data while maintaining relatively stable performance on adult and TCIA datasets.</p>\n<h3>Input</h3>\n<p>One channel CT image</p>\n<h3>Output</h3>\n<p>Four channel CT label\n- Label 3: pancreas\n- Label 2: spleen\n- Label 1: liver\n- Label 0: background\n- 96x96x96</p>\n<h2>Performance</h2>\n<ul>\n<li>MedArxiv to be linked</li>\n</ul>\n<h2>MONAI Bundle Commands</h2>\n<p>In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.</p>\n<p>For more details usage instructions, visit the <a href=\"https://docs.monai.io/en/latest/config_syntax.html\">MONAI Bundle Configuration Page</a>.</p>\n<h4>Execute training:</h4>\n<p>Dataset used defaults to TotalSegmentator (https://zenodo.org/records/6802614#.ZFPll4TMKUk)</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml\n</code></pre>\n<p>Please note that if the default dataset path is not modified with the actual path in the bundle config files, you can also override it by using <code>--dataset_dir</code>:</p>\n<pre><code>python -m monai.bundle run --config_file configs/train.yaml --dataset_dir &lt;actual dataset path&gt;\n</code></pre>\n<h4><code>train</code> config to execute multi-GPU training:</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file configs/train-multigpu.yaml\n</code></pre>\n<h4>Override the <code>train</code> config to execute evaluation with the trained model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/train.yaml','configs/evaluate.yaml']\"\n</code></pre>\n<h4>Execute inference:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/inference.yaml\n</code></pre>\n<h4>Execute standalone <code>evaluate</code>:</h4>\n<pre><code>python -m monai.bundle run --config_file configs/evaluate.yaml\n</code></pre>\n<h4>Execute standalone <code>evaluate</code> in parallel:</h4>\n<pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m monai.bundle run --config_file configs/evaluate-standalone.yaml\n</code></pre>\n<h4>Export checkpoint for TorchScript:</h4>\n<pre><code>python -m monai.bundle ckpt_export network_def --filepath models/dynunet_FT.ts --ckpt_file models/dynunet_FT.pt --meta_file configs/metadata.json --config_file configs/inference.yaml\n</code></pre>\n<h4>Export checkpoint to TensorRT based models with fp32 or fp16 precision:</h4>\n<pre><code>python -m monai.bundle trt_export --net_id network_def --filepath models/A100/dynunet_FT_trt_16.ts --ckpt_file models/dynunet_FT.pt --meta_file configs/metadata.json --config_file configs/inference.yaml  --precision &lt;fp32/fp16&gt; --use_trace \"True\" --dynamic_batchsize \"[1, 4, 8]\" --converter_kwargs \"{'truncate_long_and_double':True, 'torch_executed_ops': ['aten::upsample_trilinear3d']}\"\n</code></pre>\n<h4>Execute inference with the TensorRT model:</h4>\n<pre><code>python -m monai.bundle run --config_file \"['configs/inference.yaml', 'configs/inference_trt.yaml']\"\n</code></pre>\n<h1>References</h1>\n<p>[1] Somasundaram E, Taylor Z, Alves VV, et al. Deep-Learning Models for Abdominal CT Organ Segmentation in Children: Development and Validation in Internal and Heterogeneous Public Datasets. AJR 2024 May 1 [published online]. Accepted manuscript. doi:10.2214/AJR.24.30931</p>\n<p>[2] Wasserthal, J., Breit, H.-C., Meyer, M. T., Pradella, M., Hinck, D., Sauter, A. W., Heye, T., Boll, D., Cyriac, J., Yang, S., Bach, M., &amp; Segeroth, M. (2023, June 16). TotalSegmentator: Robust segmentation of 104 anatomical structures in CT images. arXiv.org. https://arxiv.org/abs/2208.05868 . https://doi.org/10.1148/ryai.230024</p>\n<p>[3] Jordan, P., Adamson, P. M., Bhattbhatt, V., Beriwal, S., Shen, S., Radermecker, O., Bose, S., Strain, L. S., Offe, M., Fraley, D., Principi, S., Ye, D. H., Wang, A. S., Van Heteren, J., Vo, N.-J., &amp; Schmidt, T. G. (2021). Pediatric Chest/Abdomen/Pelvic CT Exams with Expert Organ Contours (Pediatric-CT-SEG) (Version 2) [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.X0H0-1706</p>\n<p>[4] https://www.synapse.org/#!Synapse:syn3193805/wiki/89480</p>\n<h1>License</h1>\n<p>Copyright (c) MONAI Consortium</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>",
        "download_url": "https://huggingface.co/MONAI/pediatric_abdominal_ct_segmentation/tree/0.4.5",
        "changelog": {
            "0.4.5": "update to huggingface hosting",
            "0.4.4": "initial bundle assemblage."
        }
    },
    "brain_image_synthesis_latent_diffusion_model": {
        "model_name": "Brain image synthesis latent diffusion model",
        "description": "A generative model for creating high-resolution 3D brain MRI based on UK Biobank",
        "authors": "Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M. Jorge Cardoso",
        "papers": [
            "Pinaya, Walter HL, et al. \"Brain imaging generation with latent diffusion models.\" MICCAI Workshop on Deep Generative Models. Springer, Cham, 2022."
        ],
        "version": "1.0.2",
        "model_id": "brain_image_synthesis_latent_diffusion_model",
        "readme": "<h1>Brain Imaging Generation with Latent Diffusion Models</h1>\n<h3><strong>Authors</strong></h3>\n<p>Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev,\nSebastien Ourselin, and M. Jorge Cardoso</p>\n<h3><strong>Tags</strong></h3>\n<p>Synthetic data, Latent Diffusion Model, Generative model, Brain Imaging</p>\n<h2><strong>Model Description</strong></h2>\n<p>This model is trained using the Latent Diffusion Model architecture [1] and is used for the synthesis of conditioned 3D\nbrain MRI data. The model is divided into two parts: an autoencoder with a KL-regularisation model that compresses data\ninto a latent space and a diffusion model that learns to generate conditioned synthetic latent representations. This\nmodel is conditioned on age, sex, the volume of ventricular cerebrospinal fluid, and brain volume normalised for head size.</p>\n<p><img alt=\"\" src=\"./figure_1.png\"/> <br/></p>\n<p align=\"center\">\nFigure 1 - Synthetic image from the model. </p>\n<h2><strong>Data</strong></h2>\n<p>The model was trained on brain data from 31,740 participants from the UK Biobank [2]. We used high-resolution 3D T1w MRI with voxel size of 1mm3, resulting in volumes with 160 x 224 x 160 voxels</p>\n<h4><strong>Preprocessing</strong></h4>\n<p>We used UniRes [3] to perform a rigid body registration to a common MNI space for image pre-processing. The voxel intensity was normalised to be between [0, 1].</p>\n<h2><strong>Performance</strong></h2>\n<p>This model achieves the following results on UK Biobank: an FID of 0.0076, an MS-SSIM of 0.6555, and a 4-G-R-SSIM of 0.3883.</p>\n<p>Please, check Table 1 of the original paper for more details regarding evaluation results.</p>\n<h2><strong>commands example</strong></h2>\n<p>Execute sampling:</p>\n<pre><code class=\"language-shell\">python -m monai.bundle run --config_file configs/inference.json --gender 1.0 --age 0.7 --ventricular_vol 0.7 --brain_vol 0.5\n</code></pre>\n<p>All conditioning are expected to have values between 0 and 1</p>\n<h2>Using a new version of the model</h2>\n<p>If you want to use the checkpoints from a newly fine-tuned model, you need to set parameter load_old to 0 when you run inference,\nto avoid the function load_old_state_dict being called instead of load_state_dict to be called, currently default, as it is\nrequired to load the checkpoint from the original GenerativeModels repository.</p>\n<pre><code class=\"language-shell\">python -m monai.bundle run --config_file configs/inference.json --gender 1.0 --age 0.7 --ventricular_vol 0.7 --brain_vol 0.5 --load_old 0\n</code></pre>\n<h2><strong>Citation Info</strong></h2>\n<pre><code class=\"language-bibtex\">@inproceedings{pinaya2022brain,\n  title={Brain imaging generation with latent diffusion models},\n  author={Pinaya, Walter HL and Tudosiu, Petru-Daniel and Dafflon, Jessica and Da Costa, Pedro F and Fernandez, Virginia and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M Jorge},\n  booktitle={MICCAI Workshop on Deep Generative Models},\n  pages={117--126},\n  year={2022},\n  organization={Springer}\n}\n</code></pre>\n<h2><strong>References</strong></h2>\n<p>Example:</p>\n<p>[1] Pinaya, Walter HL, et al. \"Brain imaging generation with latent diffusion models.\" MICCAI Workshop on Deep Generative Models. Springer, Cham, 2022.</p>\n<p>[2] Sudlow, Cathie, et al. \"UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age.\" PLoS medicine 12.3 (2015): e1001779.</p>\n<p>[3] Brudfors, Mikael, et al. \"MRI super-resolution using multi-channel total variation.\" Annual Conference on Medical Image Understanding and Analysis. Springer, Cham, 2018.</p>",
        "download_url": "https://huggingface.co/MONAI/brain_image_synthesis_latent_diffusion_model/tree/1.0.2",
        "changelog": {
            "1.0.2": "fix missing dependencies",
            "1.0.1": "update to huggingface hosting",
            "1.0.0": "Initial release"
        }
    },
    "cxr_image_synthesis_latent_diffusion_model": {
        "model_name": "Cxr image synthesis latent diffusion model",
        "description": "A generative model for creating high-resolution chest X-ray based on MIMIC dataset",
        "authors": "Walter Hugo Lopez Pinaya, Mark Graham, Eric Kerfoot, Virginia Fernandez",
        "papers": [],
        "version": "1.0.1",
        "model_id": "cxr_image_synthesis_latent_diffusion_model",
        "readme": "<h1>Description</h1>\n<p>A diffusion model to synthetise X-Ray images based on radiological report impressions.</p>\n<h1>Model Overview</h1>\n<p>This model is trained from scratch using the Latent Diffusion Model architecture [1] and is used for the synthesis of\n2D Chest X-ray conditioned on Radiological reports. The model is divided into two parts: an autoencoder with a\nKL-regularisation model that compresses data into a latent space and a diffusion model that learns to generate\nconditioned synthetic latent representations. This model is conditioned on Findings and Impressions from radiological\nreports. The original repository can be found <a href=\"https://github.com/Warvito/generative_chestxray\">here</a></p>\n<p><img alt=\"\" src=\"./figure_1.png\"/> <br/></p>\n<p align=\"center\">\nFigure 1 - Synthetic images from the model. </p>\n<h1>Data</h1>\n<p>The model was trained on brain data from 90,000 participants from the MIMIC dataset [2] [3]. We downsampled the\noriginal images to have a format of 512 x 512 pixels.</p>\n<h2>Preprocessing</h2>\n<p>We resized the original images to make the smallest sides have 512 pixels. When inputting it to the network, we center\ncropped the images to 512 x 512. The pixel intensity was normalised to be between [0, 1]. The text data was obtained\nfrom associated radiological reports. We randoomly extracted sentences from the findings and impressions sections of the\nreports, having a maximum of 5 sentences and 77 tokens. The text was tokenised using the CLIPTokenizer from\ntransformers package (https://github.com/huggingface/transformers) (pretrained model\n\"stabilityai/stable-diffusion-2-1-base\") and then encoded using CLIPTextModel from the same package and pretrained\nmodel.</p>\n<h1>Examples of inference</h1>\n<p>Here we included a few examples of commands to sample images from the model and save them as .jpg files. The available\narguments for this task are: \"--prompt\" (str) text prompt to condition the model on; \"--guidance_scale\" (float), the\nparameter that controls how much the image generation process follows the text prompt. The higher the value, the more\nthe image sticks to a given text input (the common range is between 1-21).</p>\n<p>Examples:</p>\n<pre><code class=\"language-shell\">$ python -m monai.bundle run --config_file configs/inference.json --prompt \"Big right-sided pleural effusion\" --guidance_scale 7.0\n</code></pre>\n<pre><code class=\"language-shell\">$ python -m monai.bundle run --config_file configs/inference.json --prompt \"Small right-sided pleural effusion\" --guidance_scale 7.0\n</code></pre>\n<pre><code class=\"language-shell\">$ python -m monai.bundle run --config_file configs/inference.json --prompt \"Bilateral pleural effusion\" --guidance_scale 7.0\n</code></pre>\n<pre><code class=\"language-shell\">$ python -m monai.bundle run --config_file configs/inference.json --prompt \"Cardiomegaly\" --guidance_scale 7.0\n</code></pre>\n<h2>Using a new version of the model</h2>\n<p>If you want to use the checkpoints from a newly fine-tuned model, you need to set parameter load_old to 0 when you run inference,\nto avoid the function load_old_state_dict being called instead of load_state_dict to be called, currently default, as it is\nrequired to load the checkpoint from the original GenerativeModels repository.</p>\n<pre><code class=\"language-shell\">$ python -m monai.bundle run --config_file configs/inference.json --prompt \"Pleural effusion.\" --guidance_scale 7.0 --load_old 0\n</code></pre>\n<h2>References</h2>\n<p>[1] Pinaya, Walter HL, et al. \"Brain imaging generation with latent diffusion models.\" MICCAI Workshop on Deep Generative Models. Springer, Cham, 2022.</p>\n<p>[2] Johnson, A., Lungren, M., Peng, Y., Lu, Z., Mark, R., Berkowitz, S., &amp; Horng, S. (2019). MIMIC-CXR-JPG - chest radiographs with structured labels (version 2.0.0). PhysioNet. https://doi.org/10.13026/8360-t248.</p>\n<p>[3] Johnson AE, Pollard TJ, Berkowitz S, Greenbaum NR, Lungren MP, Deng CY, Mark RG, Horng S. MIMIC-CXR: A large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. 2019 Jan 21.</p>",
        "download_url": "https://huggingface.co/MONAI/cxr_image_synthesis_latent_diffusion_model/tree/1.0.1",
        "changelog": {
            "1.0.1": "update to huggingface hosting",
            "1.0.0": "Initial release"
        }
    },
    "mednist_ddpm": {
        "model_name": "Mednist ddpm",
        "description": "",
        "authors": "Walter Hugo Lopez Pinaya, Mark Graham, and Eric Kerfoot",
        "papers": [],
        "version": "1.0.2",
        "model_id": "mednist_ddpm",
        "readme": "<h1>MedNIST DDPM Example Bundle</h1>\n<p>This implements roughly equivalent code to the \"Denoising Diffusion Probabilistic Models with MedNIST Dataset\"\nexample notebook. This includes scripts for training with single or multiple GPUs and a visualisation notebook.</p>\n<p>The files included here demonstrate how to use the bundle:\n  * <a href=\"./2d_ddpm_bundle_tutorial.ipynb\">2d_ddpm_bundle_tutorial.ipynb</a> - demonstrates command line and in-code invocation of the bundle's training and inference scripts\n  * <a href=\"sub_train.sh\">sub_train.sh</a> - SLURM submission script example for training\n  * <a href=\"sub_train_multigpu.sh\">sub_train_multigpu.sh</a> - SLURM submission script example for training with multiple GPUs</p>",
        "download_url": "https://huggingface.co/MONAI/mednist_ddpm/tree/1.0.2",
        "changelog": {
            "1.0.2": "add missing dependencies",
            "1.0.1": "update to huggingface hosting",
            "1.0.0": "Initial release"
        }
    },
    "hf_exaonepath": {
        "model_name": "EXAONEPath",
        "description": "EXAONEPath is a patch-level pathology pretrained model with 86 million parameters, pretrained on 285,153,903 patches extracted from 34,795 WSIs.",
        "authors": "LG AI Research",
        "papers": [
            "Yun, Juseung, et al. 'EXAONEPath 1.0 Patch-level Foundation Model for Pathology', arXiv preprint arXiv:2408.00380 (2024)."
        ],
        "version": "1.0.0",
        "model_id": "hf_exaonepath",
        "readme": "<hr/>\n<p>license: other\nlicense_name: exaonepath\nlicense_link: LICENSE\ntags:\n- lg-ai\n- EXAONEPath-1.0\n- pathology\n- lg-ai</p>\n<hr/>\n<h1>EXAONEPath</h1>\n<h2>EXAONEPath 1.0 Patch-level Foundation Model for Pathology</h2>\n<p>[<a href=\"https://arxiv.org/abs/2408.00380\"><code>Paper</code></a>] [<a href=\"https://github.com/LG-AI-EXAONE/EXAONEPath\"><code>Github</code></a>] [<a href=\"https://github.com/LG-AI-EXAONE/EXAONEPath/releases/download/1.0.0/EXAONEPath.ckpt\"><code>Model</code></a>] [<a href=\"#Citation\"><code>BibTeX</code></a>]</p>\n<h2>Introduction</h2>\n<p>We introduce EXAONEPath, a patch-level pathology pretrained model with 86 million parameters.\nThe model was pretrained on 285,153,903 patches extracted from a total of 34,795 WSIs.\nEXAONEPath demonstrates superior performance considering the number of WSIs used and the model's parameter count.</p>\n<h2>Quickstart</h2>\n<p>Load EXAONEPath and run inference on tile-level images.</p>\n<h3>1. Hardware Requirements</h3>\n<ul>\n<li>NVIDIA GPU is required</li>\n<li>Minimum 8GB GPU memory recommended</li>\n<li>NVIDIA driver version &gt;= 450.80.02 required</li>\n</ul>\n<p>Note: This implementation requires NVIDIA GPU and drivers. The provided environment setup specifically uses CUDA-enabled PyTorch, making NVIDIA GPU mandatory for running the model.</p>\n<h3>2. Environment Setup</h3>\n<p>First, install Conda if you haven't already. You can find installation instructions <a href=\"https://docs.anaconda.com/miniconda/\">here</a>.\nThen create and activate the environment using the provided configuration:</p>\n<pre><code class=\"language-bash\">git clone https://github.com/LG-AI-EXAONE/EXAONEPath.git\ncd EXAONEPath\nconda env create -f environment.yaml\nconda activate exaonepath\n</code></pre>\n<h3>3. Load the model &amp; Inference</h3>\n<h4>Load with HuggingFace</h4>\n<pre><code class=\"language-python\">import torch\nfrom PIL import Image\nfrom macenko import macenko_normalizer\nimport torchvision.transforms as transforms\nfrom vision_transformer import VisionTransformer\n\nhf_token = \"YOUR_HUGGING_FACE_ACCESS_TOKEN\"\nmodel = VisionTransformer.from_pretrained(\"LGAI-EXAONE/EXAONEPath\", use_auth_token=hf_token)\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.CenterCrop(224),\n        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ]\n)\n\nnormalizer = macenko_normalizer()\nimg_path = \"images/MHIST_aaa.png\"\nimage = Image.open(img_path).convert(\"RGB\")\nimage_macenko = normalizer(image)\n\nsample_input = transform(image_macenko).unsqueeze(0)\nmodel.cuda()\nmodel.eval()\n\nfeatures = model(sample_input.cuda())\n</code></pre>\n<h4>Load Manually</h4>\n<p>First, download the EXAONEPath model checkpoint from <a href=\"https://github.com/LG-AI-EXAONE/EXAONEPath/releases/download/1.0.0/EXAONEPath.ckpt\">here</a></p>\n<pre><code class=\"language-python\">import torch\nfrom PIL import Image\nfrom macenko import macenko_normalizer\nimport torchvision.transforms as transforms\nfrom vision_transformer import vit_base\n\nfile_path = \"MODEL_CHECKPOINT_PATH\"\ncheckpoint = torch.load(file_path, map_location=torch.device('cpu'))\nstate_dict = checkpoint['state_dict']\nmodel = vit_base(patch_size=16, num_classes=0)\nmsg = model.load_state_dict(state_dict, strict=False)\nprint(f'Pretrained weights found at {file_path} and loaded with msg: {msg}')\n\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n        transforms.CenterCrop(224),\n        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ]\n)\n\nnormalizer = macenko_normalizer()\nimg_path = \"images/MHIST_aaa.png\"\nimage = Image.open(img_path).convert(\"RGB\")\nimage_macenko = normalizer(image)\n\nsample_input = transform(image_macenko).unsqueeze(0)\nmodel.cuda()\nmodel.eval()\n\nfeatures = model(sample_input.cuda())\n</code></pre>\n<h2>Model Performance Comparison</h2>\n<p>We report linear evaluation result on six downstream tasks. Top-1 accuracy is shown, with values for models other than Gigapath taken from the RudolfV paper.</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCAM</th>\n<th>MHIST</th>\n<th>CRC-100K</th>\n<th>TIL Det.</th>\n<th>MSI CRC</th>\n<th>MSI STAD</th>\n<th>Avg</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ResNet50 ImageNet</td>\n<td>0.833</td>\n<td>0.806</td>\n<td>0.849</td>\n<td>0.915</td>\n<td>0.653</td>\n<td>0.664</td>\n<td>0.787</td>\n</tr>\n<tr>\n<td>ViT-L/16 ImageNet</td>\n<td>0.852</td>\n<td>0.796</td>\n<td>0.847</td>\n<td>0.924</td>\n<td>0.669</td>\n<td>0.671</td>\n<td>0.793</td>\n</tr>\n<tr>\n<td>Lunit</td>\n<td>0.918</td>\n<td>0.771</td>\n<td>0.949</td>\n<td>0.943</td>\n<td>0.745</td>\n<td>0.756</td>\n<td>0.847</td>\n</tr>\n<tr>\n<td>CTransPath</td>\n<td>0.872</td>\n<td>0.817</td>\n<td>0.840</td>\n<td>0.930</td>\n<td>0.694</td>\n<td>0.726</td>\n<td>0.813</td>\n</tr>\n<tr>\n<td>Phikon</td>\n<td>0.906</td>\n<td>0.795</td>\n<td>0.883</td>\n<td><strong>0.946</strong></td>\n<td>0.733</td>\n<td>0.751</td>\n<td>0.836</td>\n</tr>\n<tr>\n<td>Virchow</td>\n<td>0.933</td>\n<td><strong>0.834</strong></td>\n<td>0.968</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>RudolfV</td>\n<td>0.944</td>\n<td>0.821</td>\n<td><strong>0.973</strong></td>\n<td>0.943</td>\n<td>0.755</td>\n<td>0.788</td>\n<td><strong>0.871</strong></td>\n</tr>\n<tr>\n<td>GigaPath (patch encoder)</td>\n<td><strong>0.947</strong></td>\n<td>0.822</td>\n<td>0.964</td>\n<td>0.938</td>\n<td>0.753</td>\n<td>0.748</td>\n<td>0.862</td>\n</tr>\n<tr>\n<td>EXAONEPath (ours)</td>\n<td>0.901</td>\n<td>0.818</td>\n<td>0.946</td>\n<td>0.939</td>\n<td><strong>0.756</strong></td>\n<td><strong>0.804</strong></td>\n<td>0.861</td>\n</tr>\n</tbody>\n</table>\n<p><br/></p>\n<figure>\n<div style=\"display: flex; justify-content: center; gap: 10px;\">\n<img alt=\"Model Comparison Param\" src=\"figures/model_comparison_param-1.png\" style=\"width: 49%;\"/>\n<img alt=\"Model Comparison WSIS\" src=\"figures/model_comparison_wsis-1.png\" style=\"width: 49%;\"/>\n</div>\n<figcaption style=\"text-align: left;\">\n<strong>Figure 1. Performance comparison of models based on the number of parameters and the number of WSIs used for training.</strong> The average Top-1 accuracy represents the mean linear evaluation performance across six downstream tasks.\n    </figcaption>\n</figure>\n<h2>License</h2>\n<p>The model is licensed under <a href=\"./LICENSE\">EXAONEPath AI Model License Agreement 1.0 - NC</a></p>\n<h2>Citation</h2>\n<p>If you find EXAONEPath useful, please cite it using this BibTeX:</p>\n<pre><code>@article{yun2024exaonepath,\n  title={EXAONEPath 1.0 Patch-level Foundation Model for Pathology},\n  author={Yun, Juseung and Hu, Yi and Kim, Jinhyung and Jang, Jongseong and Lee, Soonyoung},\n  journal={arXiv preprint arXiv:2408.00380},\n  year={2024}\n}\n</code></pre>\n<h2>Contact</h2>\n<p>LG AI Research Technical Support: <a href=\"mailto:contact_us1@lgresearch.ai\">contact_us1@lgresearch.ai</a></p>",
        "huggingface_url": "https://huggingface.co/LGAI-EXAONE/EXAONEPath",
        "changelog": {
            "1.0.0": "initial release of EXAONEPath 1.0"
        }
    },
    "hf_llama3_vila_m3_8b": {
        "model_name": "VILA_M3_8B",
        "description": "VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.",
        "authors": "Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, et al. from NVIDIA, SingHealth, and NIH",
        "papers": [
            "Nath, Vishwesh, et al. 'VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge', arXiv preprint arXiv:2411.12915 (2025)."
        ],
        "version": "1.0.0",
        "model_id": "hf_llama3_vila_m3_8b",
        "readme": "<h1>VILA_M3_8B</h1>\n<p>VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.</p>\n<p>This model is available at: <a href=\"https://huggingface.co/MONAI/Llama3-VILA-M3-8B\">MONAI/Llama3-VILA-M3-8B</a></p>\n<h2>Citation</h2>\n<pre><code>@article{nath2025vila,\n  title={VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge},\n  author={Nath, Vishwesh and Li, Wenqi and Yang, Dong and Myronenko, Andriy and Zheng, Mingxin and Lu, Yao and Liu, Zhijian and Yin, Hongxu and Tang, Yucheng and Guo, Pengfei and Zhao, Can and Xu, Ziyue and He, Yufan and Law, Yee Man and Simon, Benjamin and Harmon, Stephanie and Heinrich, Greg and Aylward, Stephen and Edgar, Marc and Zephyr, Michael and Han, Song and Molchanov, Pavlo and Turkbey, Baris and Roth, Holger and Xu, Daguang},\n  journal={arXiv preprint arXiv:2411.12915},\n  year={2025}\n}\n</code></pre>",
        "huggingface_url": "https://huggingface.co/MONAI/Llama3-VILA-M3-8B",
        "changelog": {
            "1.0.0": "initial release of VILA_M3_8B model"
        }
    },
    "hf_ct_chat": {
        "model_name": "CT_CHAT",
        "description": "CT-CHAT is a multimodal AI assistant designed to enhance the interpretation and diagnostic capabilities of 3D chest CT imaging. Building on the strong foundation of CT-CLIP, it integrates both visual and language processing to handle diverse tasks like visual question answering, report generation, and multiple-choice questions. Trained on over 2.7 million question-answer pairs from CT-RATE, it leverages 3D spatial information, making it superior to 2D-based models.",
        "authors": "Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, et al.",
        "papers": [
            "Hamamci, Ibrahim Ethem, et al. 'Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography.' arXiv preprint arXiv:2403.17834 (2024).",
            "Hamamci, Ibrahim Ethem, et al. 'GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes.' arXiv preprint arXiv:2305.16037 (2024).",
            "Hamamci, Ibrahim Ethem, et al. 'CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging.' arXiv preprint arXiv:2403.06801 (2024)."
        ],
        "version": "1.0.0",
        "model_id": "hf_ct_chat",
        "readme": "<hr/>\n<p>license: cc-by-nc-sa-4.0\ntags:\n- computed-tomography\n- chest-ct\n- medical-imaging\n- vision-language-model\n- multimodal\n- medical-assistant</p>\n<hr/>\n<h1>CT-CHAT Model</h1>\n<h2><a href=\"https://arxiv.org/abs/2403.17834\">Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography</a></h2>\n<h2>Model Overview</h2>\n<p>CT-CHAT is a vision-language foundational chat model for 3D chest CT volumes. Leveraging the VQA dataset derived from CT-RATE and pretrained 3D vision encoder from CT-CLIP, we developed this multimodal AI assistant specifically designed to enhance the interpretation and diagnostic capabilities of 3D chest CT imaging.</p>\n<p>Building on the strong foundation of CT-CLIP, CT-CHAT integrates both visual and language processing to handle diverse tasks including:\n- Visual question answering\n- Radiology report generation\n- Multiple-choice diagnostic questions</p>\n<p>Trained on over 2.7 million question-answer pairs from the CT-RATE dataset, CT-CHAT leverages 3D spatial information, making it superior to 2D-based models. The model not only improves radiologist workflows by reducing interpretation time but also delivers highly accurate and clinically relevant responses, pushing the boundaries of 3D medical imaging analysis.</p>\n<h2>Technical Foundation</h2>\n<p>CT-CHAT builds upon two key technological innovations:</p>\n<h3>CT-CLIP</h3>\n<p>A CT-focused contrastive language-image pre-training framework that serves as the visual encoder for CT-CHAT. As a versatile, self-supervised model, CT-CLIP is designed for broad application and outperforms state-of-the-art, fully supervised methods in multi-abnormality detection.</p>\n<h3>CT-RATE Dataset</h3>\n<p>A pioneering dataset of 25,692 non-contrast chest CT volumes (expanded to 50,188 through various reconstructions) paired with corresponding radiology text reports, multi-abnormality labels, and metadata from 21,304 unique patients.</p>\n<h2>Model Capabilities</h2>\n<ol>\n<li><strong>Visual Question Answering</strong>: Answer free-form questions about 3D CT volumes</li>\n<li><strong>Report Generation</strong>: Create comprehensive radiology reports from CT scans</li>\n<li><strong>Diagnostic Support</strong>: Assist with differential diagnoses and abnormality detection</li>\n<li><strong>Educational Use</strong>: Train medical students and residents on CT interpretation</li>\n</ol>\n<h2>Terms and Conditions</h2>\n<p>Users of the CT-CHAT model must agree to the <a href=\"https://huggingface.co/datasets/ibrahimhamamci/CT-RATE\">Terms and Conditions</a> which specify:</p>\n<ul>\n<li>The model is intended solely for academic, research, and educational purposes</li>\n<li>Any commercial exploitation is forbidden without permission</li>\n<li>Users must maintain data confidentiality and comply with data protection laws</li>\n<li>Proper attribution is required in any publications resulting from model use</li>\n<li>Redistribution of the model is not allowed</li>\n</ul>\n<h2>Citation</h2>\n<p>When using this model, please consider citing the following related papers:</p>\n<pre><code class=\"language-bibtex\">@misc{hamamci2024foundation,\n      title={Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography},\n      author={Ibrahim Ethem Hamamci and Sezgin Er and Furkan Almas and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Irem Dogan and Muhammed Furkan Dasdelen and Omer Faruk Durugol and Bastian Wittmann and Tamaz Amiranashvili and Enis Simsar and Mehmet Simsar and Emine Bensu Erdemir and Abdullah Alanbay and Anjany Sekuboyina and Berkan Lafci and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze},\n      year={2024},\n      eprint={2403.17834},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2403.17834},\n}\n\n@misc{hamamci2024generatect,\n      title={GenerateCT: Text-Conditional Generation of 3D Chest CT Volumes},\n      author={Ibrahim Ethem Hamamci and Sezgin Er and Anjany Sekuboyina and Enis Simsar and Alperen Tezcan and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Furkan Almas and Irem Dogan and Muhammed Furkan Dasdelen and Chinmay Prabhakar and Hadrien Reynaud and Sarthak Pati and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze},\n      year={2024},\n      eprint={2305.16037},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2305.16037},\n}\n\n@misc{hamamci2024ct2rep,\n      title={CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging},\n      author={Ibrahim Ethem Hamamci and Sezgin Er and Bjoern Menze},\n      year={2024},\n      eprint={2403.06801},\n      archivePrefix={arXiv},\n      primaryClass={eess.IV},\n      url={https://arxiv.org/abs/2403.06801},\n}\n</code></pre>",
        "huggingface_url": "https://huggingface.co/datasets/ibrahimhamamci/CT-RATE",
        "changelog": {
            "1.0.0": "initial release of CT_CHAT model"
        }
    },
    "hf_llama3_vila_m3_3b": {
        "model_name": "VILA_M3_3B",
        "description": "VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.",
        "authors": "Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, et al. from NVIDIA, SingHealth, and NIH",
        "papers": [
            "Nath, Vishwesh, et al. 'VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge', arXiv preprint arXiv:2411.12915 (2025)."
        ],
        "version": "1.0.0",
        "model_id": "hf_llama3_vila_m3_3b",
        "readme": "<h1>VILA_M3_3B</h1>\n<p>VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.</p>\n<p>This model is available at: <a href=\"https://huggingface.co/MONAI/Llama3-VILA-M3-3B\">MONAI/Llama3-VILA-M3-3B</a></p>\n<h2>Citation</h2>\n<pre><code>@article{nath2025vila,\n  title={VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge},\n  author={Nath, Vishwesh and Li, Wenqi and Yang, Dong and Myronenko, Andriy and Zheng, Mingxin and Lu, Yao and Liu, Zhijian and Yin, Hongxu and Tang, Yucheng and Guo, Pengfei and Zhao, Can and Xu, Ziyue and He, Yufan and Law, Yee Man and Simon, Benjamin and Harmon, Stephanie and Heinrich, Greg and Aylward, Stephen and Edgar, Marc and Zephyr, Michael and Han, Song and Molchanov, Pavlo and Turkbey, Baris and Roth, Holger and Xu, Daguang},\n  journal={arXiv preprint arXiv:2411.12915},\n  year={2025}\n}\n</code></pre>",
        "huggingface_url": "https://huggingface.co/MONAI/Llama3-VILA-M3-3B",
        "changelog": {
            "1.0.0": "initial release of VILA_M3_3B model"
        }
    },
    "hf_llama3_vila_m3_13b": {
        "model_name": "VILA_M3_13B",
        "description": "VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.",
        "authors": "Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, et al. from NVIDIA, SingHealth, and NIH",
        "papers": [
            "Nath, Vishwesh, et al. 'VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge', arXiv preprint arXiv:2411.12915 (2025)."
        ],
        "version": "1.0.0",
        "model_id": "hf_llama3_vila_m3_13b",
        "readme": "<h1>VILA_M3_13B</h1>\n<p>VILA_M3 is a medical vision language model that enhances VLMs with medical expert knowledge, utilizing domain-expert models to improve precision in medical imaging tasks.</p>\n<p>This model is available at: <a href=\"https://huggingface.co/MONAI/Llama3-VILA-M3-13B\">MONAI/Llama3-VILA-M3-13B</a></p>\n<h2>Citation</h2>\n<pre><code>@article{nath2025vila,\n  title={VILA_M3: Enhancing Vision-Language Models with Medical Expert Knowledge},\n  author={Nath, Vishwesh and Li, Wenqi and Yang, Dong and Myronenko, Andriy and Zheng, Mingxin and Lu, Yao and Liu, Zhijian and Yin, Hongxu and Tang, Yucheng and Guo, Pengfei and Zhao, Can and Xu, Ziyue and He, Yufan and Law, Yee Man and Simon, Benjamin and Harmon, Stephanie and Heinrich, Greg and Aylward, Stephen and Edgar, Marc and Zephyr, Michael and Han, Song and Molchanov, Pavlo and Turkbey, Baris and Roth, Holger and Xu, Daguang},\n  journal={arXiv preprint arXiv:2411.12915},\n  year={2025}\n}\n</code></pre>",
        "huggingface_url": "https://huggingface.co/MONAI/Llama3-VILA-M3-13B",
        "changelog": {
            "1.0.0": "initial release of VILA_M3_13B model"
        }
    }
}